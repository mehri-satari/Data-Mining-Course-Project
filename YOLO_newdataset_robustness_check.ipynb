{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPT67YkEcy1bHNPoqZy9isG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehri-satari/Data-Mining-Course-Project/blob/main/YOLO_newdataset_robustness_check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 0 — Environment Setup (Colab + Dependencies)\n",
        "\n",
        "This cell prepares the Colab runtime for the full experiment. It installs the required packages (Ultralytics YOLOv8 for training/validation, OpenCV and PIL for image processing, tqdm for progress tracking, and the AV2 library for reading Argoverse 2 files). Then it mounts Google Drive so the AV2 dataset can be accessed from /content/drive/. Finally, it imports all core libraries used later for loading AV2 annotations/calibration, building the YOLO-format dataset, and training/evaluating the detector."
      ],
      "metadata": {
        "id": "uN9sNB_C0lqw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjs9eMLeX0S2",
        "outputId": "8fcaec1d-6fc6-40c4-ffc5-9269ed4b31d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# --- Colab installs (run once) ---\n",
        "!pip -q install ultralytics opencv-python pillow tqdm\n",
        "\n",
        "# AV2: if not installed already in your env\n",
        "!pip -q install av2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "import shutil\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "from av2.utils import io as io_utils\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 — Define Scene and Load Core Inputs (Annotations + Calibration)\n",
        "\n",
        "Step 1 is just the setup: I load the 3D annotations and the camera calibration. The annotations give me 3D cuboids per timestamp in the ego frame, and intrinsics/extrinsics let me transform ego-to-camera and project those cuboids into each camera image to get 2D boxes. I finish by sanity-checking schemas and confirming the scene stats: 9 cameras, 157 timestamps, 89 tracked objects."
      ],
      "metadata": {
        "id": "VBZG5U431qSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCENE_PATH = Path(\"/content/drive/MyDrive/Argoverse2/0526e68e-2ff1-3e53-b0f8-45df02e45a93\")\n",
        "\n",
        "def load_initial_data(scene_path: Path):\n",
        "    ann_df = io_utils.read_feather(scene_path / \"annotations.feather\")\n",
        "    intr_df = io_utils.read_feather(scene_path / \"calibration\" / \"intrinsics.feather\")\n",
        "    extr_df = io_utils.read_feather(scene_path / \"calibration\" / \"egovehicle_SE3_sensor.feather\")\n",
        "    return ann_df, intr_df, extr_df\n",
        "\n",
        "ann_df, intr_df, extr_df = load_initial_data(SCENE_PATH)\n",
        "\n",
        "print(\"ann_df columns:\", ann_df.columns.tolist())\n",
        "print(\"intr_df columns:\", intr_df.columns.tolist())\n",
        "print(\"extr_df columns:\", extr_df.columns.tolist())\n",
        "\n",
        "print(\"cameras:\", intr_df[\"sensor_name\"].unique().tolist())\n",
        "print(\"timestamps in annotations:\", ann_df[\"timestamp_ns\"].nunique())\n",
        "print(\"unique tracks:\", ann_df[\"track_uuid\"].nunique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ncRFE9QX8N2",
        "outputId": "9f76ba0a-2784-42ff-8d62-f720ca500dcd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ann_df columns: ['timestamp_ns', 'track_uuid', 'category', 'length_m', 'width_m', 'height_m', 'qw', 'qx', 'qy', 'qz', 'tx_m', 'ty_m', 'tz_m', 'num_interior_pts']\n",
            "intr_df columns: ['sensor_name', 'fx_px', 'fy_px', 'cx_px', 'cy_px', 'k1', 'k2', 'k3', 'height_px', 'width_px']\n",
            "extr_df columns: ['sensor_name', 'qw', 'qx', 'qy', 'qz', 'tx_m', 'ty_m', 'tz_m']\n",
            "cameras: ['ring_front_center', 'ring_front_left', 'ring_front_right', 'ring_rear_left', 'ring_rear_right', 'ring_side_left', 'ring_side_right', 'stereo_front_left', 'stereo_front_right']\n",
            "timestamps in annotations: 157\n",
            "unique tracks: 89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 — Define the 3D Geometry / Rigid-Transform Utilities (Quaternion → Rotation, SE(3) Pose)\n",
        "\n",
        "\n",
        "In Step 2, I implement the core geometry tools that let us **move objects and points between coordinate frames** in Argoverse 2. This is necessary because AV2 provides object poses and sensor poses in **3D**, and we must correctly transform them into the **camera coordinate system** before any 2D projection is possible.\n",
        "\n",
        "---\n",
        "\n",
        "#### What I Define in This Step\n",
        "\n",
        "**1) `quat_to_rotmat()` — Quaternion to 3×3 Rotation Matrix**\n",
        "\n",
        "* AV2 stores rotations as quaternions `(qw, qx, qy, qz)` because they are numerically stable for 3D orientation.\n",
        "* For projection and frame transforms, we typically need a standard **3×3 rotation matrix**.\n",
        "* `quat_to_rotmat()` performs this conversion, giving us a usable rotation matrix **R**.\n",
        "\n",
        "**2) `SE3` dataclass — Full Rigid-Body Transform in 3D**\n",
        "\n",
        "* I define an `SE3` object to represent a complete **rigid transformation** in 3D:\n",
        "\n",
        "  * Rotation **R** (3×3)\n",
        "  * Translation **t** (3×1)\n",
        "* This is the standard mathematical representation used to map coordinates across frames (e.g., **ego → camera**, or **camera → ego**).\n",
        "\n",
        "---\n",
        "\n",
        "#### Key Methods Provided by `SE3`\n",
        "\n",
        "* **`as_matrix()`**\n",
        "  Builds the 4×4 homogeneous transform matrix (useful for debugging and consistent math operations).\n",
        "\n",
        "* **`inverse()`**\n",
        "  Computes the inverse pose. This is essential because AV2 sometimes provides a transform in the opposite direction of what we need (e.g., we may need **ego→camera** but are given **camera→ego**, or vice versa).\n",
        "\n",
        "* **`transform_points()`**\n",
        "  Applies the transform to a set of 3D points efficiently (e.g., cuboid corners).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUSZEPib2ltF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quat_to_rotmat(qw, qx, qy, qz) -> np.ndarray:\n",
        "    # Unit quaternion -> rotation matrix\n",
        "    q = np.array([qw, qx, qy, qz], dtype=np.float64)\n",
        "    q = q / np.linalg.norm(q)\n",
        "    w, x, y, z = q\n",
        "    R = np.array([\n",
        "        [1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
        "        [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
        "        [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)],\n",
        "    ], dtype=np.float64)\n",
        "    return R\n",
        "\n",
        "@dataclass\n",
        "class SE3:\n",
        "    R: np.ndarray  # 3x3\n",
        "    t: np.ndarray  # 3,\n",
        "\n",
        "    def as_matrix(self) -> np.ndarray:\n",
        "        T = np.eye(4, dtype=np.float64)\n",
        "        T[:3,:3] = self.R\n",
        "        T[:3, 3] = self.t\n",
        "        return T\n",
        "\n",
        "    def inverse(self) -> \"SE3\":\n",
        "        R_inv = self.R.T\n",
        "        t_inv = -R_inv @ self.t\n",
        "        return SE3(R_inv, t_inv)\n",
        "\n",
        "    def transform_points(self, pts: np.ndarray) -> np.ndarray:\n",
        "        # pts: (N,3)\n",
        "        return (pts @ self.R.T) + self.t.reshape(1,3)\n"
      ],
      "metadata": {
        "id": "Lx65bCDWYSlc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 — Build a Camera Intrinsics Lookup Table\n",
        "\n",
        "In this step, I convert the intrinsics table (`intr_df`) into a clean dictionary that we can query quickly during projection.\n",
        "\n",
        "* I define a small data structure **`CameraIntrinsics`** that stores the four core pinhole parameters: **fx, fy, cx, cy**.\n",
        "* In `build_intrinsics_dict()`, I first **validate** that the required columns exist in `intr_df`.\n",
        "* Then I iterate over each row and build a dictionary:\n",
        "\n",
        "[\n",
        "\\text{INTR}[camera_name] \\rightarrow (fx, fy, cx, cy)\n",
        "]\n",
        "\n",
        "So later, when I project 3D points into a specific camera image, I can directly do `INTR[\"ring_front_left\"]` (for example) instead of repeatedly searching the dataframe.\n",
        "\n",
        "**Output:** `INTR`, a dictionary mapping each camera name to its intrinsic parameters.\n"
      ],
      "metadata": {
        "id": "F3ortIGF3v_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class CameraIntrinsics:\n",
        "    fx: float\n",
        "    fy: float\n",
        "    cx: float\n",
        "    cy: float\n",
        "\n",
        "def build_intrinsics_dict(intr_df: pd.DataFrame) -> Dict[str, CameraIntrinsics]:\n",
        "    req = [\"sensor_name\", \"fx_px\", \"fy_px\", \"cx_px\", \"cy_px\"]\n",
        "    for c in req:\n",
        "        if c not in intr_df.columns:\n",
        "            raise ValueError(f\"Missing intrinsics column: {c}\")\n",
        "\n",
        "    intr = {}\n",
        "    for _, r in intr_df.iterrows():\n",
        "        intr[r[\"sensor_name\"]] = CameraIntrinsics(\n",
        "            fx=float(r[\"fx_px\"]), fy=float(r[\"fy_px\"]),\n",
        "            cx=float(r[\"cx_px\"]), cy=float(r[\"cy_px\"])\n",
        "        )\n",
        "    return intr\n",
        "\n",
        "INTR = build_intrinsics_dict(intr_df)\n"
      ],
      "metadata": {
        "id": "2NtwhdRBYYBi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 — Build Extrinsics Dictionary (Ego ↔ Camera Poses)\n",
        "\n",
        "This step converts the AV2 extrinsics table (`extr_df`) into usable **SE(3) transforms** for each camera, which is required to transform 3D annotations from the ego frame into each camera frame.\n",
        "\n",
        "* `build_extrinsics_dict()` loops over `extr_df` and builds a dictionary keyed by `sensor_name`.\n",
        "* It supports the common AV2 format where extrinsics are stored as:\n",
        "\n",
        "  * a **quaternion** (`qw,qx,qy,qz`) for rotation, and\n",
        "  * a **translation** (`tx_m,ty_m,tz_m`) in meters.\n",
        "    These are converted into an `SE3(R,t)` object per camera.\n",
        "* (As a fallback) it also supports a less common case where a full **4×4 transform matrix** is stored in a single column.\n",
        "\n",
        "**Outputs produced:**\n",
        "\n",
        "* `T_EGO_SENSOR`: a dictionary mapping each camera to an SE(3) transform as read from the file (we initially assume it represents **sensor → ego**).\n",
        "* `T_SENSOR_EGO`: the inverse transforms, computed as `inverse()`, which represent **ego → sensor**.\n",
        "\n",
        "These two directions matter because projection requires a consistent transform direction; later we use the correct one to move 3D cuboid points into the camera coordinate system before applying intrinsics.\n"
      ],
      "metadata": {
        "id": "1v7WNFOn5BJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_extrinsics_dict(extr_df: pd.DataFrame) -> Dict[str, SE3]:\n",
        "    # Try common naming conventions\n",
        "    if \"sensor_name\" not in extr_df.columns:\n",
        "        raise ValueError(\"extrinsics missing 'sensor_name'\")\n",
        "\n",
        "    # Case A: explicit quaternion + translation\n",
        "    quat_cols = [c for c in [\"qw\", \"qx\", \"qy\", \"qz\"] if c in extr_df.columns]\n",
        "    trans_cols = [c for c in [\"tx_m\", \"ty_m\", \"tz_m\"] if c in extr_df.columns]\n",
        "\n",
        "    extr = {}\n",
        "    for _, r in extr_df.iterrows():\n",
        "        name = r[\"sensor_name\"]\n",
        "\n",
        "        if len(quat_cols) == 4 and len(trans_cols) == 3:\n",
        "            R = quat_to_rotmat(float(r[\"qw\"]), float(r[\"qx\"]), float(r[\"qy\"]), float(r[\"qz\"]))\n",
        "            t = np.array([float(r[\"tx_m\"]), float(r[\"ty_m\"]), float(r[\"tz_m\"])], dtype=np.float64)\n",
        "            extr[name] = SE3(R=R, t=t)\n",
        "        else:\n",
        "            # Case B: flattened 4x4 matrix columns like \"T_egovehicle_sensor\" or similar\n",
        "            # Search for 16-number vector-ish columns\n",
        "            mat_col = None\n",
        "            for cand in [\"T_egovehicle_sensor\", \"egovehicle_SE3_sensor\", \"transform_matrix\"]:\n",
        "                if cand in extr_df.columns:\n",
        "                    mat_col = cand\n",
        "                    break\n",
        "            if mat_col is None:\n",
        "                raise ValueError(\n",
        "                    \"Extrinsics format not recognized. \"\n",
        "                    \"Expected qw/qx/qy/qz + tx_m/ty_m/tz_m OR a 4x4 matrix column.\"\n",
        "                )\n",
        "            T = np.array(r[mat_col], dtype=np.float64).reshape(4,4)\n",
        "            R, t = T[:3,:3], T[:3,3]\n",
        "            extr[name] = SE3(R=R, t=t)\n",
        "\n",
        "    return extr\n",
        "\n",
        "T_EGO_SENSOR = build_extrinsics_dict(extr_df)         # sensor -> ego (assumed)\n",
        "T_SENSOR_EGO = {k: v.inverse() for k, v in T_EGO_SENSOR.items()}  # ego -> sensor\n"
      ],
      "metadata": {
        "id": "Myv1TykKYYXb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 — Locate Camera Images and Build a Timestamp→File Index\n",
        "\n",
        "This step prepares the **raw image inputs** needed for training and redundancy analysis by indexing all camera frames in the selected AV2 scene.\n",
        "\n",
        "* First, `find_camera_root()` searches for the camera-image folder using common AV2 directory layouts (e.g., `.../sensors/cameras/...`). This makes the notebook robust to slight differences in how the dataset is stored.\n",
        "* Next, `parse_timestamp_from_filename()` extracts the timestamp from each image filename. AV2 camera frames are typically saved as files named by their `timestamp_ns` (e.g., `1234567890123456789.jpg`), which allows alignment with the annotation timestamps.\n",
        "* Then `index_images()` loops over all cameras and builds a dictionary:\n",
        "\n",
        "[\n",
        "\\text{IMG_INDEX}[camera][timestamp_ns] \\rightarrow \\text{path to image file}\n",
        "]\n",
        "\n",
        "* The printed output confirms how many frames were found per camera (about 319–320 frames each), which indicates the scene has a consistent multi-camera stream.\n",
        "\n",
        "**Output:** `IMG_INDEX`, a fast lookup structure that lets us retrieve the exact image file for a given camera and timestamp when generating 2D labels and computing redundancy.\n"
      ],
      "metadata": {
        "id": "ibw-KMzS3y6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_camera_root(scene_path: Path) -> Path:\n",
        "    # Common patterns across AV2 dumps\n",
        "    candidates = [\n",
        "        scene_path / \"sensors\" / \"cameras\",\n",
        "        scene_path / \"sensor\" / \"cameras\",\n",
        "        scene_path / \"cameras\",\n",
        "    ]\n",
        "    for p in candidates:\n",
        "        if p.exists():\n",
        "            return p\n",
        "    raise FileNotFoundError(f\"Cannot find camera root under {scene_path} (tried: {candidates})\")\n",
        "\n",
        "CAM_ROOT = find_camera_root(SCENE_PATH)\n",
        "\n",
        "def parse_timestamp_from_filename(p: Path) -> Optional[int]:\n",
        "    # expects something like \".../1234567890123456789.jpg\"\n",
        "    stem = p.stem\n",
        "    if stem.isdigit():\n",
        "        return int(stem)\n",
        "    return None\n",
        "\n",
        "def index_images(cam_root: Path, cameras: List[str]) -> Dict[str, Dict[int, Path]]:\n",
        "    idx = {}\n",
        "    for cam in cameras:\n",
        "        cam_dir = cam_root / cam\n",
        "        if not cam_dir.exists():\n",
        "            print(f\"[WARN] camera directory missing: {cam_dir}\")\n",
        "            continue\n",
        "        ts_map = {}\n",
        "        for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\"]:\n",
        "            for p in cam_dir.glob(ext):\n",
        "                ts = parse_timestamp_from_filename(p)\n",
        "                if ts is not None:\n",
        "                    ts_map[ts] = p\n",
        "        idx[cam] = ts_map\n",
        "        print(f\"{cam}: {len(ts_map)} images indexed\")\n",
        "    return idx\n",
        "\n",
        "CAMERAS = intr_df[\"sensor_name\"].unique().tolist()\n",
        "IMG_INDEX = index_images(CAM_ROOT, CAMERAS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik2XmqIYYasQ",
        "outputId": "3b9061a0-9048-4b02-8437-680344fec899"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ring_front_center: 319 images indexed\n",
            "ring_front_left: 319 images indexed\n",
            "ring_front_right: 319 images indexed\n",
            "ring_rear_left: 319 images indexed\n",
            "ring_rear_right: 319 images indexed\n",
            "ring_side_left: 320 images indexed\n",
            "ring_side_right: 320 images indexed\n",
            "stereo_front_left: 319 images indexed\n",
            "stereo_front_right: 319 images indexed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6 — Timestamp Alignment (Nearest-Neighbor Matching)\n",
        "\n",
        "This step defines how we align **annotation timestamps** with **camera image timestamps** when they do not match perfectly.\n",
        "\n",
        "* In AV2, object annotations are indexed by `timestamp_ns`, and each camera frame is also timestamped, but in practice they may be slightly offset due to sensor timing and logging.\n",
        "* `nearest_timestamp()` takes:\n",
        "\n",
        "  * `target`: the annotation timestamp we want to match,\n",
        "  * `available`: the list of image timestamps available for that camera,\n",
        "  * `max_diff_ns`: the maximum allowed mismatch (default **50 ms**).\n",
        "\n",
        "It then:\n",
        "\n",
        "1. finds the closest image timestamp using a binary search (`np.searchsorted`),\n",
        "2. checks the neighbor timestamps around that insertion point,\n",
        "3. returns the nearest one **only if** it is within the tolerance; otherwise it returns `None`.\n",
        "\n",
        "**Purpose:** ensure we only pair an annotation with an image when the time difference is small enough to be valid, which prevents incorrect 2D projections caused by using the wrong frame.\n"
      ],
      "metadata": {
        "id": "6bpbGeDc30Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_timestamp(target: int, available: List[int], max_diff_ns: int = 50_000_000) -> Optional[int]:\n",
        "    # max_diff_ns default = 50ms\n",
        "    if not available:\n",
        "        return None\n",
        "    # binary search\n",
        "    arr = np.array(available, dtype=np.int64)\n",
        "    i = int(np.searchsorted(arr, target))\n",
        "    cand = []\n",
        "    if i < len(arr): cand.append(arr[i])\n",
        "    if i > 0: cand.append(arr[i-1])\n",
        "    best = min(cand, key=lambda x: abs(int(x) - target))\n",
        "    if abs(int(best) - target) <= max_diff_ns:\n",
        "        return int(best)\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "2PaDqpzQYcah"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7 — Convert Each 3D Cuboid Annotation into 8 Corner Points (Ego Frame)\n",
        "\n",
        "This step takes one AV2 3D annotation row and converts it into the **8 corner points of the 3D cuboid in the ego-vehicle coordinate frame**. This is the key geometric object we later project into camera images to create 2D boxes and to measure redundancy at the instance level.\n",
        "\n",
        "**What the code is doing:**\n",
        "\n",
        "* **`get_col()`**: AV2 field names can vary slightly across versions or exports, so this helper searches for the first matching column name from a list of candidates (e.g., `tx_m` vs `center_x`). This makes the notebook more robust.\n",
        "\n",
        "* **`cuboid_corners_ego(row)`**:\n",
        "\n",
        "  1. Reads the cuboid **center position** from the annotation (in ego frame):\n",
        "\n",
        "     * `tx_m, ty_m, tz_m` → the 3D center of the object.\n",
        "  2. Reads the cuboid **size**:\n",
        "\n",
        "     * `length_m, width_m, height_m`\n",
        "  3. Reads the cuboid **orientation** as a quaternion `(qw,qx,qy,qz)` and converts it to a rotation matrix `R`.\n",
        "  4. Builds the **8 corners in a local cuboid coordinate system** centered at (0,0,0) using half-dimensions `(L/2, W/2, H/2)`.\n",
        "  5. Rotates these local corners by the object orientation and then shifts them by the object center, producing:\n",
        "\n",
        "[\n",
        "\\text{corners_ego} \\in \\mathbb{R}^{8 \\times 3}\n",
        "]\n",
        "\n",
        "**Output:** an `8×3` array of cuboid corners in the ego frame.\n",
        "This is essential because AV2 provides **3D cuboids**, not 2D bounding boxes—so we must generate the 3D geometry first before projecting into each camera.\n"
      ],
      "metadata": {
        "id": "ynQZzLxN6TYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_col(available_cols, candidates):\n",
        "    for c in candidates:\n",
        "        if c in available_cols:\n",
        "            return c\n",
        "    raise KeyError(f\"None of {candidates} found in columns.\")\n",
        "\n",
        "def cuboid_corners_ego(row: pd.Series) -> np.ndarray:\n",
        "    cols = set(row.index)\n",
        "\n",
        "    # Common AV2-ish names (adjust if your ann_df differs)\n",
        "    cx = float(row[get_col(cols, [\"center_x\", \"tx_m\", \"x\", \"translation_x\"])])\n",
        "    cy = float(row[get_col(cols, [\"center_y\", \"ty_m\", \"y\", \"translation_y\"])])\n",
        "    cz = float(row[get_col(cols, [\"center_z\", \"tz_m\", \"z\", \"translation_z\"])])\n",
        "\n",
        "    length = float(row[get_col(cols, [\"length_m\", \"length\"])])\n",
        "    width  = float(row[get_col(cols, [\"width_m\", \"width\"])])\n",
        "    height = float(row[get_col(cols, [\"height_m\", \"height\"])])\n",
        "\n",
        "    # orientation quaternion (ego)\n",
        "    qw = float(row[get_col(cols, [\"qw\", \"rotation_qw\"])])\n",
        "    qx = float(row[get_col(cols, [\"qx\", \"rotation_qx\"])])\n",
        "    qy = float(row[get_col(cols, [\"qy\", \"rotation_qy\"])])\n",
        "    qz = float(row[get_col(cols, [\"qz\", \"rotation_qz\"])])\n",
        "\n",
        "    R = quat_to_rotmat(qw, qx, qy, qz)\n",
        "    center = np.array([cx, cy, cz], dtype=np.float64)\n",
        "\n",
        "    # local cuboid corners around (0,0,0) with length along x, width along y, height along z (convention)\n",
        "    l2, w2, h2 = length/2, width/2, height/2\n",
        "    corners_local = np.array([\n",
        "        [ l2,  w2,  h2],\n",
        "        [ l2, -w2,  h2],\n",
        "        [-l2, -w2,  h2],\n",
        "        [-l2,  w2,  h2],\n",
        "        [ l2,  w2, -h2],\n",
        "        [ l2, -w2, -h2],\n",
        "        [-l2, -w2, -h2],\n",
        "        [-l2,  w2, -h2],\n",
        "    ], dtype=np.float64)\n",
        "\n",
        "    corners_ego = (corners_local @ R.T) + center.reshape(1,3)\n",
        "    return corners_ego\n"
      ],
      "metadata": {
        "id": "lqn4twfLYell"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8 — Project 3D Cuboids to 2D Boxes and Compute BCS (Instance Visibility Score)\n",
        "\n",
        "This step converts each **3D cuboid (ego-frame corners)** into a **2D bounding box in a specific camera image**, and it also computes the **BCS score**, which we use later for pruning.\n",
        "\n",
        "#### 8.1 Define output container: `Box2D`\n",
        "\n",
        "* `Box2D` stores the final 2D box coordinates `(xmin, ymin, xmax, ymax)` plus:\n",
        "* `bcs`: a visibility/validity score that measures how much of the box is actually inside the image.\n",
        "\n",
        "#### 8.2 `project_points_to_image()`: camera projection (pinhole model)\n",
        "\n",
        "* Input: 3D points in the **camera frame** `(x,y,z)`\n",
        "* Output: pixel coordinates `(u,v)` using intrinsics:\n",
        "\n",
        "[\n",
        "u = f_x \\cdot \\frac{x}{z} + c_x,\\quad v = f_y \\cdot \\frac{y}{z} + c_y\n",
        "]\n",
        "\n",
        "We keep `z` as well because we need depth filtering.\n",
        "\n",
        "#### 8.3 `bbox_and_bcs_from_cuboid()`: full pipeline for one object in one camera\n",
        "\n",
        "This function does four key things:\n",
        "\n",
        "1. **Ego → Camera transform**\n",
        "   We use the extrinsics transform (`T_sensor_ego`) to move cuboid corners from ego frame into the camera frame:\n",
        "\n",
        "   * `corners_cam = T_sensor_ego.transform_points(corners_ego)`\n",
        "\n",
        "2. **Depth filtering (valid projection only)**\n",
        "   If all corners are behind the camera (`z <= 0.1`), we drop the object.\n",
        "   Then we keep only the corners with positive depth (`z > 0.1`) to avoid unstable projections.\n",
        "\n",
        "3. **Compute the 2D bounding box**\n",
        "   We project corners → image pixels and take min/max over projected points:\n",
        "\n",
        "   * `xmin_full, xmax_full, ymin_full, ymax_full`\n",
        "     This produces the **full (unclipped) projected box**, which can extend beyond the image.\n",
        "\n",
        "4. **Clip to image + compute BCS**\n",
        "\n",
        "   * We clip the full box to image boundaries.\n",
        "   * Compute:\n",
        "\n",
        "     * `area_full`: area of the projected box before clipping\n",
        "     * `area_clip`: area after clipping into image bounds\n",
        "\n",
        "Then the **BCS** is:\n",
        "\n",
        "[\n",
        "\\text{BCS} = \\frac{\\text{area_clip}}{\\text{area_full}}\n",
        "]\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* **BCS ≈ 1.0** → object box is fully inside the image (good visibility)\n",
        "* **BCS small** → most of the box lies outside the image (partial visibility / edge cut)\n",
        "\n",
        "Finally, if the clipped box is basically empty (`area_clip <= 1.0`), we drop it.\n",
        "\n",
        "**Output:** either `None` (object not valid in that camera view), or a `Box2D` containing:\n",
        "\n",
        "* clipped 2D box coordinates\n",
        "* BCS score for later pruning decisions\n"
      ],
      "metadata": {
        "id": "cWIIl7lh6qDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Box2D:\n",
        "    xmin: float\n",
        "    ymin: float\n",
        "    xmax: float\n",
        "    ymax: float\n",
        "    bcs: float\n",
        "\n",
        "def project_points_to_image(pts_cam: np.ndarray, intr: CameraIntrinsics) -> np.ndarray:\n",
        "    # pts_cam: (N,3) camera frame; assume z-forward\n",
        "    x, y, z = pts_cam[:,0], pts_cam[:,1], pts_cam[:,2]\n",
        "    eps = 1e-9\n",
        "    u = intr.fx * (x / (z + eps)) + intr.cx\n",
        "    v = intr.fy * (y / (z + eps)) + intr.cy\n",
        "    return np.stack([u, v, z], axis=1)\n",
        "\n",
        "def bbox_and_bcs_from_cuboid(corners_ego: np.ndarray,\n",
        "                            cam: str,\n",
        "                            intr: CameraIntrinsics,\n",
        "                            T_sensor_ego: SE3,\n",
        "                            img_w: int,\n",
        "                            img_h: int) -> Optional[Box2D]:\n",
        "    # Transform corners ego -> camera(sensor)\n",
        "    corners_cam = T_sensor_ego.transform_points(corners_ego)\n",
        "\n",
        "    # Require some positive depth\n",
        "    if np.all(corners_cam[:,2] <= 0.1):\n",
        "        return None\n",
        "\n",
        "    uvz = project_points_to_image(corners_cam, intr)\n",
        "    u = uvz[:,0]\n",
        "    v = uvz[:,1]\n",
        "    z = uvz[:,2]\n",
        "\n",
        "    # Keep only corners with positive depth to avoid wild projections\n",
        "    valid = z > 0.1\n",
        "    if valid.sum() < 2:\n",
        "        return None\n",
        "\n",
        "    u_full = u[valid]\n",
        "    v_full = v[valid]\n",
        "\n",
        "    xmin_full, xmax_full = float(u_full.min()), float(u_full.max())\n",
        "    ymin_full, ymax_full = float(v_full.min()), float(v_full.max())\n",
        "\n",
        "    # full area can be off-image\n",
        "    full_w = max(0.0, xmax_full - xmin_full)\n",
        "    full_h = max(0.0, ymax_full - ymin_full)\n",
        "    area_full = full_w * full_h\n",
        "    if area_full <= 1e-6:\n",
        "        return None\n",
        "\n",
        "    # clip to image boundaries\n",
        "    xmin_clip = max(0.0, min(float(img_w - 1), xmin_full))\n",
        "    xmax_clip = max(0.0, min(float(img_w - 1), xmax_full))\n",
        "    ymin_clip = max(0.0, min(float(img_h - 1), ymin_full))\n",
        "    ymax_clip = max(0.0, min(float(img_h - 1), ymax_full))\n",
        "\n",
        "    clip_w = max(0.0, xmax_clip - xmin_clip)\n",
        "    clip_h = max(0.0, ymax_clip - ymin_clip)\n",
        "    area_clip = clip_w * clip_h\n",
        "\n",
        "    bcs = float(area_clip / area_full)  # Eq.(1) in the paper :contentReference[oaicite:4]{index=4}\n",
        "\n",
        "    # If it’s entirely outside after clipping, drop it\n",
        "    if area_clip <= 1.0:\n",
        "        return None\n",
        "\n",
        "    return Box2D(xmin=xmin_clip, ymin=ymin_clip, xmax=xmax_clip, ymax=ymax_clip, bcs=bcs)\n"
      ],
      "metadata": {
        "id": "o4L-PDaaYgPN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Step 9\n",
        "builds a geometric overlap map between cameras. For each camera, I estimate its viewing direction (yaw of the optical axis in ego frame) and its horizontal FOV from the intrinsics, then I compute pairwise overlap on the circular angle domain with proper wrap-around handling. The resulting top pairs—like the stereo front-left/right (~62°) and front-center with stereo (~47°)—are the exact camera pairs we use later to decide where redundancy pruning is applied.\n"
      ],
      "metadata": {
        "id": "PxA93uLeIAun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from typing import List, Tuple, Dict\n",
        "from PIL import Image\n",
        "\n",
        "def wrap_pi(a: float) -> float:\n",
        "    return float((a + np.pi) % (2*np.pi) - np.pi)\n",
        "\n",
        "def fov_segments(center: float, hfov: float):\n",
        "    a1 = wrap_pi(center - hfov/2)\n",
        "    a2 = wrap_pi(center + hfov/2)\n",
        "    if a1 <= a2:\n",
        "        return [(a1, a2)]\n",
        "    # wraps around -pi/pi\n",
        "    return [(a1, np.pi), (-np.pi, a2)]\n",
        "\n",
        "def seg_overlap(s1, s2) -> float:\n",
        "    left = max(s1[0], s2[0])\n",
        "    right = min(s1[1], s2[1])\n",
        "    return max(0.0, right - left)\n",
        "\n",
        "def circular_overlap(center1: float, hfov1: float, center2: float, hfov2: float) -> float:\n",
        "    segs1 = fov_segments(center1, hfov1)\n",
        "    segs2 = fov_segments(center2, hfov2)\n",
        "    ov = 0.0\n",
        "    for a in segs1:\n",
        "        for b in segs2:\n",
        "            ov += seg_overlap(a, b)\n",
        "    # cannot exceed the smaller hfov\n",
        "    return float(min(ov, min(hfov1, hfov2)))\n",
        "\n",
        "def camera_yaw_center_in_ego(cam: str, T_ego_sensor: SE3) -> float:\n",
        "    # assuming camera forward is +Z in camera frame\n",
        "    forward_cam = np.array([0.0, 0.0, 1.0], dtype=np.float64)\n",
        "    forward_ego = T_ego_sensor.R @ forward_cam\n",
        "    return float(np.arctan2(forward_ego[1], forward_ego[0]))\n",
        "\n",
        "def hfov_from_intrinsics(intr: CameraIntrinsics, img_w: int) -> float:\n",
        "    return float(2.0 * np.arctan(img_w / (2.0 * intr.fx)))\n",
        "\n",
        "def compute_overlap_pairs(cameras: List[str],\n",
        "                          INTR: Dict[str, CameraIntrinsics],\n",
        "                          T_EGO_SENSOR: Dict[str, SE3],\n",
        "                          IMG_INDEX: Dict[str, Dict[int, Path]],\n",
        "                          min_overlap_deg: float = 5.0) -> List[Tuple[str,str,float]]:\n",
        "\n",
        "    min_overlap = math.radians(min_overlap_deg)\n",
        "\n",
        "    # one image per camera to get W,H\n",
        "    cam_sizes = {}\n",
        "    for cam in cameras:\n",
        "        ts_map = IMG_INDEX.get(cam, {})\n",
        "        if not ts_map:\n",
        "            continue\n",
        "        any_path = next(iter(ts_map.values()))\n",
        "        with Image.open(any_path) as im:\n",
        "            cam_sizes[cam] = im.size  # (W,H)\n",
        "\n",
        "    cam_info = {}\n",
        "    for cam in cameras:\n",
        "        if cam not in cam_sizes or cam not in INTR or cam not in T_EGO_SENSOR:\n",
        "            continue\n",
        "        W, H = cam_sizes[cam]\n",
        "        yaw = camera_yaw_center_in_ego(cam, T_EGO_SENSOR[cam])\n",
        "        hfov = hfov_from_intrinsics(INTR[cam], W)\n",
        "        cam_info[cam] = (yaw, hfov)\n",
        "\n",
        "    cams = list(cam_info.keys())\n",
        "    pairs = []\n",
        "    for i in range(len(cams)):\n",
        "        for j in range(i+1, len(cams)):\n",
        "            c1, c2 = cams[i], cams[j]\n",
        "            yaw1, hfov1 = cam_info[c1]\n",
        "            yaw2, hfov2 = cam_info[c2]\n",
        "\n",
        "            ov = circular_overlap(yaw1, hfov1, yaw2, hfov2)\n",
        "\n",
        "            # HARD sanity: overlap must be <= pi\n",
        "            if ov > np.pi + 1e-6:\n",
        "                raise RuntimeError(f\"Impossible overlap > 180deg for {c1},{c2}: {ov} rad\")\n",
        "\n",
        "            if ov >= min_overlap:\n",
        "                pairs.append((c1, c2, ov))\n",
        "\n",
        "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "    return pairs\n",
        "\n",
        "# IMPORTANT: force recompute (don’t reuse cached OVERLAP_PAIRS)\n",
        "OVERLAP_PAIRS = compute_overlap_pairs(CAMERAS, INTR, T_EGO_SENSOR, IMG_INDEX, min_overlap_deg=5.0)\n",
        "\n",
        "print(\"Top overlap pairs (deg):\")\n",
        "for c1, c2, ov in OVERLAP_PAIRS[:15]:\n",
        "    print(c1, c2, ov * 180/np.pi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs0yyuW2Yhzh",
        "outputId": "6c4436af-b8b4-4239-d204-e5065c9f472e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top overlap pairs (deg):\n",
            "stereo_front_left stereo_front_right 62.11493808031442\n",
            "ring_front_center stereo_front_left 47.049396999969844\n",
            "ring_front_center stereo_front_right 47.049396999969844\n",
            "ring_front_left stereo_front_right 17.787091136414052\n",
            "ring_front_right stereo_front_left 17.66136077378026\n",
            "ring_front_left stereo_front_left 17.344141408992847\n",
            "ring_front_right stereo_front_right 17.243924899568892\n",
            "ring_front_center ring_front_right 9.831879144830882\n",
            "ring_front_center ring_front_left 9.6906460833863\n",
            "ring_rear_left ring_rear_right 8.741142569844872\n",
            "ring_rear_left ring_side_left 8.63792319760331\n",
            "ring_rear_right ring_side_right 8.448340829841815\n",
            "ring_front_right ring_side_right 8.441358960305164\n",
            "ring_front_left ring_side_left 8.415167332173757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "THe overlap results make sense and are consistent with the yaw + HFOV values\n",
        "\n",
        "* **Stereo L–R ~62°**: both cameras face ~0° with ~62° HFOV → almost full overlap.\n",
        "* **Ring front center–stereo ~47°**: ring_front_center HFOV is **47°**, so overlap is capped at **47°**.\n",
        "* **Ring front left/right with opposite/near stereo ~17–18°**: yaws are ~±45° apart, so you only get a small edge overlap.\n",
        "\n",
        "* **Other pairs ~8–10°**: neighboring ring cameras are farther apart in yaw, so overlap is smaller.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NF5tav6dSA7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deg(x):\n",
        "    return float(x * 180 / np.pi)\n",
        "\n",
        "for cam in CAMERAS:\n",
        "    if cam in INTR and cam in T_EGO_SENSOR:\n",
        "        W = next(iter(IMG_INDEX[cam].values()))\n",
        "        with Image.open(W) as im:\n",
        "            width = im.size[0]\n",
        "        yaw = camera_yaw_center_in_ego(cam, T_EGO_SENSOR[cam])\n",
        "        hfov = hfov_from_intrinsics(INTR[cam], width)\n",
        "        print(f\"{cam:20s} yaw={deg(yaw):7.2f}  hfov={deg(hfov):7.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "816txN9JRRvQ",
        "outputId": "c7db1a7a-22fd-433b-f769-a3009ba96d7b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ring_front_center    yaw=  -0.11  hfov=  47.05\n",
            "ring_front_left      yaw=  44.99  hfov=  62.52\n",
            "ring_front_right     yaw= -45.06  hfov=  62.52\n",
            "ring_rear_left       yaw= 153.00  hfov=  62.54\n",
            "ring_rear_right      yaw=-153.19  hfov=  62.57\n",
            "ring_side_left       yaw=  99.10  hfov=  62.54\n",
            "ring_side_right      yaw= -99.11  hfov=  62.47\n",
            "stereo_front_left    yaw=  -0.19  hfov=  62.53\n",
            "stereo_front_right   yaw=   0.24  hfov=  62.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from typing import List, Tuple, Dict, Any\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Helpers to be robust to different AV2 object APIs\n",
        "# ----------------------------\n",
        "def _get_R(T: Any) -> np.ndarray:\n",
        "    \"\"\"Return 3x3 rotation matrix from an SE3-like object.\"\"\"\n",
        "    if hasattr(T, \"R\"):\n",
        "        return np.asarray(T.R)\n",
        "    if hasattr(T, \"rotation\"):\n",
        "        return np.asarray(T.rotation)\n",
        "    if hasattr(T, \"rot\"):\n",
        "        return np.asarray(T.rot)\n",
        "    raise AttributeError(\"Cannot find rotation matrix on T (expected .R or .rotation or .rot)\")\n",
        "\n",
        "def _get_fx(intr: Any) -> float:\n",
        "    \"\"\"Return fx from a CameraIntrinsics-like object.\"\"\"\n",
        "    if hasattr(intr, \"fx\"):\n",
        "        return float(intr.fx)\n",
        "    if hasattr(intr, \"K\"):\n",
        "        K = np.asarray(intr.K)\n",
        "        return float(K[0, 0])\n",
        "    if hasattr(intr, \"intrinsic_matrix\"):\n",
        "        K = np.asarray(intr.intrinsic_matrix)\n",
        "        return float(K[0, 0])\n",
        "    raise AttributeError(\"Cannot find fx on intr (expected .fx or .K or .intrinsic_matrix)\")\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Angle utilities + overlap\n",
        "# ----------------------------\n",
        "def wrap_pi(a: float) -> float:\n",
        "    return float((a + np.pi) % (2 * np.pi) - np.pi)\n",
        "\n",
        "def fov_segments(center: float, hfov: float):\n",
        "    a1 = wrap_pi(center - hfov / 2)\n",
        "    a2 = wrap_pi(center + hfov / 2)\n",
        "    if a1 <= a2:\n",
        "        return [(a1, a2)]\n",
        "    # wraps around -pi/pi\n",
        "    return [(a1, np.pi), (-np.pi, a2)]\n",
        "\n",
        "def seg_overlap(s1, s2) -> float:\n",
        "    left = max(s1[0], s2[0])\n",
        "    right = min(s1[1], s2[1])\n",
        "    return max(0.0, right - left)\n",
        "\n",
        "def circular_overlap(center1: float, hfov1: float, center2: float, hfov2: float) -> float:\n",
        "    segs1 = fov_segments(center1, hfov1)\n",
        "    segs2 = fov_segments(center2, hfov2)\n",
        "    ov = 0.0\n",
        "    for a in segs1:\n",
        "        for b in segs2:\n",
        "            ov += seg_overlap(a, b)\n",
        "    # cap by the smaller HFOV\n",
        "    return float(min(ov, min(hfov1, hfov2)))\n",
        "\n",
        "def simple_overlap(center1: float, hfov1: float, center2: float, hfov2: float) -> float:\n",
        "    \"\"\"\n",
        "    Simple symmetric overlap (no wrap handling).\n",
        "    Uses angular separation on circle and overlap = max(0, (hfov1+hfov2)/2 - d) * 2? -> actually interval overlap:\n",
        "    overlap = max(0, hfov1/2 + hfov2/2 - d)\n",
        "    Then cap by min(hfov1, hfov2).\n",
        "    \"\"\"\n",
        "    # circular shortest distance between angles\n",
        "    d = abs(wrap_pi(center1 - center2))\n",
        "    ov = max(0.0, (hfov1 / 2) + (hfov2 / 2) - d)\n",
        "    return float(min(ov, min(hfov1, hfov2)))\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Camera yaw center + HFOV\n",
        "# ----------------------------\n",
        "# IMPORTANT: set this if AV2 camera forward axis differs.\n",
        "# Default assumes camera forward is +Z in camera frame.\n",
        "FORWARD_AXIS_CAM = np.array([0.0, 0.0, 1.0], dtype=np.float64)\n",
        "\n",
        "def camera_yaw_center_in_ego(T_ego_sensor: Any) -> float:\n",
        "    R = _get_R(T_ego_sensor)\n",
        "    forward_ego = R @ FORWARD_AXIS_CAM\n",
        "    return float(np.arctan2(forward_ego[1], forward_ego[0]))\n",
        "\n",
        "def hfov_from_intrinsics(intr: Any, img_w: int) -> float:\n",
        "    fx = _get_fx(intr)\n",
        "    return float(2.0 * np.arctan(img_w / (2.0 * fx)))\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Build cam_info and compute overlap pairs\n",
        "# ----------------------------\n",
        "def build_cam_info(\n",
        "    cameras: List[str],\n",
        "    INTR: Dict[str, Any],\n",
        "    T_EGO_SENSOR: Dict[str, Any],\n",
        "    IMG_INDEX: Dict[str, Dict[int, Path]],\n",
        "):\n",
        "    # one image per camera to get W,H\n",
        "    cam_sizes = {}\n",
        "    for cam in cameras:\n",
        "        ts_map = IMG_INDEX.get(cam, {})\n",
        "        if not ts_map:\n",
        "            continue\n",
        "        any_path = next(iter(ts_map.values()))\n",
        "        with Image.open(any_path) as im:\n",
        "            cam_sizes[cam] = im.size  # (W,H)\n",
        "\n",
        "    cam_info = {}\n",
        "    for cam in cameras:\n",
        "        if cam not in cam_sizes or cam not in INTR or cam not in T_EGO_SENSOR:\n",
        "            continue\n",
        "        W, H = cam_sizes[cam]\n",
        "        yaw = camera_yaw_center_in_ego(T_EGO_SENSOR[cam])\n",
        "        hfov = hfov_from_intrinsics(INTR[cam], W)\n",
        "        cam_info[cam] = (yaw, hfov, (W, H))\n",
        "\n",
        "    return cam_info\n",
        "\n",
        "def compute_overlap_pairs(\n",
        "    cam_info: Dict[str, Tuple[float, float, Tuple[int, int]]],\n",
        "    min_overlap_deg: float = 5.0\n",
        ") -> List[Tuple[str, str, float]]:\n",
        "    min_overlap = math.radians(min_overlap_deg)\n",
        "\n",
        "    cams = list(cam_info.keys())\n",
        "    pairs = []\n",
        "    for i in range(len(cams)):\n",
        "        for j in range(i + 1, len(cams)):\n",
        "            c1, c2 = cams[i], cams[j]\n",
        "            yaw1, hfov1, _ = cam_info[c1]\n",
        "            yaw2, hfov2, _ = cam_info[c2]\n",
        "\n",
        "            ov = circular_overlap(yaw1, hfov1, yaw2, hfov2)\n",
        "\n",
        "            # hard sanity: overlap must be <= 180deg\n",
        "            if ov > np.pi + 1e-6:\n",
        "                raise RuntimeError(f\"Impossible overlap >180deg for {c1},{c2}: {ov} rad\")\n",
        "\n",
        "            if ov >= min_overlap:\n",
        "                pairs.append((c1, c2, ov))\n",
        "\n",
        "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "    return pairs\n",
        "\n",
        "# ----------------------------\n",
        "# 4) RUN\n",
        "# ----------------------------\n",
        "# Assumes these exist in your notebook already:\n",
        "# CAMERAS, INTR, T_EGO_SENSOR, IMG_INDEX\n",
        "cam_info = build_cam_info(CAMERAS, INTR, T_EGO_SENSOR, IMG_INDEX)\n",
        "print(f\"Built cam_info for {len(cam_info)} cameras: {list(cam_info.keys())}\\n\")\n",
        "\n",
        "print(\"=== Camera yaw/hfov (deg) ===\")\n",
        "for cam, (yaw, hfov, (W, H)) in cam_info.items():\n",
        "    print(f\"{cam:17s} yaw={yaw*180/np.pi:8.2f}  hfov={hfov*180/np.pi:8.2f}  (W,H)=({W},{H})\")\n",
        "\n",
        "OVERLAP_PAIRS = compute_overlap_pairs(cam_info, min_overlap_deg=5.0)\n",
        "\n",
        "# Sanity check on top few pairs\n",
        "print(\"\\n=== Pair sanity check (circular vs simple overlap) ===\")\n",
        "for (c1, c2, ov) in OVERLAP_PAIRS[:6]:\n",
        "    yaw1, hfov1, _ = cam_info[c1]\n",
        "    yaw2, hfov2, _ = cam_info[c2]\n",
        "    dyaw = abs(wrap_pi(yaw1 - yaw2)) * 180 / np.pi\n",
        "    ov_c = circular_overlap(yaw1, hfov1, yaw2, hfov2) * 180 / np.pi\n",
        "    ov_s = simple_overlap(yaw1, hfov1, yaw2, hfov2) * 180 / np.pi\n",
        "    print(f\"\\nPair: {c1} vs {c2}\")\n",
        "    print(f\"  |Δyaw|=    {dyaw:6.2f}°\")\n",
        "    print(f\"  overlap(circular)= {ov_c:8.3f}°\")\n",
        "    print(f\"  overlap(simple)  = {ov_s:8.3f}°\")\n",
        "    print(f\"  diff             = {ov_c-ov_s:8.5f}°\")\n",
        "\n",
        "print(\"\\nTop overlap pairs (deg) recomputed:\")\n",
        "for c1, c2, ov in OVERLAP_PAIRS[:15]:\n",
        "    print(f\"{c1:17s} {c2:19s} {ov*180/np.pi:8.3f}°\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5vqanolUEXl",
        "outputId": "39d558bc-399c-4d8d-dffb-c8572b40d64a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built cam_info for 9 cameras: ['ring_front_center', 'ring_front_left', 'ring_front_right', 'ring_rear_left', 'ring_rear_right', 'ring_side_left', 'ring_side_right', 'stereo_front_left', 'stereo_front_right']\n",
            "\n",
            "=== Camera yaw/hfov (deg) ===\n",
            "ring_front_center yaw=   -0.11  hfov=   47.05  (W,H)=(1550,2048)\n",
            "ring_front_left   yaw=   44.99  hfov=   62.52  (W,H)=(2048,1550)\n",
            "ring_front_right  yaw=  -45.06  hfov=   62.52  (W,H)=(2048,1550)\n",
            "ring_rear_left    yaw=  153.00  hfov=   62.54  (W,H)=(2048,1550)\n",
            "ring_rear_right   yaw= -153.19  hfov=   62.57  (W,H)=(2048,1550)\n",
            "ring_side_left    yaw=   99.10  hfov=   62.54  (W,H)=(2048,1550)\n",
            "ring_side_right   yaw=  -99.11  hfov=   62.47  (W,H)=(2048,1550)\n",
            "stereo_front_left yaw=   -0.19  hfov=   62.53  (W,H)=(2048,1550)\n",
            "stereo_front_right yaw=    0.24  hfov=   62.56  (W,H)=(2048,1550)\n",
            "\n",
            "=== Pair sanity check (circular vs simple overlap) ===\n",
            "\n",
            "Pair: stereo_front_left vs stereo_front_right\n",
            "  |Δyaw|=      0.43°\n",
            "  overlap(circular)=   62.115°\n",
            "  overlap(simple)  =   62.115°\n",
            "  diff             =  0.00000°\n",
            "\n",
            "Pair: ring_front_center vs stereo_front_left\n",
            "  |Δyaw|=      0.09°\n",
            "  overlap(circular)=   47.049°\n",
            "  overlap(simple)  =   47.049°\n",
            "  diff             =  0.00000°\n",
            "\n",
            "Pair: ring_front_center vs stereo_front_right\n",
            "  |Δyaw|=      0.34°\n",
            "  overlap(circular)=   47.049°\n",
            "  overlap(simple)  =   47.049°\n",
            "  diff             =  0.00000°\n",
            "\n",
            "Pair: ring_front_left vs stereo_front_right\n",
            "  |Δyaw|=     44.75°\n",
            "  overlap(circular)=   17.787°\n",
            "  overlap(simple)  =   17.787°\n",
            "  diff             = -0.00000°\n",
            "\n",
            "Pair: ring_front_right vs stereo_front_left\n",
            "  |Δyaw|=     44.87°\n",
            "  overlap(circular)=   17.661°\n",
            "  overlap(simple)  =   17.661°\n",
            "  diff             = -0.00000°\n",
            "\n",
            "Pair: ring_front_left vs stereo_front_left\n",
            "  |Δyaw|=     45.18°\n",
            "  overlap(circular)=   17.344°\n",
            "  overlap(simple)  =   17.344°\n",
            "  diff             =  0.00000°\n",
            "\n",
            "Top overlap pairs (deg) recomputed:\n",
            "stereo_front_left stereo_front_right    62.115°\n",
            "ring_front_center stereo_front_left     47.049°\n",
            "ring_front_center stereo_front_right    47.049°\n",
            "ring_front_left   stereo_front_right    17.787°\n",
            "ring_front_right  stereo_front_left     17.661°\n",
            "ring_front_left   stereo_front_left     17.344°\n",
            "ring_front_right  stereo_front_right    17.244°\n",
            "ring_front_center ring_front_right       9.832°\n",
            "ring_front_center ring_front_left        9.691°\n",
            "ring_rear_left    ring_rear_right        8.741°\n",
            "ring_rear_left    ring_side_left         8.638°\n",
            "ring_rear_right   ring_side_right        8.448°\n",
            "ring_front_right  ring_side_right        8.441°\n",
            "ring_front_left   ring_side_left         8.415°\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 10 (Label schema for YOLO):**\n",
        "In the annotations, object categories are stored as text labels (e.g., PEDESTRIAN, BUS). Since YOLO requires numeric class IDs in the label files, I first identified the correct category column (category), then extracted the unique categories present in this scene, sorted them, and created a deterministic mapping from each category name to an integer ID. In this run there are 8 classes, mapped as: BOLLARD→0, BOX_TRUCK→1, BUS→2, CONSTRUCTION_CONE→3, LARGE_VEHICLE→4, PEDESTRIAN→5, REGULAR_VEHICLE→6, TRUCK→7. These IDs are what appear as the first number in each YOLO label line."
      ],
      "metadata": {
        "id": "U0pI3Aap-6Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect category column name first\n",
        "print(\"Potential category columns:\", [c for c in ann_df.columns if \"category\" in c.lower() or \"label\" in c.lower()])\n",
        "\n",
        "CATEGORY_COL = None\n",
        "for cand in [\"category\", \"category_name\", \"label\", \"class_name\"]:\n",
        "    if cand in ann_df.columns:\n",
        "        CATEGORY_COL = cand\n",
        "        break\n",
        "if CATEGORY_COL is None:\n",
        "    raise ValueError(\"Could not find a category column in ann_df. Update CATEGORY_COL manually.\")\n",
        "\n",
        "# Build mapping from observed categories\n",
        "cats = sorted(ann_df[CATEGORY_COL].dropna().unique().tolist())\n",
        "CLASS_MAP = {c:i for i,c in enumerate(cats)}\n",
        "NAMES = cats\n",
        "\n",
        "print(\"Num classes:\", len(NAMES))\n",
        "print(\"Example class map (first 10):\", list(CLASS_MAP.items())[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ7hZhY5Ypw5",
        "outputId": "50ad2b30-a363-45ca-f685-5c4a56b0d892"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Potential category columns: ['category']\n",
            "Num classes: 8\n",
            "Example class map (first 10): [('BOLLARD', 0), ('BOX_TRUCK', 1), ('BUS', 2), ('CONSTRUCTION_CONE', 3), ('LARGE_VEHICLE', 4), ('PEDESTRIAN', 5), ('REGULAR_VEHICLE', 6), ('TRUCK', 7)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#step 11-\n",
        "This cell converts each projected 2D bounding box into the exact YOLO label format. Given a box in pixel coordinates `(xmin, ymin, xmax, ymax)`, it computes the box **center** `(cx, cy)` and **size** `(w, h)`, then **normalizes** all four by the image width/height so they are in [0, 1]. Finally, it clamps values to [0, 1] for safety and returns one label line as:\n",
        "`<class_id> <cx> <cy> <w> <h>`\n",
        "with 6 decimal precision. This is what gets written into each `.txt` label file next to the image."
      ],
      "metadata": {
        "id": "2n-T57rYAZSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yolo_line_from_box(box: Box2D, cls_id: int, img_w: int, img_h: int) -> str:\n",
        "    # YOLO format: class cx cy w h (normalized)\n",
        "    cx = ((box.xmin + box.xmax) / 2.0) / img_w\n",
        "    cy = ((box.ymin + box.ymax) / 2.0) / img_h\n",
        "    w  = (box.xmax - box.xmin) / img_w\n",
        "    h  = (box.ymax - box.ymin) / img_h\n",
        "    cx, cy = float(cx), float(cy)\n",
        "    w, h = float(w), float(h)\n",
        "    # clamp lightly\n",
        "    cx = min(max(cx, 0.0), 1.0)\n",
        "    cy = min(max(cy, 0.0), 1.0)\n",
        "    w  = min(max(w, 0.0), 1.0)\n",
        "    h  = min(max(h, 0.0), 1.0)\n",
        "    return f\"{cls_id} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\"\n"
      ],
      "metadata": {
        "id": "_GE56zNOYtDd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 12\n",
        "exports AV2 into a YOLO dataset: it projects 3D cuboids to 2D boxes for each camera image, splits train/val by timestamp, then prunes redundant duplicate views across overlapping cameras using a BCS threshold `tau_bcs` (keep the higher-quality view).\n",
        " It removes redundancy by comparing overlapping camera pairs and, for any object that appears in both views, we keep only the higher-quality projection (higher BCS) when the BCS gap exceeds a threshold `tau_bcs`; otherwise we keep both.\n",
        "\n",
        "It saves `images/`, `labels/`, and a `data.yaml` ready for YOLOv8 training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OV6b8jn4XCfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_yolo_from_av2_scene(\n",
        "    scene_path: Path,\n",
        "    ann_df: pd.DataFrame,\n",
        "    INTR: Dict[str, CameraIntrinsics],\n",
        "    T_EGO_SENSOR: Dict[str, SE3],\n",
        "    T_SENSOR_EGO: Dict[str, SE3],\n",
        "    IMG_INDEX: Dict[str, Dict[int, Path]],\n",
        "    overlap_pairs: List[Tuple[str,str,float]],\n",
        "    out_root: Path,\n",
        "    tau_bcs: float,\n",
        "    train_ratio: float = 0.8,\n",
        "    seed: int = 7,\n",
        "    max_frames: Optional[int] = None,\n",
        "    drop_empty_images: bool = False,\n",
        "    max_ts_diff_ns: int = 50_000_000\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates a YOLO dataset with projected 2D boxes.\n",
        "    If tau_bcs is small -> more pruning (less redundancy).\n",
        "    If tau_bcs is large -> less pruning (more redundancy).\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    cameras = [c for c in INTR.keys() if c in IMG_INDEX and len(IMG_INDEX[c]) > 0]\n",
        "    if not cameras:\n",
        "        raise ValueError(\"No cameras with images found.\")\n",
        "\n",
        "    # Precompute one image size per camera\n",
        "    cam_size = {}\n",
        "    for cam in cameras:\n",
        "        any_path = next(iter(IMG_INDEX[cam].values()))\n",
        "        with Image.open(any_path) as im:\n",
        "            cam_size[cam] = im.size  # (W,H)\n",
        "\n",
        "    # Group annotations by timestamp\n",
        "    ts_groups = list(ann_df.groupby(\"timestamp_ns\"))\n",
        "    timestamps = [int(ts) for ts,_ in ts_groups]\n",
        "    if max_frames is not None:\n",
        "        timestamps = timestamps[:max_frames]\n",
        "\n",
        "    # train/val split by timestamp\n",
        "    timestamps = np.array(timestamps, dtype=np.int64)\n",
        "    rng.shuffle(timestamps)\n",
        "    n_train = int(len(timestamps) * train_ratio)\n",
        "    train_ts = set(map(int, timestamps[:n_train]))\n",
        "    val_ts   = set(map(int, timestamps[n_train:]))\n",
        "\n",
        "    # Prepare dirs\n",
        "    def reset_dir(p: Path):\n",
        "        if p.exists():\n",
        "            shutil.rmtree(p)\n",
        "        p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    dataset_dir = out_root\n",
        "    reset_dir(dataset_dir)\n",
        "\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        (dataset_dir / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
        "        (dataset_dir / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Fast lookup: timestamps available per camera\n",
        "    cam_ts_sorted = {cam: sorted(IMG_INDEX[cam].keys()) for cam in cameras}\n",
        "\n",
        "    # Helper: assign split\n",
        "    def which_split(ts: int) -> Optional[str]:\n",
        "        if ts in train_ts: return \"train\"\n",
        "        if ts in val_ts: return \"val\"\n",
        "        return None\n",
        "\n",
        "    # Iterate timestamps\n",
        "    kept_images = 0\n",
        "    dropped_images = 0\n",
        "\n",
        "    for ts in tqdm(sorted(list(train_ts | val_ts)), desc=f\"Exporting (tau_bcs={tau_bcs})\"):\n",
        "        split = which_split(ts)\n",
        "        if split is None:\n",
        "            continue\n",
        "\n",
        "        # Get annotations at ts\n",
        "        ann_rows = ann_df[ann_df[\"timestamp_ns\"] == ts]\n",
        "        if ann_rows.empty:\n",
        "            continue\n",
        "\n",
        "        # 1) Build per-camera label candidates (before pruning)\n",
        "        per_cam_boxes = {cam: {} for cam in cameras}\n",
        "        # structure: per_cam_boxes[cam][track_uuid] = (Box2D, class_id)\n",
        "\n",
        "        for _, row in ann_rows.iterrows():\n",
        "            track = row[\"track_uuid\"]\n",
        "            cat = row[CATEGORY_COL]\n",
        "            if pd.isna(cat):\n",
        "                continue\n",
        "            cls_id = CLASS_MAP[str(cat)]\n",
        "\n",
        "            corners = cuboid_corners_ego(row)\n",
        "\n",
        "            for cam in cameras:\n",
        "                # match nearest image timestamp for that camera\n",
        "                ts_img = nearest_timestamp(ts, cam_ts_sorted[cam], max_diff_ns=max_ts_diff_ns)\n",
        "                if ts_img is None:\n",
        "                    continue\n",
        "                img_path = IMG_INDEX[cam][ts_img]\n",
        "                W,H = cam_size[cam]\n",
        "\n",
        "                box = bbox_and_bcs_from_cuboid(\n",
        "                    corners_ego=corners,\n",
        "                    cam=cam,\n",
        "                    intr=INTR[cam],\n",
        "                    T_sensor_ego=T_SENSOR_EGO[cam],  # ego -> camera\n",
        "                    img_w=W,\n",
        "                    img_h=H\n",
        "                )\n",
        "                if box is None:\n",
        "                    continue\n",
        "                per_cam_boxes[cam][track] = (box, cls_id, ts_img, img_path, W, H)\n",
        "\n",
        "        # 2) Apply redundancy pruning across overlap pairs using BCS rule (Eq.2)\n",
        "        # For each overlap pair, if same track appears in both cams, compare BCS.\n",
        "        to_drop = set()  # entries identified by (cam, track, ts_img)\n",
        "\n",
        "        for camA, camB, _ in overlap_pairs:\n",
        "            if camA not in per_cam_boxes or camB not in per_cam_boxes:\n",
        "                continue\n",
        "            common_tracks = set(per_cam_boxes[camA].keys()) & set(per_cam_boxes[camB].keys())\n",
        "            for track in common_tracks:\n",
        "                boxA, clsA, tsA, _, _, _ = per_cam_boxes[camA][track]\n",
        "                boxB, clsB, tsB, _, _, _ = per_cam_boxes[camB][track]\n",
        "\n",
        "                # (sanity) if categories differ, skip pruning for this object\n",
        "                if clsA != clsB:\n",
        "                    continue\n",
        "\n",
        "                bcs_vals = [boxA.bcs, boxB.bcs]\n",
        "                if (max(bcs_vals) - min(bcs_vals)) > tau_bcs:\n",
        "                    # keep higher BCS only, drop lower :contentReference[oaicite:9]{index=9}\n",
        "                    if boxA.bcs >= boxB.bcs:\n",
        "                        to_drop.add((camB, track, tsB))\n",
        "                    else:\n",
        "                        to_drop.add((camA, track, tsA))\n",
        "\n",
        "        # 3) Write images + labels (post-pruning)\n",
        "        for cam in cameras:\n",
        "            # Group by ts_img because nearest_timestamp may create slightly different image timestamps across cams\n",
        "            # We'll write one file per (cam, ts_img).\n",
        "            # Collect all tracks for this cam at this base annotation timestamp.\n",
        "            entries = list(per_cam_boxes[cam].items())\n",
        "            if not entries:\n",
        "                continue\n",
        "\n",
        "            # We might have multiple tracks but same ts_img (likely)\n",
        "            # Use the first entry’s ts_img/img_path\n",
        "            _, (_, _, ts_img, img_path, W, H) = entries[0]\n",
        "            out_img_name = f\"{cam}_{ts_img}.jpg\"\n",
        "            out_lbl_name = f\"{cam}_{ts_img}.txt\"\n",
        "\n",
        "            # Build label lines\n",
        "            lines = []\n",
        "            for track, (box, cls_id, ts_img2, img_path2, W2, H2) in per_cam_boxes[cam].items():\n",
        "                key = (cam, track, ts_img2)\n",
        "                if key in to_drop:\n",
        "                    continue\n",
        "                lines.append(yolo_line_from_box(box, cls_id, W2, H2))\n",
        "\n",
        "            # Optionally drop empty images\n",
        "            if drop_empty_images and len(lines) == 0:\n",
        "                dropped_images += 1\n",
        "                continue\n",
        "\n",
        "            # Copy image\n",
        "            dst_img = dataset_dir / \"images\" / split / out_img_name\n",
        "            shutil.copy(img_path, dst_img)\n",
        "\n",
        "            # Write label file (empty allowed)\n",
        "            dst_lbl = dataset_dir / \"labels\" / split / out_lbl_name\n",
        "            with open(dst_lbl, \"w\") as f:\n",
        "                f.write(\"\\n\".join(lines))\n",
        "\n",
        "            kept_images += 1\n",
        "\n",
        "    # 4) Write data.yaml\n",
        "    data_yaml = dataset_dir / \"data.yaml\"\n",
        "    yaml_text = (\n",
        "        f\"path: {dataset_dir}\\n\"\n",
        "        f\"train: images/train\\n\"\n",
        "        f\"val: images/val\\n\"\n",
        "        f\"nc: {len(NAMES)}\\n\"\n",
        "        f\"names: {json.dumps(NAMES)}\\n\"\n",
        "    )\n",
        "    with open(data_yaml, \"w\") as f:\n",
        "        f.write(yaml_text)\n",
        "\n",
        "    print(f\"Done. kept_images={kept_images}, dropped_images={dropped_images}\")\n",
        "    print(\"data.yaml:\", data_yaml)\n",
        "    return dataset_dir\n"
      ],
      "metadata": {
        "id": "ZGDT4RU0YuxU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 13\n",
        "\n",
        "In this cell, I generate **two YOLO datasets** from the same AV2 scene so we can compare **no-pruning vs. redundancy-pruning** under identical conditions. Both use the same camera calibrations, timestamp-based train/val split (80/20), and the same overlap camera-pairs list. The only change is the pruning threshold `tau_bcs`:\n",
        "\n",
        "* **Baseline (`tau_bcs = 1.0`)**: pruning is very mild, so it behaves close to ‘keep everything’.\n",
        "* **Pruned (`tau_bcs = 0.2`)**: pruning is stricter; when the same object appears in two overlapping cameras and their BCS differs enough, we drop the lower-quality duplicate view.\n",
        "\n",
        "The output confirms both datasets were exported successfully (157 timestamps processed) and it prints where the `data.yaml` files are saved for YOLO training.”\n",
        "\n",
        "### What the output is telling\n",
        "\n",
        "* `kept_images=1413` for both runs means **the number of exported image files ended up the same** in baseline and pruned.\n",
        "* `dropped_images=0` means **no entire images were removed** (because `drop_empty_images=False`, and even if some images had zero labels they are still kept).\n",
        "\n",
        "So: pruning may still have happened at the **box level** (fewer labels per image), but not at the **image-file level**—that’s why these counters didn’t change.\n"
      ],
      "metadata": {
        "id": "_7xOtU90BPkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_ROOT = Path(\"/content/drive/MyDrive/av2_redundancy_yolo\")\n",
        "\n",
        "baseline_dir = build_yolo_from_av2_scene(\n",
        "    scene_path=SCENE_PATH,\n",
        "    ann_df=ann_df,\n",
        "    INTR=INTR,\n",
        "    T_EGO_SENSOR=T_EGO_SENSOR,\n",
        "    T_SENSOR_EGO=T_SENSOR_EGO,\n",
        "    IMG_INDEX=IMG_INDEX,\n",
        "    overlap_pairs=OVERLAP_PAIRS,\n",
        "    out_root=OUT_ROOT / \"baseline_tau1.0\",\n",
        "    tau_bcs=1.0,\n",
        "    train_ratio=0.8,\n",
        "    max_frames=None,\n",
        "    drop_empty_images=False\n",
        ")\n",
        "\n",
        "pruned_dir = build_yolo_from_av2_scene(\n",
        "    scene_path=SCENE_PATH,\n",
        "    ann_df=ann_df,\n",
        "    INTR=INTR,\n",
        "    T_EGO_SENSOR=T_EGO_SENSOR,\n",
        "    T_SENSOR_EGO=T_SENSOR_EGO,\n",
        "    IMG_INDEX=IMG_INDEX,\n",
        "    overlap_pairs=OVERLAP_PAIRS,\n",
        "    out_root=OUT_ROOT / \"pruned_tau0.2\",\n",
        "    tau_bcs=0.2,\n",
        "    train_ratio=0.8,\n",
        "    max_frames=None,\n",
        "    drop_empty_images=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvi0ZOWTYwVb",
        "outputId": "8d4f7a19-cbcd-4ae9-fd34-fa94d5ed2118"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exporting (tau_bcs=1.0): 100%|██████████| 157/157 [00:29<00:00,  5.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. kept_images=1413, dropped_images=0\n",
            "data.yaml: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/data.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exporting (tau_bcs=0.2): 100%|██████████| 157/157 [00:30<00:00,  5.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. kept_images=1413, dropped_images=0\n",
            "data.yaml: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/data.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 14: Train + Evaluate YOLO\n",
        "\n",
        "### Goal of Step 14\n",
        "\n",
        "In this step, we **train YOLOv8** on each exported dataset (baseline vs pruned) and then **evaluate performance** to see whether redundancy pruning improves detection quality.\n",
        "\n",
        "### What the Step 14 function does (`train_and_eval_yolo`)\n",
        "\n",
        "For each dataset directory (`baseline_dir` or `pruned_dir`), the function runs the same pipeline:\n",
        "\n",
        "1. **Initialize the same model**\n",
        "\n",
        "* `model = YOLO(\"yolov8n.pt\")`\n",
        "  So both experiments start from the same pretrained YOLOv8n weights.\n",
        "\n",
        "2. **Train the model**\n",
        "\n",
        "* `model.train(...)` is called with:\n",
        "\n",
        "  * `data = data.yaml` (YOLO format dataset created in Step 13)\n",
        "  * `epochs=30`, `imgsz=640`, `batch=16`\n",
        "  * `device=0` (GPU)\n",
        "  * `cache=True`, `workers=4`\n",
        "  * Outputs saved under: `data_dir / \"runs\" / run_name`\n",
        "\n",
        "This ensures the only real difference between the two trainings is the **dataset content** (baseline vs pruned labels).\n",
        "\n",
        "3. **Validate the model**\n",
        "   After training finishes, the function runs:\n",
        "\n",
        "* `metrics = model.val(data=data.yaml, device=0)`\n",
        "\n",
        "From YOLO validation it extracts:\n",
        "\n",
        "* **Precision** = `metrics.box.mp`\n",
        "* **Recall** = `metrics.box.mr`\n",
        "* **mAP50** = `metrics.box.map50`\n",
        "* **mAP50-95** = `metrics.box.map`\n",
        "\n",
        "Then it computes:\n",
        "\n",
        "* **F1-score** = `2PR/(P+R)`\n",
        "\n",
        "4. **Pull final training box loss**\n",
        "   The function reads:\n",
        "\n",
        "* `results.csv` from the training run directory\n",
        "  and extracts the last available value of `train/box_loss` (if present) as a sanity check on training stability.\n",
        "\n",
        "5. **Return a compact metrics dictionary**\n",
        "   So we can store results for baseline and pruned in a clean table.\n",
        "\n",
        "---\n",
        "\n",
        "## What Step 14 produced\n",
        "\n",
        "### Baseline training + eval (tau=1.0)\n",
        "\n",
        "* Precision: **0.942**\n",
        "* Recall: **0.753**\n",
        "* F1: **0.837**\n",
        "* mAP50: **0.862**\n",
        "* mAP50-95: **0.722**\n",
        "* Box loss: **0.578**\n",
        "\n",
        "### Pruned training + eval (tau=0.2)\n",
        "\n",
        "* Precision: **0.948**\n",
        "* Recall: **0.829**\n",
        "* F1: **0.885**\n",
        "* mAP50: **0.888**\n",
        "* mAP50-95: **0.743**\n",
        "* Box loss: **0.579**\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "* Step 14 shows that **training on pruned labels improved performance** overall.\n",
        "* The **largest gain is Recall** (0.753 → 0.829), meaning the pruned-label model is detecting more true objects.\n",
        "* **mAP50 and mAP50-95 improved**, indicating better overall detection quality across IoU thresholds.\n",
        "* **Box loss stayed almost the same**, so the training behavior remains stable; performance gains are not coming from an unstable training run.\n",
        "\n"
      ],
      "metadata": {
        "id": "FNm6mN6YB-Iv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Threshold = 0.2**"
      ],
      "metadata": {
        "id": "EyEvxRkWlVpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval_yolo(data_dir: Path, run_name: str, epochs: int = 30, imgsz: int = 640, batch: int = 16):\n",
        "    model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "    train_res = model.train(\n",
        "        data=str(data_dir / \"data.yaml\"),\n",
        "        epochs=epochs,\n",
        "        imgsz=imgsz,\n",
        "        batch=batch,\n",
        "        device=0,          # force GPU\n",
        "        cache=True,        # recommended\n",
        "        workers=4,         # recommended\n",
        "        name=run_name,\n",
        "        project=str(data_dir / \"runs\"),\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    metrics = model.val(\n",
        "        data=str(data_dir / \"data.yaml\"),\n",
        "        device=0           # force GPU for eval too\n",
        "    )\n",
        "\n",
        "    precision = float(metrics.box.mp)\n",
        "    recall    = float(metrics.box.mr)\n",
        "    map50     = float(metrics.box.map50)\n",
        "    map5095   = float(metrics.box.map)\n",
        "    f1        = (2 * precision * recall / (precision + recall + 1e-12))\n",
        "\n",
        "    box_loss = None\n",
        "    results_csv = Path(train_res.save_dir) / \"results.csv\"\n",
        "    if results_csv.exists():\n",
        "        df = pd.read_csv(results_csv)\n",
        "        box_cols = [c for c in df.columns if \"train/box_loss\" in c.lower()]\n",
        "        if box_cols:\n",
        "            box_loss = float(df[box_cols[0]].iloc[-1])\n",
        "\n",
        "    return {\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1,\n",
        "        \"mAP50\": map50,\n",
        "        \"mAP50-95\": map5095,\n",
        "        \"Box Loss\": box_loss\n",
        "    }\n"
      ],
      "metadata": {
        "id": "UJu6Lpp8Y1FZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "base_metrics = train_and_eval_yolo(baseline_dir, run_name=\"baseline_tau1.0\", epochs=30, imgsz=640, batch=16)\n",
        "rows.append({\n",
        "    \"Model Strategy\": \"Baseline (no pruning)\",\n",
        "    \"Dataset (Images)\": \"AV2\",\n",
        "    **base_metrics\n",
        "})\n",
        "\n",
        "pruned_metrics = train_and_eval_yolo(pruned_dir, run_name=\"pruned_tau0.2\", epochs=30, imgsz=640, batch=16)\n",
        "rows.append({\n",
        "    \"Model Strategy\": \"Pruned (tau_BCS=0.2)\",\n",
        "    \"Dataset (Images)\": \"AV2\",\n",
        "    **pruned_metrics\n",
        "})\n",
        "\n",
        "report_df = pd.DataFrame(rows, columns=[\n",
        "    \"Model Strategy\", \"Dataset (Images)\", \"Precision\", \"Recall\", \"F1-Score\", \"mAP50\", \"mAP50-95\", \"Box Loss\"\n",
        "])\n",
        "\n",
        "report_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JgDzLNEKadyK",
        "outputId": "7d13f395-a7d5-4c72-e995-885a0d098384"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=baseline_tau1.0, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.5±0.1 ms, read: 99.4±23.6 MB/s, size: 280.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/train... 1125 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1125/1125 99.2it/s 11.3s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/train.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB RAM): 100% ━━━━━━━━━━━━ 1125/1125 261.6it/s 4.3s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.9±1.0 ms, read: 68.2±61.0 MB/s, size: 297.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val... 288 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 228.3it/s 1.3s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB RAM): 100% ━━━━━━━━━━━━ 288/288 257.1it/s 1.1s\n",
            "Plotting labels to /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      2.65G      1.686      2.489      1.252        125        640: 100% ━━━━━━━━━━━━ 71/71 8.4it/s 8.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 6.6it/s 1.4s\n",
            "                   all        288       3981      0.954       0.23      0.467      0.314\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      2.68G      1.342      1.258      1.109        190        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 6.8it/s 1.3s\n",
            "                   all        288       3981      0.829      0.533      0.608      0.387\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30       2.7G       1.23      1.075       1.06        130        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.4it/s 1.2s\n",
            "                   all        288       3981      0.868      0.588      0.653      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      2.71G      1.132     0.9759      1.013        145        640: 100% ━━━━━━━━━━━━ 71/71 11.8it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.4it/s 1.2s\n",
            "                   all        288       3981      0.918      0.628      0.688      0.526\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      2.72G       1.06     0.8974     0.9953         62        640: 100% ━━━━━━━━━━━━ 71/71 11.9it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.3it/s 1.2s\n",
            "                   all        288       3981      0.929      0.662      0.715      0.553\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      2.75G      1.003     0.8299     0.9734         93        640: 100% ━━━━━━━━━━━━ 71/71 11.7it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.4it/s 1.2s\n",
            "                   all        288       3981      0.906      0.686      0.725      0.562\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      2.77G     0.9634     0.7933     0.9631         81        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.929      0.687      0.741      0.578\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      2.78G     0.9364     0.7497     0.9478        113        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.1s\n",
            "                   all        288       3981      0.913      0.703      0.746      0.595\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30      2.79G      0.895     0.7104     0.9351        144        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.929      0.705      0.751      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      2.82G     0.8674     0.6819     0.9257         95        640: 100% ━━━━━━━━━━━━ 71/71 12.0it/s 5.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.962      0.697      0.759      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      2.84G      0.845     0.6621     0.9203        110        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.957      0.701      0.762      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      2.85G     0.8207     0.6417     0.9149        107        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.917      0.717      0.767      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      2.86G     0.8084     0.6303     0.9112         77        640: 100% ━━━━━━━━━━━━ 71/71 12.0it/s 5.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981       0.92       0.71      0.762      0.619\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      2.89G     0.7931     0.6187     0.9099        107        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.934      0.719      0.767      0.632\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      3.11G     0.7913     0.6129     0.9131        142        640: 100% ━━━━━━━━━━━━ 71/71 11.8it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.804      0.723      0.774       0.64\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      3.13G     0.7618     0.5831      0.896        137        640: 100% ━━━━━━━━━━━━ 71/71 11.9it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.1s\n",
            "                   all        288       3981      0.808      0.723      0.774      0.637\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30      3.14G     0.7565      0.572     0.8967         99        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3981      0.808      0.733      0.776      0.644\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      3.16G     0.7349     0.5604     0.8942        152        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.3it/s 1.1s\n",
            "                   all        288       3981      0.825      0.731      0.772      0.645\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      3.18G     0.7234     0.5488     0.8833        143        640: 100% ━━━━━━━━━━━━ 71/71 11.9it/s 5.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3981      0.811       0.74      0.778      0.655\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30       3.2G     0.7182     0.5458     0.8883        104        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.826      0.738      0.782      0.658\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      3.21G     0.7025      0.543     0.8762         73        640: 100% ━━━━━━━━━━━━ 71/71 10.5it/s 6.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3981      0.947      0.735      0.774      0.647\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      3.23G     0.6854     0.5209     0.8652         80        640: 100% ━━━━━━━━━━━━ 71/71 11.7it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.944      0.737      0.851      0.681\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      3.25G     0.6604      0.504     0.8577         63        640: 100% ━━━━━━━━━━━━ 71/71 12.3it/s 5.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.942      0.732       0.87      0.691\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      3.27G     0.6445     0.4915     0.8555         50        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3981       0.95      0.741      0.862      0.701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      3.27G     0.6311      0.481     0.8521         61        640: 100% ━━━━━━━━━━━━ 71/71 12.0it/s 5.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981      0.954      0.739      0.862      0.692\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30       3.3G     0.6235     0.4742     0.8499         44        640: 100% ━━━━━━━━━━━━ 71/71 11.9it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.4it/s 1.1s\n",
            "                   all        288       3981      0.944      0.749      0.864      0.711\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30      3.32G       0.61     0.4632     0.8497         50        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3981      0.949      0.744      0.862      0.713\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30      3.33G     0.6008     0.4605     0.8484         58        640: 100% ━━━━━━━━━━━━ 71/71 11.8it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3981      0.946      0.756      0.865      0.717\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30      3.34G     0.5856     0.4494     0.8408         47        640: 100% ━━━━━━━━━━━━ 71/71 11.8it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.4it/s 1.1s\n",
            "                   all        288       3981      0.939      0.754      0.861      0.715\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      3.37G     0.5784     0.4484     0.8409         17        640: 100% ━━━━━━━━━━━━ 71/71 12.0it/s 5.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3981      0.942      0.752      0.867      0.723\n",
            "\n",
            "30 epochs completed in 0.064 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/best.pt...\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 3.4it/s 2.6s\n",
            "                   all        288       3981      0.942      0.752      0.867      0.723\n",
            "               BOLLARD         11         11          1          0      0.657      0.321\n",
            "             BOX_TRUCK         60         60      0.878      0.838      0.866      0.691\n",
            "                   BUS         96         96      0.994          1      0.995       0.96\n",
            "     CONSTRUCTION_CONE         32         94      0.982      0.979      0.988      0.919\n",
            "         LARGE_VEHICLE        104        252      0.959      0.897       0.94      0.806\n",
            "            PEDESTRIAN        248       1402      0.876      0.595      0.683      0.512\n",
            "       REGULAR_VEHICLE        237       1971      0.886      0.719      0.816      0.622\n",
            "                 TRUCK         95         95      0.961      0.989      0.993      0.955\n",
            "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 4.5ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 133.9±98.7 MB/s, size: 305.9 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val.cache... 288 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 395.0Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 18/18 4.4it/s 4.1s\n",
            "                   all        288       3981      0.942      0.753      0.862      0.722\n",
            "               BOLLARD         11         11          1          0      0.617      0.306\n",
            "             BOX_TRUCK         60         60      0.878      0.839      0.867      0.691\n",
            "                   BUS         96         96      0.994          1      0.995      0.966\n",
            "     CONSTRUCTION_CONE         32         94      0.984      0.979      0.988      0.912\n",
            "         LARGE_VEHICLE        104        252      0.959      0.897      0.939      0.803\n",
            "            PEDESTRIAN        248       1402      0.875      0.596      0.683      0.515\n",
            "       REGULAR_VEHICLE        237       1971      0.884       0.72      0.817      0.626\n",
            "                 TRUCK         95         95      0.961      0.989      0.993      0.954\n",
            "Speed: 1.1ms preprocess, 1.2ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val18\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=pruned_tau0.2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/runs/pruned_tau0.2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 151.1±41.2 MB/s, size: 321.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/labels/train... 1125 images, 13 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1125/1125 115.8it/s 9.7s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/labels/train.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB RAM): 100% ━━━━━━━━━━━━ 1125/1125 285.6it/s 3.9s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 151.5±39.6 MB/s, size: 257.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/labels/val... 288 images, 5 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 169.5it/s 1.7s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/labels/val.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB RAM): 100% ━━━━━━━━━━━━ 288/288 273.3it/s 1.1s\n",
            "Plotting labels to /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/runs/pruned_tau0.2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/runs/pruned_tau0.2\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      2.64G      1.713      2.493      1.233        114        640: 100% ━━━━━━━━━━━━ 71/71 8.8it/s 8.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.1it/s 1.3s\n",
            "                   all        288       3693      0.955      0.212      0.474      0.322\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      2.64G      1.359       1.27      1.093        180        640: 100% ━━━━━━━━━━━━ 71/71 10.5it/s 6.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.1it/s 1.3s\n",
            "                   all        288       3693      0.768      0.502      0.573      0.394\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      2.64G      1.248      1.092      1.048        119        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.3it/s 1.2s\n",
            "                   all        288       3693      0.881      0.603      0.658      0.474\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      2.64G      1.128     0.9825     0.9974        137        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.1it/s 1.3s\n",
            "                   all        288       3693      0.891      0.628      0.701      0.536\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      2.64G      1.067     0.9062     0.9812         57        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.4it/s 1.2s\n",
            "                   all        288       3693      0.886      0.652      0.705      0.549\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      2.65G      1.014     0.8407     0.9628         83        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3693      0.937      0.665      0.723      0.558\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      2.67G      0.981        0.8     0.9533         76        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3693      0.972      0.654      0.735      0.582\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      2.68G     0.9533     0.7585     0.9384        105        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.1s\n",
            "                   all        288       3693      0.956       0.66      0.747       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30      2.89G     0.8889     0.7117     0.9188        134        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.1s\n",
            "                   all        288       3693      0.908      0.708       0.75      0.592\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      2.92G     0.8678     0.6917     0.9144         88        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3693      0.949      0.673      0.756      0.603\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      2.93G      0.857     0.6761     0.9118        104        640: 100% ━━━━━━━━━━━━ 71/71 11.8it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3693      0.913      0.693      0.753      0.626\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      2.95G     0.8361     0.6599      0.908        101        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3693        0.9      0.706      0.752      0.611\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      2.96G     0.8117     0.6324     0.9008         68        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3693      0.907      0.712      0.758      0.627\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      2.98G     0.7988     0.6235     0.8981         97        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3693      0.907      0.735      0.772       0.64\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30       3.2G     0.7964     0.6181     0.8998        139        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3693      0.903      0.738      0.771      0.636\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      3.22G     0.7626     0.5869     0.8835        134        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3693      0.925      0.731      0.801      0.658\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30      3.23G     0.7604     0.5798     0.8872         92        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3693      0.948      0.713      0.819      0.667\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      3.25G     0.7417     0.5679     0.8849        144        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3693      0.923      0.725      0.829      0.666\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      3.27G     0.7292     0.5529     0.8747        131        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.1s\n",
            "                   all        288       3693      0.919      0.747      0.859      0.691\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30      3.29G     0.7162      0.546      0.877        102        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.3it/s 1.1s\n",
            "                   all        288       3693      0.937      0.739      0.866      0.716\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      3.29G     0.7049     0.5471     0.8672         68        640: 100% ━━━━━━━━━━━━ 71/71 10.1it/s 7.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3693      0.777      0.809      0.849      0.697\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      3.32G       0.69     0.5255     0.8576         77        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3693      0.808      0.838      0.856      0.706\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      3.34G     0.6622     0.5087     0.8493         61        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3693      0.865      0.832      0.862       0.71\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      3.36G     0.6469     0.4972     0.8481         47        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3693      0.936       0.79      0.864      0.728\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      3.36G     0.6411     0.4872     0.8458         56        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3693      0.935      0.808      0.873      0.727\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30      3.39G     0.6263     0.4793     0.8424         39        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3693      0.883      0.864      0.884      0.721\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30      3.41G     0.6072      0.469     0.8408         47        640: 100% ━━━━━━━━━━━━ 71/71 11.8it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3693      0.902      0.845      0.878       0.73\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30      3.43G      0.604     0.4656     0.8419         54        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.3it/s 1.1s\n",
            "                   all        288       3693      0.907      0.828       0.88      0.733\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30      3.43G     0.5903     0.4551     0.8351         45        640: 100% ━━━━━━━━━━━━ 71/71 11.8it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3693      0.893      0.829      0.875      0.741\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      3.46G     0.5791     0.4524     0.8347         15        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3693      0.948      0.829      0.888      0.743\n",
            "\n",
            "30 epochs completed in 0.066 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/runs/pruned_tau0.2/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/runs/pruned_tau0.2/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/runs/pruned_tau0.2/weights/best.pt...\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 3.6it/s 2.5s\n",
            "                   all        288       3693      0.947      0.829      0.887      0.742\n",
            "               BOLLARD         11         11          1      0.627      0.719      0.338\n",
            "             BOX_TRUCK         41         41      0.839      0.854      0.901      0.739\n",
            "                   BUS         96         96      0.996          1      0.995      0.966\n",
            "     CONSTRUCTION_CONE         32         94      0.985      0.979      0.989      0.914\n",
            "         LARGE_VEHICLE         99        217      0.946      0.978      0.993      0.893\n",
            "            PEDESTRIAN        244       1345       0.91      0.584      0.694      0.524\n",
            "       REGULAR_VEHICLE        216       1834      0.922      0.673      0.816      0.638\n",
            "                 TRUCK         55         55      0.981      0.939      0.991      0.926\n",
            "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 4.9ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/runs/pruned_tau0.2\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 204.3±64.2 MB/s, size: 320.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.2/labels/val.cache... 288 images, 5 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 335.5Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 18/18 4.6it/s 4.0s\n",
            "                   all        288       3693      0.948      0.829      0.888      0.743\n",
            "               BOLLARD         11         11          1      0.627      0.726      0.337\n",
            "             BOX_TRUCK         41         41       0.84      0.854      0.901      0.739\n",
            "                   BUS         96         96      0.996          1      0.995      0.967\n",
            "     CONSTRUCTION_CONE         32         94      0.985      0.979      0.989      0.915\n",
            "         LARGE_VEHICLE         99        217      0.946      0.978      0.993      0.894\n",
            "            PEDESTRIAN        244       1345       0.91      0.584      0.694      0.525\n",
            "       REGULAR_VEHICLE        216       1834      0.925      0.672      0.816       0.64\n",
            "                 TRUCK         55         55      0.981      0.939      0.991      0.927\n",
            "Speed: 1.1ms preprocess, 0.7ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val19\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Model Strategy Dataset (Images)  Precision    Recall  F1-Score  \\\n",
              "0  Baseline (no pruning)              AV2   0.941997  0.752534  0.836674   \n",
              "1   Pruned (tau_BCS=0.2)              AV2   0.947917  0.829100  0.884536   \n",
              "\n",
              "      mAP50  mAP50-95  Box Loss  \n",
              "0  0.862451  0.721629   0.57838  \n",
              "1  0.888146  0.742988   0.57911  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8326709f-77b3-4db7-8170-592256eaadcc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Strategy</th>\n",
              "      <th>Dataset (Images)</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>mAP50</th>\n",
              "      <th>mAP50-95</th>\n",
              "      <th>Box Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline (no pruning)</td>\n",
              "      <td>AV2</td>\n",
              "      <td>0.941997</td>\n",
              "      <td>0.752534</td>\n",
              "      <td>0.836674</td>\n",
              "      <td>0.862451</td>\n",
              "      <td>0.721629</td>\n",
              "      <td>0.57838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pruned (tau_BCS=0.2)</td>\n",
              "      <td>AV2</td>\n",
              "      <td>0.947917</td>\n",
              "      <td>0.829100</td>\n",
              "      <td>0.884536</td>\n",
              "      <td>0.888146</td>\n",
              "      <td>0.742988</td>\n",
              "      <td>0.57911</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8326709f-77b3-4db7-8170-592256eaadcc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8326709f-77b3-4db7-8170-592256eaadcc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8326709f-77b3-4db7-8170-592256eaadcc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b3d37fdb-7402-4106-9717-8d63f9ee1bfb\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b3d37fdb-7402-4106-9717-8d63f9ee1bfb')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b3d37fdb-7402-4106-9717-8d63f9ee1bfb button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_be95b1a0-f6a4-488e-9df9-a4481dd11a45\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('report_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_be95b1a0-f6a4-488e-9df9-a4481dd11a45 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('report_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "report_df",
              "summary": "{\n  \"name\": \"report_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Model Strategy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Pruned (tau_BCS=0.2)\",\n          \"Baseline (no pruning)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dataset (Images)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AV2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004185592173730653,\n        \"min\": 0.9419974283143011,\n        \"max\": 0.9479167495329537,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.9479167495329537\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05414019470825853,\n        \"min\": 0.7525337778376321,\n        \"max\": 0.8290995754635714,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8290995754635714\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1-Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.033843661696152494,\n        \"min\": 0.8366737430018357,\n        \"max\": 0.8845359083729014,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8845359083729014\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP50\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.018169203909090947,\n        \"min\": 0.8624508207877787,\n        \"max\": 0.8881459553735374,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8881459553735374\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP50-95\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.015103467630794096,\n        \"min\": 0.7216288428174538,\n        \"max\": 0.7429883715797858,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.7429883715797858\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Box Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005161879502661856,\n        \"min\": 0.57838,\n        \"max\": 0.57911,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.57911\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Try other threshold pruning**"
      ],
      "metadata": {
        "id": "szblKQg7pHdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Threshold = 0.1**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NxaKYKl6r2Wj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u4RgGu4QpJ7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_ROOT = Path(\"/content/drive/MyDrive/av2_redundancy_yolo\")\n",
        "\n",
        "baseline_dir = build_yolo_from_av2_scene(\n",
        "    scene_path=SCENE_PATH,\n",
        "    ann_df=ann_df,\n",
        "    INTR=INTR,\n",
        "    T_EGO_SENSOR=T_EGO_SENSOR,\n",
        "    T_SENSOR_EGO=T_SENSOR_EGO,\n",
        "    IMG_INDEX=IMG_INDEX,\n",
        "    overlap_pairs=OVERLAP_PAIRS,\n",
        "    out_root=OUT_ROOT / \"baseline_tau1.0\",\n",
        "    tau_bcs=1.0,\n",
        "    train_ratio=0.8,\n",
        "    max_frames=None,\n",
        "    drop_empty_images=False\n",
        ")\n",
        "\n",
        "pruned_dir = build_yolo_from_av2_scene(\n",
        "    scene_path=SCENE_PATH,\n",
        "    ann_df=ann_df,\n",
        "    INTR=INTR,\n",
        "    T_EGO_SENSOR=T_EGO_SENSOR,\n",
        "    T_SENSOR_EGO=T_SENSOR_EGO,\n",
        "    IMG_INDEX=IMG_INDEX,\n",
        "    overlap_pairs=OVERLAP_PAIRS,\n",
        "    out_root=OUT_ROOT / \"pruned_tau0.1\",\n",
        "    tau_bcs=0.1,\n",
        "    train_ratio=0.8,\n",
        "    max_frames=None,\n",
        "    drop_empty_images=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXNtIHthr5Bk",
        "outputId": "9722d76f-3ad6-4360-f246-24cbc08b7e48"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exporting (tau_bcs=1.0): 100%|██████████| 157/157 [00:28<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. kept_images=1413, dropped_images=0\n",
            "data.yaml: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/data.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exporting (tau_bcs=0.1): 100%|██████████| 157/157 [00:29<00:00,  5.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. kept_images=1413, dropped_images=0\n",
            "data.yaml: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/data.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval_yolo(data_dir: Path, run_name: str, epochs: int = 30, imgsz: int = 640, batch: int = 16):\n",
        "    model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "    train_res = model.train(\n",
        "        data=str(data_dir / \"data.yaml\"),\n",
        "        epochs=epochs,\n",
        "        imgsz=imgsz,\n",
        "        batch=batch,\n",
        "        device=0,          # force GPU\n",
        "        cache=True,        # recommended\n",
        "        workers=4,         # recommended\n",
        "        name=run_name,\n",
        "        project=str(data_dir / \"runs\"),\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    metrics = model.val(\n",
        "        data=str(data_dir / \"data.yaml\"),\n",
        "        device=0           # force GPU for eval too\n",
        "    )\n",
        "\n",
        "    precision = float(metrics.box.mp)\n",
        "    recall    = float(metrics.box.mr)\n",
        "    map50     = float(metrics.box.map50)\n",
        "    map5095   = float(metrics.box.map)\n",
        "    f1        = (2 * precision * recall / (precision + recall + 1e-12))\n",
        "\n",
        "    box_loss = None\n",
        "    results_csv = Path(train_res.save_dir) / \"results.csv\"\n",
        "    if results_csv.exists():\n",
        "        df = pd.read_csv(results_csv)\n",
        "        box_cols = [c for c in df.columns if \"train/box_loss\" in c.lower()]\n",
        "        if box_cols:\n",
        "            box_loss = float(df[box_cols[0]].iloc[-1])\n",
        "\n",
        "    return {\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1,\n",
        "        \"mAP50\": map50,\n",
        "        \"mAP50-95\": map5095,\n",
        "        \"Box Loss\": box_loss\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ncPD7C0Dr9L7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "base_metrics = train_and_eval_yolo(baseline_dir, run_name=\"baseline_tau1.0\", epochs=30, imgsz=640, batch=16)\n",
        "rows.append({\n",
        "    \"Model Strategy\": \"Baseline (no pruning)\",\n",
        "    \"Dataset (Images)\": \"AV2\",\n",
        "    **base_metrics\n",
        "})\n",
        "\n",
        "pruned_metrics = train_and_eval_yolo(pruned_dir, run_name=\"pruned_tau0.1\", epochs=30, imgsz=640, batch=16)\n",
        "rows.append({\n",
        "    \"Model Strategy\": \"Pruned (tau_BCS=0.1)\",\n",
        "    \"Dataset (Images)\": \"AV2\",\n",
        "    **pruned_metrics\n",
        "})\n",
        "\n",
        "report_df = pd.DataFrame(rows, columns=[\n",
        "    \"Model Strategy\", \"Dataset (Images)\", \"Precision\", \"Recall\", \"F1-Score\", \"mAP50\", \"mAP50-95\", \"Box Loss\"\n",
        "])\n",
        "\n",
        "report_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lM7JmmSasEKQ",
        "outputId": "1db82532-9ec6-4cd8-9ccc-44a537cb9cca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=baseline_tau1.0, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 120.9±34.8 MB/s, size: 321.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/train... 1125 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1125/1125 115.0it/s 9.8s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/train.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB RAM): 100% ━━━━━━━━━━━━ 1125/1125 247.0it/s 4.6s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 122.5±29.9 MB/s, size: 257.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val... 288 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 147.0it/s 2.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB RAM): 100% ━━━━━━━━━━━━ 288/288 253.2it/s 1.1s\n",
            "Plotting labels to /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      2.94G      1.686      2.489      1.252        125        640: 100% ━━━━━━━━━━━━ 71/71 9.2it/s 7.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 6.9it/s 1.3s\n",
            "                   all        288       3981      0.954       0.23      0.467      0.314\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      2.94G      1.342      1.258      1.109        190        640: 100% ━━━━━━━━━━━━ 71/71 10.6it/s 6.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 6.8it/s 1.3s\n",
            "                   all        288       3981      0.829      0.533      0.608      0.387\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      2.94G       1.23      1.075       1.06        130        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.2it/s 1.3s\n",
            "                   all        288       3981      0.868      0.588      0.653      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      2.94G      1.132     0.9759      1.013        145        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.2it/s 1.2s\n",
            "                   all        288       3981      0.918      0.628      0.688      0.526\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      2.94G       1.06     0.8974     0.9953         62        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.3it/s 1.2s\n",
            "                   all        288       3981      0.929      0.662      0.715      0.553\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      2.94G      1.003     0.8299     0.9734         93        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.5it/s 1.2s\n",
            "                   all        288       3981      0.906      0.686      0.725      0.562\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      2.94G     0.9634     0.7933     0.9631         81        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3981      0.929      0.687      0.741      0.578\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      2.94G     0.9364     0.7497     0.9478        113        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.5it/s 1.2s\n",
            "                   all        288       3981      0.913      0.703      0.746      0.595\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30      2.94G      0.895     0.7104     0.9351        144        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981      0.929      0.705      0.751      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      2.94G     0.8674     0.6819     0.9257         95        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981      0.962      0.697      0.759      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      2.94G      0.845     0.6621     0.9203        110        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.957      0.701      0.762      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      2.94G     0.8207     0.6417     0.9149        107        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.917      0.717      0.767      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      2.94G     0.8084     0.6303     0.9112         77        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981       0.92       0.71      0.762      0.619\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      2.96G     0.7931     0.6187     0.9099        107        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.934      0.719      0.767      0.632\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      3.19G     0.7913     0.6129     0.9131        142        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.804      0.723      0.774       0.64\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30       3.2G     0.7618     0.5831      0.896        137        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.808      0.723      0.774      0.637\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30      3.21G     0.7565      0.572     0.8967         99        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.808      0.733      0.776      0.644\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      3.24G     0.7349     0.5604     0.8942        152        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3981      0.825      0.731      0.772      0.645\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      3.25G     0.7234     0.5488     0.8833        143        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3981      0.811       0.74      0.778      0.655\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30      3.27G     0.7182     0.5458     0.8883        104        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3981      0.826      0.738      0.782      0.658\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      3.28G     0.7025      0.543     0.8762         73        640: 100% ━━━━━━━━━━━━ 71/71 10.4it/s 6.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.947      0.735      0.774      0.647\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30       3.3G     0.6854     0.5209     0.8652         80        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3981      0.944      0.737      0.851      0.681\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      3.32G     0.6604      0.504     0.8577         63        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3981      0.942      0.732       0.87      0.691\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      3.34G     0.6445     0.4915     0.8555         50        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3981       0.95      0.741      0.862      0.701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      3.35G     0.6311      0.481     0.8521         61        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3981      0.954      0.739      0.862      0.692\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30      3.37G     0.6235     0.4742     0.8499         44        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3981      0.944      0.749      0.864      0.711\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30      3.39G       0.61     0.4632     0.8497         50        640: 100% ━━━━━━━━━━━━ 71/71 11.8it/s 6.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3981      0.949      0.744      0.862      0.713\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30      3.41G     0.6008     0.4605     0.8484         58        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.3it/s 1.1s\n",
            "                   all        288       3981      0.946      0.756      0.865      0.717\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30      3.41G     0.5856     0.4494     0.8408         47        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3981      0.939      0.754      0.861      0.715\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      3.44G     0.5784     0.4484     0.8409         17        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3981      0.942      0.752      0.867      0.723\n",
            "\n",
            "30 epochs completed in 0.066 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/best.pt...\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 3.6it/s 2.5s\n",
            "                   all        288       3981      0.942      0.752      0.867      0.723\n",
            "               BOLLARD         11         11          1          0      0.657      0.321\n",
            "             BOX_TRUCK         60         60      0.878      0.838      0.866      0.691\n",
            "                   BUS         96         96      0.994          1      0.995       0.96\n",
            "     CONSTRUCTION_CONE         32         94      0.982      0.979      0.988      0.919\n",
            "         LARGE_VEHICLE        104        252      0.959      0.897       0.94      0.806\n",
            "            PEDESTRIAN        248       1402      0.876      0.595      0.683      0.512\n",
            "       REGULAR_VEHICLE        237       1971      0.886      0.719      0.816      0.622\n",
            "                 TRUCK         95         95      0.961      0.989      0.993      0.955\n",
            "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 4.6ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 5.2±10.8 ms, read: 121.8±72.4 MB/s, size: 320.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val.cache... 288 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 379.5Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 18/18 4.5it/s 4.0s\n",
            "                   all        288       3981      0.942      0.753      0.862      0.722\n",
            "               BOLLARD         11         11          1          0      0.617      0.306\n",
            "             BOX_TRUCK         60         60      0.878      0.839      0.867      0.691\n",
            "                   BUS         96         96      0.994          1      0.995      0.966\n",
            "     CONSTRUCTION_CONE         32         94      0.984      0.979      0.988      0.912\n",
            "         LARGE_VEHICLE        104        252      0.959      0.897      0.939      0.803\n",
            "            PEDESTRIAN        248       1402      0.875      0.596      0.683      0.515\n",
            "       REGULAR_VEHICLE        237       1971      0.884       0.72      0.817      0.626\n",
            "                 TRUCK         95         95      0.961      0.989      0.993      0.954\n",
            "Speed: 1.1ms preprocess, 0.7ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val20\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=pruned_tau0.1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/runs/pruned_tau0.1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 140.9±42.1 MB/s, size: 321.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/labels/train... 1125 images, 15 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1125/1125 112.5it/s 10.0s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/labels/train.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB RAM): 100% ━━━━━━━━━━━━ 1125/1125 259.3it/s 4.3s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 106.4±26.5 MB/s, size: 257.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/labels/val... 288 images, 5 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 160.5it/s 1.8s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/labels/val.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB RAM): 100% ━━━━━━━━━━━━ 288/288 246.0it/s 1.2s\n",
            "Plotting labels to /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/runs/pruned_tau0.1/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/runs/pruned_tau0.1\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      2.77G      1.726       2.51      1.235        110        640: 100% ━━━━━━━━━━━━ 71/71 9.1it/s 7.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.2it/s 1.2s\n",
            "                   all        288       3658       0.96      0.167      0.442      0.301\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      2.77G      1.366      1.283      1.093        179        640: 100% ━━━━━━━━━━━━ 71/71 10.6it/s 6.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.2it/s 1.3s\n",
            "                   all        288       3658      0.834      0.508      0.601      0.409\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      2.77G      1.248      1.107      1.047        117        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.3it/s 1.2s\n",
            "                   all        288       3658      0.863       0.59      0.672      0.496\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      2.77G      1.148      0.999      1.005        135        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.1it/s 1.3s\n",
            "                   all        288       3658      0.914      0.644      0.696       0.53\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      2.77G      1.076     0.9198     0.9852         56        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3658      0.939      0.645      0.702      0.547\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      2.77G      1.018     0.8513     0.9637         83        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3658      0.913      0.671      0.725      0.566\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      2.77G     0.9805     0.8019     0.9493         75        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.5it/s 1.2s\n",
            "                   all        288       3658       0.94      0.679       0.74      0.592\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      2.77G      0.953     0.7634     0.9356        103        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3658      0.933      0.676      0.747      0.602\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30      2.77G     0.8915     0.7174     0.9205        132        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3658      0.913      0.716      0.761      0.603\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      2.77G     0.8692     0.6995     0.9143         88        640: 100% ━━━━━━━━━━━━ 71/71 11.7it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3658      0.936      0.705      0.754      0.619\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      2.77G     0.8674     0.6802     0.9153        101        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3658      0.926      0.711      0.765      0.618\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      2.77G     0.8373     0.6593     0.9076        101        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.5it/s 1.2s\n",
            "                   all        288       3658      0.916      0.707      0.763      0.617\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      2.77G     0.8106     0.6324     0.8998         67        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3658      0.802      0.707      0.759       0.63\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      2.77G     0.7932     0.6241     0.8963         97        640: 100% ━━━━━━━━━━━━ 71/71 10.8it/s 6.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3658      0.836        0.7      0.768      0.638\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      2.99G     0.7985     0.6206     0.9002        137        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3658      0.806      0.731      0.781      0.651\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      3.01G     0.7659     0.5915     0.8849        132        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3658      0.798      0.746      0.785      0.663\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30      3.02G     0.7727     0.5833     0.8888         90        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3658      0.937      0.738      0.855      0.689\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      3.04G     0.7375     0.5648     0.8833        144        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.4it/s 1.1s\n",
            "                   all        288       3658      0.911      0.745      0.846      0.704\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      3.06G     0.7235     0.5494     0.8718        131        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3658       0.92      0.743       0.84      0.689\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30      3.08G     0.7194     0.5483     0.8783        102        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3658      0.924      0.751      0.848      0.708\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      3.09G     0.7072     0.5455     0.8655         68        640: 100% ━━━━━━━━━━━━ 71/71 10.0it/s 7.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3658       0.94      0.744       0.85      0.694\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      3.11G     0.6862     0.5261     0.8578         76        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3658      0.894      0.795      0.861      0.699\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      3.13G      0.668     0.5103     0.8503         60        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3658       0.82      0.844      0.863       0.71\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      3.15G     0.6508     0.4993     0.8501         46        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3658       0.89      0.833       0.87      0.722\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      3.15G     0.6409     0.4881     0.8465         56        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3658      0.899      0.834      0.872      0.733\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30      3.18G     0.6278     0.4795     0.8435         39        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.3it/s 1.1s\n",
            "                   all        288       3658      0.893      0.845       0.88       0.74\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30       3.2G     0.6135     0.4693     0.8416         47        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3658       0.91      0.837      0.882      0.747\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30      3.21G     0.6068     0.4675     0.8412         54        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3658       0.96      0.808      0.876      0.748\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30      3.22G     0.5924     0.4587     0.8354         45        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3658      0.956      0.807      0.881       0.75\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      3.25G      0.586     0.4565     0.8362         14        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3658      0.952       0.81      0.883      0.755\n",
            "\n",
            "30 epochs completed in 0.066 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/runs/pruned_tau0.1/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/runs/pruned_tau0.1/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/runs/pruned_tau0.1/weights/best.pt...\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 3.7it/s 2.5s\n",
            "                   all        288       3658      0.952       0.81      0.883      0.757\n",
            "               BOLLARD         11         11      0.934      0.545       0.69      0.455\n",
            "             BOX_TRUCK         37         37       0.86      0.838      0.916      0.773\n",
            "                   BUS         96         96      0.997          1      0.995      0.966\n",
            "     CONSTRUCTION_CONE         32         93      0.978      0.979      0.976      0.881\n",
            "         LARGE_VEHICLE         98        213      0.975      0.962      0.992      0.866\n",
            "            PEDESTRIAN        243       1342      0.945      0.548       0.69      0.521\n",
            "       REGULAR_VEHICLE        215       1813      0.949      0.649      0.816      0.633\n",
            "                 TRUCK         53         53      0.981      0.961      0.992      0.958\n",
            "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 4.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/runs/pruned_tau0.1\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 141.8±38.5 MB/s, size: 320.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.1/labels/val.cache... 288 images, 5 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 408.1Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 18/18 4.4it/s 4.1s\n",
            "                   all        288       3658      0.952       0.81      0.883      0.756\n",
            "               BOLLARD         11         11      0.932      0.545       0.69      0.432\n",
            "             BOX_TRUCK         37         37      0.859      0.838      0.914      0.773\n",
            "                   BUS         96         96      0.997          1      0.995       0.97\n",
            "     CONSTRUCTION_CONE         32         93      0.978      0.979      0.976      0.886\n",
            "         LARGE_VEHICLE         98        213      0.975      0.962      0.992      0.869\n",
            "            PEDESTRIAN        243       1342      0.942      0.548      0.687      0.522\n",
            "       REGULAR_VEHICLE        215       1813       0.95       0.65      0.819      0.637\n",
            "                 TRUCK         53         53      0.981      0.961      0.992      0.955\n",
            "Speed: 1.2ms preprocess, 0.7ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val21\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Model Strategy Dataset (Images)  Precision    Recall  F1-Score  \\\n",
              "0  Baseline (no pruning)              AV2   0.941997  0.752534  0.836674   \n",
              "1   Pruned (tau_BCS=0.1)              AV2   0.951829  0.810456  0.875472   \n",
              "\n",
              "      mAP50  mAP50-95  Box Loss  \n",
              "0  0.862451  0.721629   0.57838  \n",
              "1  0.883290  0.755696   0.58597  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da53afd2-33e3-4aca-b04c-563e179e2a1d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Strategy</th>\n",
              "      <th>Dataset (Images)</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>mAP50</th>\n",
              "      <th>mAP50-95</th>\n",
              "      <th>Box Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline (no pruning)</td>\n",
              "      <td>AV2</td>\n",
              "      <td>0.941997</td>\n",
              "      <td>0.752534</td>\n",
              "      <td>0.836674</td>\n",
              "      <td>0.862451</td>\n",
              "      <td>0.721629</td>\n",
              "      <td>0.57838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pruned (tau_BCS=0.1)</td>\n",
              "      <td>AV2</td>\n",
              "      <td>0.951829</td>\n",
              "      <td>0.810456</td>\n",
              "      <td>0.875472</td>\n",
              "      <td>0.883290</td>\n",
              "      <td>0.755696</td>\n",
              "      <td>0.58597</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da53afd2-33e3-4aca-b04c-563e179e2a1d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-da53afd2-33e3-4aca-b04c-563e179e2a1d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-da53afd2-33e3-4aca-b04c-563e179e2a1d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e80d9b6a-373b-4eab-b34b-11bb5ff98f90\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e80d9b6a-373b-4eab-b34b-11bb5ff98f90')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e80d9b6a-373b-4eab-b34b-11bb5ff98f90 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_0a510147-fcde-4fe8-a1e7-93bf55bd8c1a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('report_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0a510147-fcde-4fe8-a1e7-93bf55bd8c1a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('report_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "report_df",
              "summary": "{\n  \"name\": \"report_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Model Strategy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Pruned (tau_BCS=0.1)\",\n          \"Baseline (no pruning)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dataset (Images)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AV2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006951995295121752,\n        \"min\": 0.9419974283143011,\n        \"max\": 0.9518290343462162,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.9518290343462162\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.040957129303808605,\n        \"min\": 0.7525337778376321,\n        \"max\": 0.8104559055749467,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8104559055749467\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1-Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.027434431594005715,\n        \"min\": 0.8366737430018357,\n        \"max\": 0.8754718882380755,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8754718882380755\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP50\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.014735565060817102,\n        \"min\": 0.8624508207877787,\n        \"max\": 0.8832900567460173,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8832900567460173\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP50-95\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02408935231398551,\n        \"min\": 0.7216288428174538,\n        \"max\": 0.7556963315686758,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.7556963315686758\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Box Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005366940469205886,\n        \"min\": 0.57838,\n        \"max\": 0.58597,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.58597\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Threshold 0.3**"
      ],
      "metadata": {
        "id": "O9id3UrJyA_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_ROOT = Path(\"/content/drive/MyDrive/av2_redundancy_yolo\")\n",
        "\n",
        "baseline_dir = build_yolo_from_av2_scene(\n",
        "    scene_path=SCENE_PATH,\n",
        "    ann_df=ann_df,\n",
        "    INTR=INTR,\n",
        "    T_EGO_SENSOR=T_EGO_SENSOR,\n",
        "    T_SENSOR_EGO=T_SENSOR_EGO,\n",
        "    IMG_INDEX=IMG_INDEX,\n",
        "    overlap_pairs=OVERLAP_PAIRS,\n",
        "    out_root=OUT_ROOT / \"baseline_tau1.0\",\n",
        "    tau_bcs=1.0,\n",
        "    train_ratio=0.8,\n",
        "    max_frames=None,\n",
        "    drop_empty_images=False\n",
        ")\n",
        "\n",
        "pruned_dir = build_yolo_from_av2_scene(\n",
        "    scene_path=SCENE_PATH,\n",
        "    ann_df=ann_df,\n",
        "    INTR=INTR,\n",
        "    T_EGO_SENSOR=T_EGO_SENSOR,\n",
        "    T_SENSOR_EGO=T_SENSOR_EGO,\n",
        "    IMG_INDEX=IMG_INDEX,\n",
        "    overlap_pairs=OVERLAP_PAIRS,\n",
        "    out_root=OUT_ROOT / \"pruned_tau0.3\",\n",
        "    tau_bcs=0.3,\n",
        "    train_ratio=0.8,\n",
        "    max_frames=None,\n",
        "    drop_empty_images=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCYh6MOtsOxD",
        "outputId": "093ce346-305b-45de-ba69-e6dc85ad8bd7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exporting (tau_bcs=1.0): 100%|██████████| 157/157 [00:28<00:00,  5.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. kept_images=1413, dropped_images=0\n",
            "data.yaml: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/data.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exporting (tau_bcs=0.3): 100%|██████████| 157/157 [00:29<00:00,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. kept_images=1413, dropped_images=0\n",
            "data.yaml: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/data.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval_yolo(data_dir: Path, run_name: str, epochs: int = 30, imgsz: int = 640, batch: int = 16):\n",
        "    model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "    train_res = model.train(\n",
        "        data=str(data_dir / \"data.yaml\"),\n",
        "        epochs=epochs,\n",
        "        imgsz=imgsz,\n",
        "        batch=batch,\n",
        "        device=0,          # force GPU\n",
        "        cache=True,        # recommended\n",
        "        workers=4,         # recommended\n",
        "        name=run_name,\n",
        "        project=str(data_dir / \"runs\"),\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    metrics = model.val(\n",
        "        data=str(data_dir / \"data.yaml\"),\n",
        "        device=0           # force GPU for eval too\n",
        "    )\n",
        "\n",
        "    precision = float(metrics.box.mp)\n",
        "    recall    = float(metrics.box.mr)\n",
        "    map50     = float(metrics.box.map50)\n",
        "    map5095   = float(metrics.box.map)\n",
        "    f1        = (2 * precision * recall / (precision + recall + 1e-12))\n",
        "\n",
        "    box_loss = None\n",
        "    results_csv = Path(train_res.save_dir) / \"results.csv\"\n",
        "    if results_csv.exists():\n",
        "        df = pd.read_csv(results_csv)\n",
        "        box_cols = [c for c in df.columns if \"train/box_loss\" in c.lower()]\n",
        "        if box_cols:\n",
        "            box_loss = float(df[box_cols[0]].iloc[-1])\n",
        "\n",
        "    return {\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1,\n",
        "        \"mAP50\": map50,\n",
        "        \"mAP50-95\": map5095,\n",
        "        \"Box Loss\": box_loss\n",
        "    }\n"
      ],
      "metadata": {
        "id": "hXf-x9VQyFZ7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "base_metrics = train_and_eval_yolo(baseline_dir, run_name=\"baseline_tau1.0\", epochs=30, imgsz=640, batch=16)\n",
        "rows.append({\n",
        "    \"Model Strategy\": \"Baseline (no pruning)\",\n",
        "    \"Dataset (Images)\": \"AV2\",\n",
        "    **base_metrics\n",
        "})\n",
        "\n",
        "pruned_metrics = train_and_eval_yolo(pruned_dir, run_name=\"pruned_tau0.3\", epochs=30, imgsz=640, batch=16)\n",
        "rows.append({\n",
        "    \"Model Strategy\": \"Pruned (tau_BCS=0.3)\",\n",
        "    \"Dataset (Images)\": \"AV2\",\n",
        "    **pruned_metrics\n",
        "})\n",
        "\n",
        "report_df = pd.DataFrame(rows, columns=[\n",
        "    \"Model Strategy\", \"Dataset (Images)\", \"Precision\", \"Recall\", \"F1-Score\", \"mAP50\", \"mAP50-95\", \"Box Loss\"\n",
        "])\n",
        "\n",
        "report_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XUFJodYByIGF",
        "outputId": "fcd63cea-994d-43f4-c8b2-44e08012df68"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=baseline_tau1.0, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 1.3±1.9 ms, read: 138.8±95.7 MB/s, size: 321.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/train... 1125 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1125/1125 112.6it/s 10.0s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/train.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB RAM): 100% ━━━━━━━━━━━━ 1125/1125 245.9it/s 4.6s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.6±0.5 ms, read: 70.2±44.2 MB/s, size: 257.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val... 288 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 155.2it/s 1.9s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB RAM): 100% ━━━━━━━━━━━━ 288/288 220.3it/s 1.3s\n",
            "Plotting labels to /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      2.96G      1.686      2.489      1.252        125        640: 100% ━━━━━━━━━━━━ 71/71 9.2it/s 7.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.2it/s 1.2s\n",
            "                   all        288       3981      0.954       0.23      0.467      0.314\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      2.96G      1.342      1.258      1.109        190        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.0it/s 1.3s\n",
            "                   all        288       3981      0.829      0.533      0.608      0.387\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      2.96G       1.23      1.075       1.06        130        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.2it/s 1.2s\n",
            "                   all        288       3981      0.868      0.588      0.653      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      2.96G      1.132     0.9759      1.013        145        640: 100% ━━━━━━━━━━━━ 71/71 10.7it/s 6.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.2it/s 1.2s\n",
            "                   all        288       3981      0.918      0.628      0.688      0.526\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      2.96G       1.06     0.8974     0.9953         62        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.4it/s 1.2s\n",
            "                   all        288       3981      0.929      0.662      0.715      0.553\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      2.96G      1.003     0.8299     0.9734         93        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.3it/s 1.2s\n",
            "                   all        288       3981      0.906      0.686      0.725      0.562\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      2.96G     0.9634     0.7933     0.9631         81        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.2it/s 1.3s\n",
            "                   all        288       3981      0.929      0.687      0.741      0.578\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      2.96G     0.9364     0.7497     0.9478        113        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.5it/s 1.2s\n",
            "                   all        288       3981      0.913      0.703      0.746      0.595\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30      2.96G      0.895     0.7104     0.9351        144        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.929      0.705      0.751      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      2.96G     0.8674     0.6819     0.9257         95        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.5it/s 1.2s\n",
            "                   all        288       3981      0.962      0.697      0.759      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      2.96G      0.845     0.6621     0.9203        110        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3981      0.957      0.701      0.762      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      2.96G     0.8207     0.6417     0.9149        107        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.917      0.717      0.767      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      2.96G     0.8084     0.6303     0.9112         77        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981       0.92       0.71      0.762      0.619\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      2.96G     0.7931     0.6187     0.9099        107        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.934      0.719      0.767      0.632\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      3.17G     0.7913     0.6129     0.9131        142        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.804      0.723      0.774       0.64\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      3.19G     0.7618     0.5831      0.896        137        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.808      0.723      0.774      0.637\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30       3.2G     0.7565      0.572     0.8967         99        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3981      0.808      0.733      0.776      0.644\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      3.22G     0.7349     0.5604     0.8942        152        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981      0.825      0.731      0.772      0.645\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      3.24G     0.7234     0.5488     0.8833        143        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.811       0.74      0.778      0.655\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30      3.26G     0.7182     0.5458     0.8883        104        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3981      0.826      0.738      0.782      0.658\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      3.27G     0.7025      0.543     0.8762         73        640: 100% ━━━━━━━━━━━━ 71/71 10.3it/s 6.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981      0.947      0.735      0.774      0.647\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      3.29G     0.6854     0.5209     0.8652         80        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.944      0.737      0.851      0.681\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      3.31G     0.6604      0.504     0.8577         63        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.1s\n",
            "                   all        288       3981      0.942      0.732       0.87      0.691\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      3.33G     0.6445     0.4915     0.8555         50        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981       0.95      0.741      0.862      0.701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      3.33G     0.6311      0.481     0.8521         61        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.954      0.739      0.862      0.692\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30      3.36G     0.6235     0.4742     0.8499         44        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.944      0.749      0.864      0.711\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30      3.38G       0.61     0.4632     0.8497         50        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.949      0.744      0.862      0.713\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30       3.4G     0.6008     0.4605     0.8484         58        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.946      0.756      0.865      0.717\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30       3.4G     0.5856     0.4494     0.8408         47        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.939      0.754      0.861      0.715\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      3.43G     0.5784     0.4484     0.8409         17        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.942      0.752      0.867      0.723\n",
            "\n",
            "30 epochs completed in 0.067 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/best.pt...\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 3.5it/s 2.6s\n",
            "                   all        288       3981      0.942      0.752      0.867      0.723\n",
            "               BOLLARD         11         11          1          0      0.657      0.321\n",
            "             BOX_TRUCK         60         60      0.878      0.838      0.866      0.691\n",
            "                   BUS         96         96      0.994          1      0.995       0.96\n",
            "     CONSTRUCTION_CONE         32         94      0.982      0.979      0.988      0.919\n",
            "         LARGE_VEHICLE        104        252      0.959      0.897       0.94      0.806\n",
            "            PEDESTRIAN        248       1402      0.876      0.595      0.683      0.512\n",
            "       REGULAR_VEHICLE        237       1971      0.886      0.719      0.816      0.622\n",
            "                 TRUCK         95         95      0.961      0.989      0.993      0.955\n",
            "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 5.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 181.2±54.5 MB/s, size: 320.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val.cache... 288 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 385.1Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 18/18 4.4it/s 4.1s\n",
            "                   all        288       3981      0.942      0.753      0.862      0.722\n",
            "               BOLLARD         11         11          1          0      0.617      0.306\n",
            "             BOX_TRUCK         60         60      0.878      0.839      0.867      0.691\n",
            "                   BUS         96         96      0.994          1      0.995      0.966\n",
            "     CONSTRUCTION_CONE         32         94      0.984      0.979      0.988      0.912\n",
            "         LARGE_VEHICLE        104        252      0.959      0.897      0.939      0.803\n",
            "            PEDESTRIAN        248       1402      0.875      0.596      0.683      0.515\n",
            "       REGULAR_VEHICLE        237       1971      0.884       0.72      0.817      0.626\n",
            "                 TRUCK         95         95      0.961      0.989      0.993      0.954\n",
            "Speed: 1.5ms preprocess, 0.8ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val22\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=pruned_tau0.3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/runs/pruned_tau0.3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 120.2±30.5 MB/s, size: 321.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/labels/train... 1125 images, 11 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1125/1125 113.7it/s 9.9s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/labels/train.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB RAM): 100% ━━━━━━━━━━━━ 1125/1125 256.1it/s 4.4s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 19.9±43.3 ms, read: 43.1±41.4 MB/s, size: 257.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/labels/val... 288 images, 5 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 232.6it/s 1.2s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/labels/val.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB RAM): 100% ━━━━━━━━━━━━ 288/288 243.5it/s 1.2s\n",
            "Plotting labels to /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/runs/pruned_tau0.3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/runs/pruned_tau0.3\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      2.75G      1.714      2.493      1.239        116        640: 100% ━━━━━━━━━━━━ 71/71 9.2it/s 7.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 6.8it/s 1.3s\n",
            "                   all        288       3721      0.928      0.182       0.45      0.312\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      2.75G      1.357      1.268      1.096        181        640: 100% ━━━━━━━━━━━━ 71/71 10.6it/s 6.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.2it/s 1.3s\n",
            "                   all        288       3721      0.771      0.516      0.592      0.412\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      2.75G      1.251        1.1       1.05        119        640: 100% ━━━━━━━━━━━━ 71/71 10.8it/s 6.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.3it/s 1.2s\n",
            "                   all        288       3721      0.859      0.613      0.661      0.499\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      2.75G      1.138     0.9764      1.001        139        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.1it/s 1.3s\n",
            "                   all        288       3721      0.891      0.646      0.703      0.543\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      2.75G      1.071     0.9036     0.9828         57        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3721      0.903       0.65      0.696      0.534\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      2.75G      1.009      0.837     0.9641         84        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.3it/s 1.2s\n",
            "                   all        288       3721      0.898      0.668      0.731      0.579\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      2.75G      0.971     0.7945     0.9508         76        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3721      0.881        0.7      0.747      0.592\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      2.75G     0.9431      0.754     0.9385        108        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.1s\n",
            "                   all        288       3721      0.889      0.701      0.748      0.598\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30      2.95G     0.8885     0.7083     0.9231        134        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3721      0.938      0.701      0.763      0.618\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      2.95G     0.8614     0.6861     0.9148         90        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3721      0.935        0.7      0.763       0.61\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      2.95G     0.8661     0.6758     0.9163        104        640: 100% ━━━━━━━━━━━━ 71/71 10.8it/s 6.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3721      0.889      0.735      0.766      0.634\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      2.95G     0.8381     0.6543     0.9094        101        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3721      0.797      0.728      0.775      0.631\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      2.95G     0.8038      0.627        0.9         69        640: 100% ━━━━━━━━━━━━ 71/71 11.7it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3721      0.926      0.718      0.773      0.637\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      2.97G     0.7993     0.6214     0.8997         97        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3721      0.807      0.731      0.779      0.646\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      3.19G     0.7943     0.6145     0.9005        139        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3721       0.95      0.711      0.778       0.65\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      3.21G     0.7668     0.5881     0.8861        135        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3721      0.922      0.732      0.777      0.652\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30      3.22G     0.7603     0.5738     0.8891         92        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3721      0.936      0.717      0.852      0.655\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      3.24G     0.7341     0.5604      0.883        145        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3721      0.918      0.729      0.846      0.686\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      3.26G     0.7274     0.5493     0.8734        133        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.3it/s 1.1s\n",
            "                   all        288       3721       0.92      0.751      0.849      0.692\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30      3.28G     0.7161     0.5431     0.8769        102        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3721      0.932      0.738      0.865      0.699\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      3.29G     0.7053     0.5461     0.8679         70        640: 100% ━━━━━━━━━━━━ 71/71 10.1it/s 7.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3721       0.93       0.75      0.866      0.701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      3.31G       0.69     0.5271     0.8578         77        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3721       0.95      0.739      0.866      0.698\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      3.33G     0.6652     0.5062      0.851         62        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3721      0.847      0.821      0.854      0.706\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      3.35G     0.6511     0.4949     0.8493         48        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3721      0.875      0.851      0.874      0.706\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      3.36G     0.6388     0.4831     0.8458         56        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3721      0.883       0.84      0.872       0.72\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30      3.38G     0.6296     0.4788     0.8442         40        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3721      0.884      0.841      0.872      0.717\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30       3.4G     0.6128     0.4669     0.8414         47        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3721      0.889      0.839      0.875      0.728\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30      3.42G     0.6025      0.465     0.8421         54        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3721      0.913      0.824      0.877      0.736\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30      3.42G     0.5902     0.4555     0.8362         45        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3721      0.927      0.821      0.876       0.74\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      3.45G     0.5849     0.4514     0.8369         15        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3721      0.944      0.817      0.883       0.74\n",
            "\n",
            "30 epochs completed in 0.066 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/runs/pruned_tau0.3/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/runs/pruned_tau0.3/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/runs/pruned_tau0.3/weights/best.pt...\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 3.6it/s 2.5s\n",
            "                   all        288       3721      0.942      0.819      0.883       0.74\n",
            "               BOLLARD         11         11      0.932      0.545      0.657      0.301\n",
            "             BOX_TRUCK         43         43      0.794      0.898      0.917       0.75\n",
            "                   BUS         96         96      0.996          1      0.995      0.971\n",
            "     CONSTRUCTION_CONE         32         94      0.977      0.979      0.987      0.897\n",
            "         LARGE_VEHICLE        100        224      0.982      0.967      0.992      0.889\n",
            "            PEDESTRIAN        246       1351      0.936      0.567      0.698      0.525\n",
            "       REGULAR_VEHICLE        216       1846      0.939      0.666      0.825      0.636\n",
            "                 TRUCK         56         56      0.983      0.929      0.992      0.947\n",
            "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 5.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/runs/pruned_tau0.3\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 8.8±18.0 ms, read: 104.8±92.5 MB/s, size: 320.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.3/labels/val.cache... 288 images, 5 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 460.5Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 18/18 4.3it/s 4.2s\n",
            "                   all        288       3721      0.944      0.818      0.881      0.742\n",
            "               BOLLARD         11         11       0.94      0.545       0.65      0.302\n",
            "             BOX_TRUCK         43         43      0.794      0.896      0.917      0.747\n",
            "                   BUS         96         96      0.996          1      0.995      0.972\n",
            "     CONSTRUCTION_CONE         32         94      0.977      0.979      0.987      0.908\n",
            "         LARGE_VEHICLE        100        224      0.982      0.966      0.991      0.893\n",
            "            PEDESTRIAN        246       1351      0.941      0.565      0.698      0.528\n",
            "       REGULAR_VEHICLE        216       1846       0.94      0.662      0.822      0.639\n",
            "                 TRUCK         56         56      0.984      0.929      0.992      0.949\n",
            "Speed: 1.2ms preprocess, 0.7ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val23\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Model Strategy Dataset (Images)  Precision    Recall  F1-Score  \\\n",
              "0  Baseline (no pruning)              AV2   0.941997  0.752534  0.836674   \n",
              "1   Pruned (tau_BCS=0.3)              AV2   0.944090  0.817721  0.876373   \n",
              "\n",
              "      mAP50  mAP50-95  Box Loss  \n",
              "0  0.862451  0.721629   0.57838  \n",
              "1  0.881457  0.742123   0.58487  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d7647de-68a6-4a14-aca7-3884f8764f53\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Strategy</th>\n",
              "      <th>Dataset (Images)</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>mAP50</th>\n",
              "      <th>mAP50-95</th>\n",
              "      <th>Box Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline (no pruning)</td>\n",
              "      <td>AV2</td>\n",
              "      <td>0.941997</td>\n",
              "      <td>0.752534</td>\n",
              "      <td>0.836674</td>\n",
              "      <td>0.862451</td>\n",
              "      <td>0.721629</td>\n",
              "      <td>0.57838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pruned (tau_BCS=0.3)</td>\n",
              "      <td>AV2</td>\n",
              "      <td>0.944090</td>\n",
              "      <td>0.817721</td>\n",
              "      <td>0.876373</td>\n",
              "      <td>0.881457</td>\n",
              "      <td>0.742123</td>\n",
              "      <td>0.58487</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d7647de-68a6-4a14-aca7-3884f8764f53')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4d7647de-68a6-4a14-aca7-3884f8764f53 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4d7647de-68a6-4a14-aca7-3884f8764f53');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-364deeb5-0c54-4452-ad1c-8965d15ff22e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-364deeb5-0c54-4452-ad1c-8965d15ff22e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-364deeb5-0c54-4452-ad1c-8965d15ff22e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_395aa120-04a2-4c79-9001-915f1d498247\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('report_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_395aa120-04a2-4c79-9001-915f1d498247 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('report_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "report_df",
              "summary": "{\n  \"name\": \"report_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Model Strategy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Pruned (tau_BCS=0.3)\",\n          \"Baseline (no pruning)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dataset (Images)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AV2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0014793291004758877,\n        \"min\": 0.9419974283143011,\n        \"max\": 0.9440895155914073,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.9440895155914073\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04609451887608289,\n        \"min\": 0.7525337778376321,\n        \"max\": 0.8177212715832511,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8177212715832511\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1-Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.028071916196693098,\n        \"min\": 0.8366737430018357,\n        \"max\": 0.876373427609,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.876373427609\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP50\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013439225406260061,\n        \"min\": 0.8624508207877787,\n        \"max\": 0.8814567556251007,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8814567556251007\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP50-95\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.014491401425897997,\n        \"min\": 0.7216288428174538,\n        \"max\": 0.7421227792517515,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.7421227792517515\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Box Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004589123009900691,\n        \"min\": 0.57838,\n        \"max\": 0.58487,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.58487\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **threhsold = 0.5**"
      ],
      "metadata": {
        "id": "CMAJrMZ8rxlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rXrujGCDlcDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_ROOT = Path(\"/content/drive/MyDrive/av2_redundancy_yolo\")\n",
        "\n",
        "baseline_dir = build_yolo_from_av2_scene(\n",
        "    scene_path=SCENE_PATH,\n",
        "    ann_df=ann_df,\n",
        "    INTR=INTR,\n",
        "    T_EGO_SENSOR=T_EGO_SENSOR,\n",
        "    T_SENSOR_EGO=T_SENSOR_EGO,\n",
        "    IMG_INDEX=IMG_INDEX,\n",
        "    overlap_pairs=OVERLAP_PAIRS,\n",
        "    out_root=OUT_ROOT / \"baseline_tau1.0\",\n",
        "    tau_bcs=1.0,\n",
        "    train_ratio=0.8,\n",
        "    max_frames=None,\n",
        "    drop_empty_images=False\n",
        ")\n",
        "\n",
        "pruned_dir = build_yolo_from_av2_scene(\n",
        "    scene_path=SCENE_PATH,\n",
        "    ann_df=ann_df,\n",
        "    INTR=INTR,\n",
        "    T_EGO_SENSOR=T_EGO_SENSOR,\n",
        "    T_SENSOR_EGO=T_SENSOR_EGO,\n",
        "    IMG_INDEX=IMG_INDEX,\n",
        "    overlap_pairs=OVERLAP_PAIRS,\n",
        "    out_root=OUT_ROOT / \"pruned_tau0.5\",\n",
        "    tau_bcs=0.5,\n",
        "    train_ratio=0.8,\n",
        "    max_frames=None,\n",
        "    drop_empty_images=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23gVNXo4pQya",
        "outputId": "ed1c66fe-21b3-42a7-8fc3-f1de30fa4b5a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exporting (tau_bcs=1.0): 100%|██████████| 157/157 [00:28<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. kept_images=1413, dropped_images=0\n",
            "data.yaml: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/data.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exporting (tau_bcs=0.5): 100%|██████████| 157/157 [00:29<00:00,  5.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. kept_images=1413, dropped_images=0\n",
            "data.yaml: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/data.yaml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval_yolo(data_dir: Path, run_name: str, epochs: int = 30, imgsz: int = 640, batch: int = 16):\n",
        "    model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "    train_res = model.train(\n",
        "        data=str(data_dir / \"data.yaml\"),\n",
        "        epochs=epochs,\n",
        "        imgsz=imgsz,\n",
        "        batch=batch,\n",
        "        device=0,          # force GPU\n",
        "        cache=True,        # recommended\n",
        "        workers=4,         # recommended\n",
        "        name=run_name,\n",
        "        project=str(data_dir / \"runs\"),\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    metrics = model.val(\n",
        "        data=str(data_dir / \"data.yaml\"),\n",
        "        device=0           # force GPU for eval too\n",
        "    )\n",
        "\n",
        "    precision = float(metrics.box.mp)\n",
        "    recall    = float(metrics.box.mr)\n",
        "    map50     = float(metrics.box.map50)\n",
        "    map5095   = float(metrics.box.map)\n",
        "    f1        = (2 * precision * recall / (precision + recall + 1e-12))\n",
        "\n",
        "    box_loss = None\n",
        "    results_csv = Path(train_res.save_dir) / \"results.csv\"\n",
        "    if results_csv.exists():\n",
        "        df = pd.read_csv(results_csv)\n",
        "        box_cols = [c for c in df.columns if \"train/box_loss\" in c.lower()]\n",
        "        if box_cols:\n",
        "            box_loss = float(df[box_cols[0]].iloc[-1])\n",
        "\n",
        "    return {\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1,\n",
        "        \"mAP50\": map50,\n",
        "        \"mAP50-95\": map5095,\n",
        "        \"Box Loss\": box_loss\n",
        "    }\n"
      ],
      "metadata": {
        "id": "fQEvHsGTqDRy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "base_metrics = train_and_eval_yolo(baseline_dir, run_name=\"baseline_tau1.0\", epochs=30, imgsz=640, batch=16)\n",
        "rows.append({\n",
        "    \"Model Strategy\": \"Baseline (no pruning)\",\n",
        "    \"Dataset (Images)\": \"AV2\",\n",
        "    **base_metrics\n",
        "})\n",
        "\n",
        "pruned_metrics = train_and_eval_yolo(pruned_dir, run_name=\"pruned_tau0.5\", epochs=30, imgsz=640, batch=16)\n",
        "rows.append({\n",
        "    \"Model Strategy\": \"Pruned (tau_BCS=0.5)\",\n",
        "    \"Dataset (Images)\": \"AV2\",\n",
        "    **pruned_metrics\n",
        "})\n",
        "\n",
        "report_df = pd.DataFrame(rows, columns=[\n",
        "    \"Model Strategy\", \"Dataset (Images)\", \"Precision\", \"Recall\", \"F1-Score\", \"mAP50\", \"mAP50-95\", \"Box Loss\"\n",
        "])\n",
        "\n",
        "report_df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RpttqOarqKTx",
        "outputId": "34e9d6ba-a238-4831-97b5-80885f64f495"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=baseline_tau1.0, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 122.5±62.7 MB/s, size: 321.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/train... 1125 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1125/1125 102.7it/s 11.0s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/train.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB RAM): 100% ━━━━━━━━━━━━ 1125/1125 236.8it/s 4.8s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mSlow image access detected (ping: 14.7±31.4 ms, read: 72.2±67.4 MB/s, size: 257.2 KB). Use local storage instead of remote/mounted storage for better performance. See https://docs.ultralytics.com/guides/model-training-tips/\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val... 288 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 203.4it/s 1.4s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB RAM): 100% ━━━━━━━━━━━━ 288/288 241.9it/s 1.2s\n",
            "Plotting labels to /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      2.93G      1.686      2.489      1.252        125        640: 100% ━━━━━━━━━━━━ 71/71 9.1it/s 7.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.1it/s 1.3s\n",
            "                   all        288       3981      0.954       0.23      0.467      0.314\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      2.93G      1.342      1.258      1.109        190        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 6.8it/s 1.3s\n",
            "                   all        288       3981      0.829      0.533      0.608      0.387\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      2.93G       1.23      1.075       1.06        130        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.0it/s 1.3s\n",
            "                   all        288       3981      0.868      0.588      0.653      0.476\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      2.93G      1.132     0.9759      1.013        145        640: 100% ━━━━━━━━━━━━ 71/71 10.8it/s 6.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 6.9it/s 1.3s\n",
            "                   all        288       3981      0.918      0.628      0.688      0.526\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      2.93G       1.06     0.8974     0.9953         62        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.3it/s 1.2s\n",
            "                   all        288       3981      0.929      0.662      0.715      0.553\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      2.93G      1.003     0.8299     0.9734         93        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.2it/s 1.2s\n",
            "                   all        288       3981      0.906      0.686      0.725      0.562\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      2.93G     0.9634     0.7933     0.9631         81        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.4it/s 1.2s\n",
            "                   all        288       3981      0.929      0.687      0.741      0.578\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      2.93G     0.9364     0.7497     0.9478        113        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981      0.913      0.703      0.746      0.595\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30      2.93G      0.895     0.7104     0.9351        144        640: 100% ━━━━━━━━━━━━ 71/71 10.7it/s 6.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981      0.929      0.705      0.751      0.593\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      2.93G     0.8674     0.6819     0.9257         95        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.4it/s 1.2s\n",
            "                   all        288       3981      0.962      0.697      0.759      0.613\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      2.93G      0.845     0.6621     0.9203        110        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.957      0.701      0.762      0.614\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30      2.93G     0.8207     0.6417     0.9149        107        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.917      0.717      0.767      0.608\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30      2.93G     0.8084     0.6303     0.9112         77        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981       0.92       0.71      0.762      0.619\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30      2.95G     0.7931     0.6187     0.9099        107        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3981      0.934      0.719      0.767      0.632\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      3.17G     0.7913     0.6129     0.9131        142        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981      0.804      0.723      0.774       0.64\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      3.19G     0.7618     0.5831      0.896        137        640: 100% ━━━━━━━━━━━━ 71/71 11.7it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981      0.808      0.723      0.774      0.637\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30       3.2G     0.7565      0.572     0.8967         99        640: 100% ━━━━━━━━━━━━ 71/71 10.8it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.808      0.733      0.776      0.644\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      3.22G     0.7349     0.5604     0.8942        152        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.825      0.731      0.772      0.645\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      3.24G     0.7234     0.5488     0.8833        143        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981      0.811       0.74      0.778      0.655\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30      3.26G     0.7182     0.5458     0.8883        104        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3981      0.826      0.738      0.782      0.658\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30      3.27G     0.7025      0.543     0.8762         73        640: 100% ━━━━━━━━━━━━ 71/71 10.3it/s 6.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.947      0.735      0.774      0.647\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      3.29G     0.6854     0.5209     0.8652         80        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.944      0.737      0.851      0.681\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      3.31G     0.6604      0.504     0.8577         63        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3981      0.942      0.732       0.87      0.691\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      3.33G     0.6445     0.4915     0.8555         50        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3981       0.95      0.741      0.862      0.701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      3.33G     0.6311      0.481     0.8521         61        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3981      0.954      0.739      0.862      0.692\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30      3.36G     0.6235     0.4742     0.8499         44        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.944      0.749      0.864      0.711\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30      3.38G       0.61     0.4632     0.8497         50        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.949      0.744      0.862      0.713\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30       3.4G     0.6008     0.4605     0.8484         58        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.946      0.756      0.865      0.717\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30       3.4G     0.5856     0.4494     0.8408         47        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3981      0.939      0.754      0.861      0.715\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      3.43G     0.5784     0.4484     0.8409         17        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.4it/s 1.2s\n",
            "                   all        288       3981      0.942      0.752      0.867      0.723\n",
            "\n",
            "30 epochs completed in 0.066 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0/weights/best.pt...\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 3.5it/s 2.6s\n",
            "                   all        288       3981      0.942      0.752      0.867      0.723\n",
            "               BOLLARD         11         11          1          0      0.657      0.321\n",
            "             BOX_TRUCK         60         60      0.878      0.838      0.866      0.691\n",
            "                   BUS         96         96      0.994          1      0.995       0.96\n",
            "     CONSTRUCTION_CONE         32         94      0.982      0.979      0.988      0.919\n",
            "         LARGE_VEHICLE        104        252      0.959      0.897       0.94      0.806\n",
            "            PEDESTRIAN        248       1402      0.876      0.595      0.683      0.512\n",
            "       REGULAR_VEHICLE        237       1971      0.886      0.719      0.816      0.622\n",
            "                 TRUCK         95         95      0.961      0.989      0.993      0.955\n",
            "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 4.9ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/runs/baseline_tau1.0\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.4±0.1 ms, read: 167.8±59.1 MB/s, size: 320.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/baseline_tau1.0/labels/val.cache... 288 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 357.8Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 18/18 4.4it/s 4.1s\n",
            "                   all        288       3981      0.942      0.753      0.862      0.722\n",
            "               BOLLARD         11         11          1          0      0.617      0.306\n",
            "             BOX_TRUCK         60         60      0.878      0.839      0.867      0.691\n",
            "                   BUS         96         96      0.994          1      0.995      0.966\n",
            "     CONSTRUCTION_CONE         32         94      0.984      0.979      0.988      0.912\n",
            "         LARGE_VEHICLE        104        252      0.959      0.897      0.939      0.803\n",
            "            PEDESTRIAN        248       1402      0.875      0.596      0.683      0.515\n",
            "       REGULAR_VEHICLE        237       1971      0.884       0.72      0.817      0.626\n",
            "                 TRUCK         95         95      0.961      0.989      0.993      0.954\n",
            "Speed: 1.3ms preprocess, 0.7ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val24\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=pruned_tau0.5, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/runs/pruned_tau0.5, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=8\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,012,408 parameters, 3,012,392 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 144.9±45.4 MB/s, size: 321.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/labels/train... 1125 images, 7 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1125/1125 109.9it/s 10.2s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/labels/train.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (1.0GB RAM): 100% ━━━━━━━━━━━━ 1125/1125 251.0it/s 4.5s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.3±0.0 ms, read: 106.4±28.1 MB/s, size: 257.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/labels/val... 288 images, 5 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 162.8it/s 1.8s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/labels/val.cache\n",
            "WARNING ⚠️ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB RAM): 100% ━━━━━━━━━━━━ 288/288 246.7it/s 1.2s\n",
            "Plotting labels to /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/runs/pruned_tau0.5/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/runs/pruned_tau0.5\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/30      2.79G      1.705      2.468      1.236        119        640: 100% ━━━━━━━━━━━━ 71/71 9.0it/s 7.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.2it/s 1.2s\n",
            "                   all        288       3792      0.934      0.158      0.463      0.319\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/30      2.79G      1.338      1.234      1.092        183        640: 100% ━━━━━━━━━━━━ 71/71 10.6it/s 6.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 6.8it/s 1.3s\n",
            "                   all        288       3792      0.824      0.521      0.608      0.423\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/30      2.79G      1.246      1.076      1.051        121        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.1it/s 1.3s\n",
            "                   all        288       3792      0.825      0.609      0.673      0.497\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/30      2.79G      1.148     0.9833      1.005        143        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.0it/s 1.3s\n",
            "                   all        288       3792      0.897      0.667       0.72      0.544\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/30      2.79G       1.07     0.9005     0.9812         60        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.1it/s 1.3s\n",
            "                   all        288       3792      0.906      0.648      0.715      0.545\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/30      2.79G      1.017     0.8432     0.9657         86        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.5it/s 1.2s\n",
            "                   all        288       3792      0.883      0.677      0.721      0.562\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/30      2.79G     0.9642     0.7908     0.9502         77        640: 100% ━━━━━━━━━━━━ 71/71 10.9it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.5it/s 1.2s\n",
            "                   all        288       3792      0.916      0.691      0.744      0.578\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/30      2.79G     0.9492     0.7535     0.9394        111        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.6it/s 1.2s\n",
            "                   all        288       3792      0.929      0.706      0.756      0.598\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/30      2.99G     0.9065     0.7149     0.9274        137        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3792      0.915      0.705      0.756        0.6\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/30      2.99G     0.8662      0.687     0.9165         93        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.3it/s 1.2s\n",
            "                   all        288       3792      0.926      0.717      0.759      0.611\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/30      2.99G     0.8571     0.6711     0.9153        106        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3792      0.927       0.73      0.772      0.628\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/30       3.2G     0.8335     0.6495       0.91        103        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3792      0.897      0.724      0.766      0.623\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/30       3.2G      0.814     0.6282     0.9027         73        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3792      0.919      0.729      0.772       0.63\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/30       3.2G     0.7951      0.618      0.899        102        640: 100% ━━━━━━━━━━━━ 71/71 11.0it/s 6.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3792      0.808       0.73      0.775      0.642\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/30      3.41G     0.7916     0.6077     0.9004        140        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3792      0.819      0.728      0.782      0.652\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/30      3.42G      0.761     0.5807     0.8856        135        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.0it/s 1.1s\n",
            "                   all        288       3792      0.912      0.751      0.785      0.656\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/30      3.43G     0.7637     0.5742     0.8907         95        640: 100% ━━━━━━━━━━━━ 71/71 10.8it/s 6.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3792      0.795      0.738      0.779      0.652\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/30      3.46G     0.7361     0.5593     0.8863        146        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3792      0.802      0.739      0.783      0.663\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/30      3.48G     0.7267     0.5487     0.8752        136        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.3it/s 1.1s\n",
            "                   all        288       3792      0.798      0.743      0.783      0.655\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/30      3.49G      0.721     0.5448     0.8802        103        640: 100% ━━━━━━━━━━━━ 71/71 10.6it/s 6.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.1s\n",
            "                   all        288       3792      0.934      0.753      0.859      0.714\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/30       3.5G     0.7074     0.5452     0.8687         71        640: 100% ━━━━━━━━━━━━ 71/71 10.0it/s 7.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3792      0.942      0.745      0.844      0.685\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/30      3.53G     0.6897     0.5229     0.8588         79        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3792      0.939      0.751      0.853      0.701\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/30      3.54G     0.6647     0.5071     0.8514         63        640: 100% ━━━━━━━━━━━━ 71/71 11.1it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3792      0.923      0.762      0.853       0.69\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/30      3.56G     0.6506     0.4954     0.8497         49        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.8it/s 1.2s\n",
            "                   all        288       3792      0.942      0.756      0.856      0.703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/30      3.57G      0.639     0.4851     0.8478         56        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.9it/s 1.1s\n",
            "                   all        288       3792      0.944      0.762      0.863      0.705\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/30       3.6G     0.6272     0.4769     0.8443         41        640: 100% ━━━━━━━━━━━━ 71/71 11.5it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 7.7it/s 1.2s\n",
            "                   all        288       3792       0.93      0.765       0.88      0.724\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/30      3.61G     0.6122     0.4652     0.8431         48        640: 100% ━━━━━━━━━━━━ 71/71 11.4it/s 6.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3792      0.768      0.865      0.867      0.723\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/30      3.63G      0.607     0.4644     0.8435         55        640: 100% ━━━━━━━━━━━━ 71/71 11.3it/s 6.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3792       0.81      0.847      0.856      0.727\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/30      3.64G     0.5946     0.4536     0.8368         46        640: 100% ━━━━━━━━━━━━ 71/71 11.6it/s 6.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.2it/s 1.1s\n",
            "                   all        288       3792      0.872      0.852       0.87       0.73\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/30      3.67G     0.5842     0.4517     0.8371         16        640: 100% ━━━━━━━━━━━━ 71/71 11.2it/s 6.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 8.1it/s 1.1s\n",
            "                   all        288       3792      0.863      0.854      0.877      0.725\n",
            "\n",
            "30 epochs completed in 0.067 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/runs/pruned_tau0.5/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/runs/pruned_tau0.5/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/runs/pruned_tau0.5/weights/best.pt...\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 3.6it/s 2.5s\n",
            "                   all        288       3792      0.872      0.852       0.87       0.73\n",
            "               BOLLARD         11         11      0.868      0.545      0.602      0.277\n",
            "             BOX_TRUCK         48         48      0.748      0.896      0.878      0.733\n",
            "                   BUS         96         96      0.992          1      0.995      0.968\n",
            "     CONSTRUCTION_CONE         32         94       0.99      0.989      0.988      0.902\n",
            "         LARGE_VEHICLE        102        228      0.943      0.978       0.99      0.887\n",
            "            PEDESTRIAN        248       1374      0.793      0.647      0.696      0.521\n",
            "       REGULAR_VEHICLE        225       1885      0.797      0.762      0.819      0.627\n",
            "                 TRUCK         56         56      0.843          1      0.992      0.926\n",
            "Speed: 0.1ms preprocess, 0.5ms inference, 0.0ms loss, 5.1ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/runs/pruned_tau0.5\u001b[0m\n",
            "Ultralytics 8.3.241 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-80GB, 81222MiB)\n",
            "Model summary (fused): 72 layers, 3,007,208 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 7.2±15.2 ms, read: 108.7±69.6 MB/s, size: 320.5 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/av2_redundancy_yolo/pruned_tau0.5/labels/val.cache... 288 images, 5 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 288/288 403.6Kit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 18/18 4.5it/s 4.0s\n",
            "                   all        288       3792      0.873      0.852       0.87       0.73\n",
            "               BOLLARD         11         11      0.874      0.545      0.602      0.268\n",
            "             BOX_TRUCK         48         48      0.748      0.896      0.879      0.737\n",
            "                   BUS         96         96      0.992          1      0.995      0.971\n",
            "     CONSTRUCTION_CONE         32         94       0.99      0.989      0.988      0.901\n",
            "         LARGE_VEHICLE        102        228      0.947      0.978      0.991       0.89\n",
            "            PEDESTRIAN        248       1374      0.792      0.646      0.695      0.522\n",
            "       REGULAR_VEHICLE        225       1885      0.801      0.764       0.82      0.631\n",
            "                 TRUCK         56         56      0.843          1      0.992      0.924\n",
            "Speed: 1.5ms preprocess, 3.5ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/val25\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Model Strategy Dataset (Images)  Precision    Recall  F1-Score  \\\n",
              "0  Baseline (no pruning)              AV2   0.941997  0.752534  0.836674   \n",
              "1   Pruned (tau_BCS=0.5)              AV2   0.873335  0.852320  0.862700   \n",
              "\n",
              "      mAP50  mAP50-95  Box Loss  \n",
              "0  0.862451  0.721629   0.57838  \n",
              "1  0.870235  0.730433   0.58421  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed13b0c1-0a30-4a9f-b54f-71e23ede7840\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Strategy</th>\n",
              "      <th>Dataset (Images)</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>mAP50</th>\n",
              "      <th>mAP50-95</th>\n",
              "      <th>Box Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline (no pruning)</td>\n",
              "      <td>AV2</td>\n",
              "      <td>0.941997</td>\n",
              "      <td>0.752534</td>\n",
              "      <td>0.836674</td>\n",
              "      <td>0.862451</td>\n",
              "      <td>0.721629</td>\n",
              "      <td>0.57838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pruned (tau_BCS=0.5)</td>\n",
              "      <td>AV2</td>\n",
              "      <td>0.873335</td>\n",
              "      <td>0.852320</td>\n",
              "      <td>0.862700</td>\n",
              "      <td>0.870235</td>\n",
              "      <td>0.730433</td>\n",
              "      <td>0.58421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed13b0c1-0a30-4a9f-b54f-71e23ede7840')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed13b0c1-0a30-4a9f-b54f-71e23ede7840 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed13b0c1-0a30-4a9f-b54f-71e23ede7840');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-18fa8483-3f5a-4304-aac6-c50dcc4c88ad\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-18fa8483-3f5a-4304-aac6-c50dcc4c88ad')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-18fa8483-3f5a-4304-aac6-c50dcc4c88ad button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3049d5b9-5b11-4072-93c7-e3b46d422b90\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('report_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3049d5b9-5b11-4072-93c7-e3b46d422b90 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('report_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "report_df",
              "summary": "{\n  \"name\": \"report_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Model Strategy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Pruned (tau_BCS=0.5)\",\n          \"Baseline (no pruning)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dataset (Images)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"AV2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0485514927368592,\n        \"min\": 0.873335248812376,\n        \"max\": 0.9419974283143011,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.873335248812376\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07055953500988879,\n        \"min\": 0.7525337778376321,\n        \"max\": 0.852320029203356,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.852320029203356\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1-Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.018403113837326575,\n        \"min\": 0.8366737430018357,\n        \"max\": 0.8626996761804789,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8626996761804789\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP50\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005504434084721996,\n        \"min\": 0.8624508207877787,\n        \"max\": 0.8702352661235813,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8702352661235813\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mAP50-95\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006225559448368418,\n        \"min\": 0.7216288428174538,\n        \"max\": 0.7304331134226963,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.7304331134226963\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Box Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004122432534317573,\n        \"min\": 0.57838,\n        \"max\": 0.58421,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.58421\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}