{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehri-satari/Data-Mining-Course-Project/blob/main/change(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX3FUPc68E98"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 0 — Colab installs + imports + Drive mount\n",
        "# ============================================================\n",
        "\n",
        "# --- Colab installs (run once) ---\n",
        "!pip -q install ultralytics opencv-python pillow tqdm\n",
        "!pip -q install av2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "import shutil\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, List, Optional, Any, Set\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from av2.utils import io as io_utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvLpelbt-9W1"
      },
      "outputs": [],
      "source": [
        "LOG_IDS = [\n",
        "    \"0526e68e-2ff1-3e53-b0f8-45df02e45a93\",\n",
        "    \"04973bcf-fc64-367c-9642-6d6c5f363b61\",\n",
        "    \"03fba633-8085-30bc-b675-687a715536ac\",\n",
        "    \"03b2cf2d-fb61-36fe-936f-36bbf197a8ac\",\n",
        "    \"0322b098-7e42-34db-bcec-9a4d072191e9\",\n",
        "    \"022af476-9937-3e70-be52-f65420d52703\",\n",
        "    \"01bb304d-7bd8-35f8-bbef-7086b688e35e\",\n",
        "    \"00a6ffc1-6ce9-3bc3-a060-6006e9893a1a\",\n",
        "]\n",
        "from pathlib import Path\n",
        "\n",
        "def find_av2_root_by_log_ids(\n",
        "    log_ids,\n",
        "    search_roots=(Path(\"/content/drive/MyDrive\"), Path(\"/content/drive/Shareddrives\")),\n",
        "    max_matches_per_root=5\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Finds the parent directory that contains the AV2 log folders.\n",
        "    We search for the first log_id directory, then validate that the other log_ids\n",
        "    exist alongside it.\n",
        "    Returns: AV2_ROOT such that AV2_ROOT/<log_id>/annotations.feather exists.\n",
        "    \"\"\"\n",
        "    first = log_ids[0]\n",
        "\n",
        "    for root in search_roots:\n",
        "        if not root.exists():\n",
        "            continue\n",
        "\n",
        "        # Find candidate directories named like the first log id\n",
        "        candidates = []\n",
        "        for p in root.rglob(first):\n",
        "            if p.is_dir():\n",
        "                candidates.append(p)\n",
        "                if len(candidates) >= max_matches_per_root:\n",
        "                    break\n",
        "\n",
        "        for log_dir in candidates:\n",
        "            av2_root = log_dir.parent  # should be .../Argoverse2\n",
        "            # Validate: all logs exist and look like AV2 logs\n",
        "            ok = True\n",
        "            for lid in log_ids:\n",
        "                lid_dir = av2_root / lid\n",
        "                if not lid_dir.is_dir():\n",
        "                    ok = False\n",
        "                    break\n",
        "                if not (lid_dir / \"annotations.feather\").exists():\n",
        "                    ok = False\n",
        "                    break\n",
        "                if not (lid_dir / \"calibration\" / \"intrinsics.feather\").exists():\n",
        "                    ok = False\n",
        "                    break\n",
        "                if not (lid_dir / \"calibration\" / \"egovehicle_SE3_sensor.feather\").exists():\n",
        "                    ok = False\n",
        "                    break\n",
        "\n",
        "            if ok:\n",
        "                return av2_root\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not locate the AV2 root folder in mounted Drive.\\n\"\n",
        "        \"Make sure the shared folder is added as a Shortcut into MyDrive or is in a Shared Drive, \"\n",
        "        \"and that the log folders contain annotations.feather and calibration/*.feather.\"\n",
        "    )\n",
        "\n",
        "AV2_ROOT = find_av2_root_by_log_ids(LOG_IDS)\n",
        "print(\"✅ Found AV2_ROOT:\", AV2_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyNOFTl67-2D"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 1 — Configure logs + thresholds + output\n",
        "# ============================================================\n",
        "\n",
        "AV2_ROOT = Path(\"/content/drive/MyDrive/Argoverse2\")\n",
        "\n",
        "LOG_IDS = [\n",
        "    \"0526e68e-2ff1-3e53-b0f8-45df02e45a93\",\n",
        "    \"04973bcf-fc64-367c-9642-6d6c5f363b61\",\n",
        "    \"03fba633-8085-30bc-b675-687a715536ac\",\n",
        "    \"03b2cf2d-fb61-36fe-936f-36bbf197a8ac\",\n",
        "    \"0322b098-7e42-34db-bcec-9a4d072191e9\",\n",
        "    \"022af476-9937-3e70-be52-f65420d52703\",\n",
        "    \"01bb304d-7bd8-35f8-bbef-7086b688e35e\",\n",
        "    \"00a6ffc1-6ce9-3bc3-a060-6006e9893a1a\",\n",
        "]\n",
        "\n",
        "# pruning thresholds requested\n",
        "TAU_LIST = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "\n",
        "# output root\n",
        "OUT_ROOT = Path(\"/content/drive/MyDrive/av2_redundancy_yolo_multi_logs\")\n",
        "\n",
        "# global split seed\n",
        "SPLIT_SEED = 7\n",
        "\n",
        "# max allowable timestamp mismatch when pairing annotation timestamp to image timestamp (ns)\n",
        "MAX_TS_DIFF_NS = 50_000_000  # 50ms\n",
        "\n",
        "# If you also want to train YOLO for each tau, set TRAIN_MODELS=True.\n",
        "# (This can take a long time.)\n",
        "TRAIN_MODELS = False\n",
        "EPOCHS = 30\n",
        "IMGSZ = 640\n",
        "BATCH = 16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vzvYNDaAH36"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 2 — Geometry utilities (same logic as your notebook)\n",
        "# ============================================================\n",
        "\n",
        "def quat_to_rotmat(qw, qx, qy, qz) -> np.ndarray:\n",
        "    q = np.array([qw, qx, qy, qz], dtype=np.float64)\n",
        "    q = q / (np.linalg.norm(q) + 1e-12)\n",
        "    w, x, y, z = q\n",
        "    R = np.array([\n",
        "        [1-2*(y*y+z*z), 2*(x*y - z*w), 2*(x*z + y*w)],\n",
        "        [2*(x*y + z*w), 1-2*(x*x+z*z), 2*(y*z - x*w)],\n",
        "        [2*(x*z - y*w), 2*(y*z + x*w), 1-2*(x*x+y*y)],\n",
        "    ], dtype=np.float64)\n",
        "    return R\n",
        "\n",
        "@dataclass\n",
        "class SE3:\n",
        "    R: np.ndarray  # 3x3\n",
        "    t: np.ndarray  # (3,)\n",
        "\n",
        "    def inverse(self) -> \"SE3\":\n",
        "        R_inv = self.R.T\n",
        "        t_inv = -R_inv @ self.t\n",
        "        return SE3(R=R_inv, t=t_inv)\n",
        "\n",
        "    def transform_points(self, pts: np.ndarray) -> np.ndarray:\n",
        "        # pts: (N,3)\n",
        "        return (pts @ self.R.T) + self.t.reshape(1, 3)\n",
        "\n",
        "@dataclass\n",
        "class CameraIntrinsics:\n",
        "    fx: float\n",
        "    fy: float\n",
        "    cx: float\n",
        "    cy: float\n",
        "\n",
        "def build_intrinsics_dict(intr_df: pd.DataFrame) -> Dict[str, CameraIntrinsics]:\n",
        "    req = [\"sensor_name\", \"fx_px\", \"fy_px\", \"cx_px\", \"cy_px\"]\n",
        "    for c in req:\n",
        "        if c not in intr_df.columns:\n",
        "            raise ValueError(f\"Missing intrinsics column: {c}\")\n",
        "\n",
        "    intr = {}\n",
        "    for _, r in intr_df.iterrows():\n",
        "        intr[str(r[\"sensor_name\"])] = CameraIntrinsics(\n",
        "            fx=float(r[\"fx_px\"]),\n",
        "            fy=float(r[\"fy_px\"]),\n",
        "            cx=float(r[\"cx_px\"]),\n",
        "            cy=float(r[\"cy_px\"]),\n",
        "        )\n",
        "    return intr\n",
        "\n",
        "def build_extrinsics_dict(extr_df: pd.DataFrame) -> Dict[str, SE3]:\n",
        "    if \"sensor_name\" not in extr_df.columns:\n",
        "        raise ValueError(\"extrinsics missing 'sensor_name'\")\n",
        "\n",
        "    quat_cols = [c for c in [\"qw\", \"qx\", \"qy\", \"qz\"] if c in extr_df.columns]\n",
        "    trans_cols = [c for c in [\"tx_m\", \"ty_m\", \"tz_m\"] if c in extr_df.columns]\n",
        "\n",
        "    extr: Dict[str, SE3] = {}\n",
        "    for _, r in extr_df.iterrows():\n",
        "        name = str(r[\"sensor_name\"])\n",
        "        if len(quat_cols) == 4 and len(trans_cols) == 3:\n",
        "            R = quat_to_rotmat(float(r[\"qw\"]), float(r[\"qx\"]), float(r[\"qy\"]), float(r[\"qz\"]))\n",
        "            t = np.array([float(r[\"tx_m\"]), float(r[\"ty_m\"]), float(r[\"tz_m\"])], dtype=np.float64)\n",
        "            extr[name] = SE3(R=R, t=t)\n",
        "        else:\n",
        "            mat_col = None\n",
        "            for cand in [\"T_egovehicle_sensor\", \"egovehicle_SE3_sensor\", \"transform_matrix\"]:\n",
        "                if cand in extr_df.columns:\n",
        "                    mat_col = cand\n",
        "                    break\n",
        "            if mat_col is None:\n",
        "                raise ValueError(\n",
        "                    \"Extrinsics format not recognized. Expected qw/qx/qy/qz + tx_m/ty_m/tz_m OR a 4x4 matrix column.\"\n",
        "                )\n",
        "            T = np.array(r[mat_col], dtype=np.float64).reshape(4, 4)\n",
        "            extr[name] = SE3(R=T[:3, :3], t=T[:3, 3])\n",
        "    return extr\n",
        "\n",
        "def get_col(available_cols, candidates):\n",
        "    for c in candidates:\n",
        "        if c in available_cols:\n",
        "            return c\n",
        "    raise KeyError(f\"None of {candidates} found in columns.\")\n",
        "\n",
        "def cuboid_corners_ego(row: pd.Series) -> np.ndarray:\n",
        "    cols = set(row.index)\n",
        "\n",
        "    cx = float(row[get_col(cols, [\"center_x\", \"tx_m\", \"x\", \"translation_x\"])])\n",
        "    cy = float(row[get_col(cols, [\"center_y\", \"ty_m\", \"y\", \"translation_y\"])])\n",
        "    cz = float(row[get_col(cols, [\"center_z\", \"tz_m\", \"z\", \"translation_z\"])])\n",
        "\n",
        "    length = float(row[get_col(cols, [\"length_m\", \"length\"])])\n",
        "    width  = float(row[get_col(cols, [\"width_m\", \"width\"])])\n",
        "    height = float(row[get_col(cols, [\"height_m\", \"height\"])])\n",
        "\n",
        "    qw = float(row[get_col(cols, [\"qw\", \"rotation_qw\"])])\n",
        "    qx = float(row[get_col(cols, [\"qx\", \"rotation_qx\"])])\n",
        "    qy = float(row[get_col(cols, [\"qy\", \"rotation_qy\"])])\n",
        "    qz = float(row[get_col(cols, [\"qz\", \"rotation_qz\"])])\n",
        "\n",
        "    R = quat_to_rotmat(qw, qx, qy, qz)\n",
        "    center = np.array([cx, cy, cz], dtype=np.float64)\n",
        "\n",
        "    l2, w2, h2 = length / 2, width / 2, height / 2\n",
        "    corners_local = np.array([\n",
        "        [ l2,  w2,  h2],\n",
        "        [ l2, -w2,  h2],\n",
        "        [-l2, -w2,  h2],\n",
        "        [-l2,  w2,  h2],\n",
        "        [ l2,  w2, -h2],\n",
        "        [ l2, -w2, -h2],\n",
        "        [-l2, -w2, -h2],\n",
        "        [-l2,  w2, -h2],\n",
        "    ], dtype=np.float64)\n",
        "\n",
        "    return (corners_local @ R.T) + center.reshape(1, 3)\n",
        "\n",
        "@dataclass\n",
        "class Box2D:\n",
        "    xmin: float\n",
        "    ymin: float\n",
        "    xmax: float\n",
        "    ymax: float\n",
        "    bcs: float\n",
        "\n",
        "def project_points_to_image(pts_cam: np.ndarray, intr: CameraIntrinsics) -> np.ndarray:\n",
        "    x, y, z = pts_cam[:, 0], pts_cam[:, 1], pts_cam[:, 2]\n",
        "    eps = 1e-9\n",
        "    u = intr.fx * (x / (z + eps)) + intr.cx\n",
        "    v = intr.fy * (y / (z + eps)) + intr.cy\n",
        "    return np.stack([u, v, z], axis=1)\n",
        "\n",
        "def bbox_and_bcs_from_cuboid(\n",
        "    corners_ego: np.ndarray,\n",
        "    intr: CameraIntrinsics,\n",
        "    T_sensor_ego: SE3,   # ego -> camera\n",
        "    img_w: int,\n",
        "    img_h: int\n",
        ") -> Optional[Box2D]:\n",
        "    corners_cam = T_sensor_ego.transform_points(corners_ego)\n",
        "\n",
        "    if np.all(corners_cam[:, 2] <= 0.1):\n",
        "        return None\n",
        "\n",
        "    uvz = project_points_to_image(corners_cam, intr)\n",
        "    u, v, z = uvz[:, 0], uvz[:, 1], uvz[:, 2]\n",
        "\n",
        "    valid = z > 0.1\n",
        "    if valid.sum() < 2:\n",
        "        return None\n",
        "\n",
        "    u_full = u[valid]\n",
        "    v_full = v[valid]\n",
        "\n",
        "    xmin_full, xmax_full = float(u_full.min()), float(u_full.max())\n",
        "    ymin_full, ymax_full = float(v_full.min()), float(v_full.max())\n",
        "\n",
        "    full_w = max(0.0, xmax_full - xmin_full)\n",
        "    full_h = max(0.0, ymax_full - ymin_full)\n",
        "    area_full = full_w * full_h\n",
        "    if area_full <= 1e-6:\n",
        "        return None\n",
        "\n",
        "    xmin_clip = max(0.0, min(float(img_w - 1), xmin_full))\n",
        "    xmax_clip = max(0.0, min(float(img_w - 1), xmax_full))\n",
        "    ymin_clip = max(0.0, min(float(img_h - 1), ymin_full))\n",
        "    ymax_clip = max(0.0, min(float(img_h - 1), ymax_full))\n",
        "\n",
        "    clip_w = max(0.0, xmax_clip - xmin_clip)\n",
        "    clip_h = max(0.0, ymax_clip - ymin_clip)\n",
        "    area_clip = clip_w * clip_h\n",
        "\n",
        "    bcs = float(area_clip / area_full)\n",
        "\n",
        "    if area_clip <= 1.0:\n",
        "        return None\n",
        "\n",
        "    return Box2D(xmin=xmin_clip, ymin=ymin_clip, xmax=xmax_clip, ymax=ymax_clip, bcs=bcs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J1sbO51AV7_"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 3 — Camera image indexing + overlap pairs (same logic)\n",
        "# ============================================================\n",
        "\n",
        "def find_camera_root(scene_path: Path) -> Path:\n",
        "    candidates = [\n",
        "        scene_path / \"sensors\" / \"cameras\",\n",
        "        scene_path / \"sensor\" / \"cameras\",\n",
        "        scene_path / \"cameras\",\n",
        "    ]\n",
        "    for p in candidates:\n",
        "        if p.exists():\n",
        "            return p\n",
        "    raise FileNotFoundError(f\"Cannot find camera root under {scene_path} (tried: {candidates})\")\n",
        "\n",
        "def parse_timestamp_from_filename(p: Path) -> Optional[int]:\n",
        "    stem = p.stem\n",
        "    return int(stem) if stem.isdigit() else None\n",
        "\n",
        "def index_images(cam_root: Path, cameras: List[str]) -> Dict[str, Dict[int, Path]]:\n",
        "    idx: Dict[str, Dict[int, Path]] = {}\n",
        "    for cam in cameras:\n",
        "        cam_dir = cam_root / cam\n",
        "        if not cam_dir.exists():\n",
        "            # keep consistent key presence\n",
        "            idx[cam] = {}\n",
        "            continue\n",
        "        ts_map: Dict[int, Path] = {}\n",
        "        for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\"]:\n",
        "            for p in cam_dir.glob(ext):\n",
        "                ts = parse_timestamp_from_filename(p)\n",
        "                if ts is not None:\n",
        "                    ts_map[ts] = p\n",
        "        idx[cam] = ts_map\n",
        "    return idx\n",
        "\n",
        "def nearest_timestamp(target: int, available_sorted: List[int], max_diff_ns: int = 50_000_000) -> Optional[int]:\n",
        "    if not available_sorted:\n",
        "        return None\n",
        "    arr = np.array(available_sorted, dtype=np.int64)\n",
        "    i = int(np.searchsorted(arr, target))\n",
        "    cand = []\n",
        "    if i < len(arr): cand.append(int(arr[i]))\n",
        "    if i > 0: cand.append(int(arr[i-1]))\n",
        "    best = min(cand, key=lambda x: abs(int(x) - int(target)))\n",
        "    return int(best) if abs(int(best) - int(target)) <= max_diff_ns else None\n",
        "\n",
        "# overlap helpers\n",
        "def wrap_pi(a: float) -> float:\n",
        "    return float((a + np.pi) % (2*np.pi) - np.pi)\n",
        "\n",
        "def fov_segments(center: float, hfov: float):\n",
        "    a1 = wrap_pi(center - hfov/2)\n",
        "    a2 = wrap_pi(center + hfov/2)\n",
        "    if a1 <= a2:\n",
        "        return [(a1, a2)]\n",
        "    return [(a1, np.pi), (-np.pi, a2)]\n",
        "\n",
        "def seg_overlap(s1, s2) -> float:\n",
        "    left = max(s1[0], s2[0])\n",
        "    right = min(s1[1], s2[1])\n",
        "    return max(0.0, right - left)\n",
        "\n",
        "def circular_overlap(center1: float, hfov1: float, center2: float, hfov2: float) -> float:\n",
        "    segs1 = fov_segments(center1, hfov1)\n",
        "    segs2 = fov_segments(center2, hfov2)\n",
        "    ov = 0.0\n",
        "    for a in segs1:\n",
        "        for b in segs2:\n",
        "            ov += seg_overlap(a, b)\n",
        "    return float(min(ov, min(hfov1, hfov2)))\n",
        "\n",
        "def camera_yaw_center_in_ego(T_ego_sensor: SE3) -> float:\n",
        "    # assumes camera forward is +Z in camera frame\n",
        "    forward_cam = np.array([0.0, 0.0, 1.0], dtype=np.float64)\n",
        "    forward_ego = T_ego_sensor.R @ forward_cam\n",
        "    return float(np.arctan2(forward_ego[1], forward_ego[0]))\n",
        "\n",
        "def hfov_from_intrinsics(intr: CameraIntrinsics, img_w: int) -> float:\n",
        "    return float(2.0 * np.arctan(img_w / (2.0 * intr.fx)))\n",
        "\n",
        "def compute_overlap_pairs(\n",
        "    cameras: List[str],\n",
        "    INTR: Dict[str, CameraIntrinsics],\n",
        "    T_EGO_SENSOR: Dict[str, SE3],\n",
        "    IMG_INDEX: Dict[str, Dict[int, Path]],\n",
        "    min_overlap_deg: float = 5.0\n",
        ") -> List[Tuple[str, str, float]]:\n",
        "    min_overlap = math.radians(min_overlap_deg)\n",
        "\n",
        "    # one image per camera to get W,H\n",
        "    cam_sizes = {}\n",
        "    for cam in cameras:\n",
        "        ts_map = IMG_INDEX.get(cam, {})\n",
        "        if not ts_map:\n",
        "            continue\n",
        "        any_path = next(iter(ts_map.values()))\n",
        "        with Image.open(any_path) as im:\n",
        "            cam_sizes[cam] = im.size  # (W,H)\n",
        "\n",
        "    cam_info = {}\n",
        "    for cam in cameras:\n",
        "        if cam not in cam_sizes or cam not in INTR or cam not in T_EGO_SENSOR:\n",
        "            continue\n",
        "        W, _ = cam_sizes[cam]\n",
        "        yaw = camera_yaw_center_in_ego(T_EGO_SENSOR[cam])\n",
        "        hfov = hfov_from_intrinsics(INTR[cam], W)\n",
        "        cam_info[cam] = (yaw, hfov)\n",
        "\n",
        "    cams = list(cam_info.keys())\n",
        "    pairs = []\n",
        "    for i in range(len(cams)):\n",
        "        for j in range(i+1, len(cams)):\n",
        "            c1, c2 = cams[i], cams[j]\n",
        "            yaw1, hfov1 = cam_info[c1]\n",
        "            yaw2, hfov2 = cam_info[c2]\n",
        "            ov = circular_overlap(yaw1, hfov1, yaw2, hfov2)\n",
        "            if ov > np.pi + 1e-6:\n",
        "                raise RuntimeError(f\"Impossible overlap > 180deg for {c1},{c2}: {ov} rad\")\n",
        "            if ov >= min_overlap:\n",
        "                pairs.append((c1, c2, ov))\n",
        "\n",
        "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "    return pairs\n",
        "\n",
        "def yolo_line_from_box(box: Box2D, cls_id: int, img_w: int, img_h: int) -> str:\n",
        "    cx = ((box.xmin + box.xmax) / 2.0) / img_w\n",
        "    cy = ((box.ymin + box.ymax) / 2.0) / img_h\n",
        "    w  = (box.xmax - box.xmin) / img_w\n",
        "    h  = (box.ymax - box.ymin) / img_h\n",
        "    cx = min(max(float(cx), 0.0), 1.0)\n",
        "    cy = min(max(float(cy), 0.0), 1.0)\n",
        "    w  = min(max(float(w),  0.0), 1.0)\n",
        "    h  = min(max(float(h),  0.0), 1.0)\n",
        "    return f\"{cls_id} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMafUpUSAafh"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 4 — Load metadata for ALL logs + build GLOBAL class map\n",
        "# ============================================================\n",
        "\n",
        "def load_initial_data(scene_path: Path):\n",
        "    ann_df = io_utils.read_feather(scene_path / \"annotations.feather\")\n",
        "    intr_df = io_utils.read_feather(scene_path / \"calibration\" / \"intrinsics.feather\")\n",
        "    extr_df = io_utils.read_feather(scene_path / \"calibration\" / \"egovehicle_SE3_sensor.feather\")\n",
        "    return ann_df, intr_df, extr_df\n",
        "\n",
        "# Load each log’s ann_df (for categories + splits)\n",
        "ANN_BY_LOG: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "all_categories: Set[str] = set()\n",
        "CATEGORY_COL = \"category\"  # confirmed by your printout, keep fixed for all logs\n",
        "\n",
        "for log_id in LOG_IDS:\n",
        "    scene_path = AV2_ROOT / log_id\n",
        "    ann_df, intr_df, extr_df = load_initial_data(scene_path)\n",
        "    ANN_BY_LOG[log_id] = ann_df\n",
        "\n",
        "    if CATEGORY_COL not in ann_df.columns:\n",
        "        raise ValueError(f\"{log_id}: missing '{CATEGORY_COL}' in annotations.feather\")\n",
        "\n",
        "    cats = ann_df[CATEGORY_COL].dropna().astype(str).unique().tolist()\n",
        "    all_categories.update(cats)\n",
        "\n",
        "# GLOBAL classes across all logs (important!)\n",
        "NAMES = sorted(list(all_categories))\n",
        "CLASS_MAP = {c: i for i, c in enumerate(NAMES)}\n",
        "\n",
        "print(\"GLOBAL Num classes:\", len(NAMES))\n",
        "print(\"GLOBAL Example class map (first 15):\", list(CLASS_MAP.items())[:15])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyHiIFJfAeMI"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 5 — Shared train/val split across ALL logs (timestamp keys)\n",
        "# ============================================================\n",
        "\n",
        "def make_train_val_split_by_log_timestamp(\n",
        "    ann_by_log: Dict[str, pd.DataFrame],\n",
        "    train_ratio: float = 0.8,\n",
        "    seed: int = 7\n",
        ") -> Tuple[Set[Tuple[str, int]], Set[Tuple[str, int]]]:\n",
        "    keys: List[Tuple[str, int]] = []\n",
        "    for log_id, ann_df in ann_by_log.items():\n",
        "        if \"timestamp_ns\" not in ann_df.columns:\n",
        "            raise ValueError(f\"{log_id}: annotations missing 'timestamp_ns'\")\n",
        "        for ts in ann_df[\"timestamp_ns\"].dropna().astype(np.int64).unique():\n",
        "            keys.append((log_id, int(ts)))\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    rng.shuffle(keys)\n",
        "\n",
        "    n_train = int(len(keys) * train_ratio)\n",
        "    train_keys = set(keys[:n_train])\n",
        "    val_keys   = set(keys[n_train:])\n",
        "    return train_keys, val_keys\n",
        "\n",
        "TRAIN_KEYS, VAL_KEYS = make_train_val_split_by_log_timestamp(ANN_BY_LOG, train_ratio=0.8, seed=SPLIT_SEED)\n",
        "print(f\"Total (log,timestamp) keys: {len(TRAIN_KEYS) + len(VAL_KEYS)}\")\n",
        "print(f\"Train keys: {len(TRAIN_KEYS)} | Val keys: {len(VAL_KEYS)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtKBYAGMAf-8"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 6 — Build YOLO dataset for ALL logs (baseline + pruned taus)\n",
        "#         Also compute deletion statistics.\n",
        "# ============================================================\n",
        "\n",
        "def reset_dir(p: Path):\n",
        "    if p.exists():\n",
        "        shutil.rmtree(p)\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def build_yolo_from_av2_logs(\n",
        "    log_ids: List[str],\n",
        "    av2_root: Path,\n",
        "    ann_by_log: Dict[str, pd.DataFrame],\n",
        "    out_root: Path,\n",
        "    tau_bcs: float,\n",
        "    train_keys: Set[Tuple[str, int]],\n",
        "    val_keys: Set[Tuple[str, int]],\n",
        "    class_map: Dict[str, int],\n",
        "    names: List[str],\n",
        "    max_ts_diff_ns: int = 50_000_000,\n",
        "    drop_empty_images: bool = False,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Builds a single YOLO dataset combining multiple logs.\n",
        "    Pruning rule (same as your code):\n",
        "      For overlap camera pair, if same (timestamp, track_uuid) appears in both cams and\n",
        "      |BCS_A - BCS_B| > tau_bcs -> drop the lower-BCS label instance (keep higher-BCS).\n",
        "\n",
        "    Reports:\n",
        "      - total_candidate_labels_before_pruning\n",
        "      - total_deleted_label_instances (unique (cam, track, ts_img) drops)\n",
        "      - total_unique_3d_objects_seen (unique (log, timestamp, track_uuid) with at least 1 camera projection)\n",
        "    \"\"\"\n",
        "\n",
        "    dataset_dir = out_root\n",
        "    reset_dir(dataset_dir)\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        (dataset_dir / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
        "        (dataset_dir / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    total_candidate_labels = 0\n",
        "    total_deleted_labels = 0\n",
        "    total_unique_3d_objects = set()  # (log_id, ts, track_uuid) that got at least one projected box\n",
        "\n",
        "    # Iterate each log and export its (log,timestamp) keys\n",
        "    for log_id in log_ids:\n",
        "        scene_path = av2_root / log_id\n",
        "        ann_df = ann_by_log[log_id]\n",
        "\n",
        "        # Load per-log intr/extr and build dictionaries\n",
        "        _, intr_df, extr_df = load_initial_data(scene_path)\n",
        "        INTR = build_intrinsics_dict(intr_df)\n",
        "        T_EGO_SENSOR = build_extrinsics_dict(extr_df)             # sensor -> ego (as in your notebook)\n",
        "        T_SENSOR_EGO = {k: v.inverse() for k, v in T_EGO_SENSOR.items()}  # ego -> sensor\n",
        "\n",
        "        # Cameras and images for this log\n",
        "        CAMERAS = intr_df[\"sensor_name\"].astype(str).unique().tolist()\n",
        "        CAM_ROOT = find_camera_root(scene_path)\n",
        "        IMG_INDEX = index_images(CAM_ROOT, CAMERAS)\n",
        "\n",
        "        # per-log overlap pairs\n",
        "        OVERLAP_PAIRS = compute_overlap_pairs(CAMERAS, INTR, T_EGO_SENSOR, IMG_INDEX, min_overlap_deg=5.0)\n",
        "\n",
        "        # keep only cameras that actually have images indexed\n",
        "        cameras = [c for c in INTR.keys() if c in IMG_INDEX and len(IMG_INDEX[c]) > 0]\n",
        "        if not cameras:\n",
        "            print(f\"[WARN] {log_id}: no cameras with images. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # one image size per camera\n",
        "        cam_size = {}\n",
        "        for cam in cameras:\n",
        "            any_path = next(iter(IMG_INDEX[cam].values()))\n",
        "            with Image.open(any_path) as im:\n",
        "                cam_size[cam] = im.size  # (W,H)\n",
        "\n",
        "        cam_ts_sorted = {cam: sorted(IMG_INDEX[cam].keys()) for cam in cameras}\n",
        "\n",
        "        # process only timestamps in our global train/val split\n",
        "        keys_for_log = sorted(list({k for k in (train_keys | val_keys) if k[0] == log_id}), key=lambda x: x[1])\n",
        "\n",
        "        for (_, ts) in tqdm(keys_for_log, desc=f\"Building log {log_id[:8]}... tau={tau_bcs}\", leave=False):\n",
        "            split = \"train\" if (log_id, ts) in train_keys else (\"val\" if (log_id, ts) in val_keys else None)\n",
        "            if split is None:\n",
        "                continue\n",
        "\n",
        "            ann_rows = ann_df[ann_df[\"timestamp_ns\"] == ts]\n",
        "            if ann_rows.empty:\n",
        "                continue\n",
        "\n",
        "            # per-camera candidates for this timestamp\n",
        "            # per_cam_boxes[cam][track_uuid] = (Box2D, cls_id, ts_img, img_path, W, H)\n",
        "            per_cam_boxes: Dict[str, Dict[str, Tuple[Box2D, int, int, Path, int, int]]] = {cam: {} for cam in cameras}\n",
        "\n",
        "            # Build candidates\n",
        "            for _, row in ann_rows.iterrows():\n",
        "                track = str(row[\"track_uuid\"])\n",
        "                cat = row[CATEGORY_COL]\n",
        "                if pd.isna(cat):\n",
        "                    continue\n",
        "                cat_str = str(cat)\n",
        "                if cat_str not in class_map:\n",
        "                    continue\n",
        "                cls_id = int(class_map[cat_str])\n",
        "\n",
        "                corners = cuboid_corners_ego(row)\n",
        "\n",
        "                any_projection = False\n",
        "                for cam in cameras:\n",
        "                    ts_img = nearest_timestamp(ts, cam_ts_sorted[cam], max_diff_ns=max_ts_diff_ns)\n",
        "                    if ts_img is None:\n",
        "                        continue\n",
        "                    img_path = IMG_INDEX[cam][ts_img]\n",
        "                    W, H = cam_size[cam]\n",
        "\n",
        "                    box = bbox_and_bcs_from_cuboid(\n",
        "                        corners_ego=corners,\n",
        "                        intr=INTR[cam],\n",
        "                        T_sensor_ego=T_SENSOR_EGO[cam],  # ego -> camera\n",
        "                        img_w=W,\n",
        "                        img_h=H,\n",
        "                    )\n",
        "                    if box is None:\n",
        "                        continue\n",
        "\n",
        "                    per_cam_boxes[cam][track] = (box, cls_id, ts_img, img_path, W, H)\n",
        "                    any_projection = True\n",
        "\n",
        "                if any_projection:\n",
        "                    total_unique_3d_objects.add((log_id, int(ts), track))\n",
        "\n",
        "            # Count candidate label instances before pruning\n",
        "            # (one label instance == one track in one camera at that ts)\n",
        "            candidates_this_ts = sum(len(per_cam_boxes[cam]) for cam in cameras)\n",
        "            total_candidate_labels += candidates_this_ts\n",
        "\n",
        "            # Prune: mark (cam, track, ts_img) to drop\n",
        "            to_drop: Set[Tuple[str, str, int]] = set()\n",
        "\n",
        "            for camA, camB, _ in OVERLAP_PAIRS:\n",
        "                if camA not in per_cam_boxes or camB not in per_cam_boxes:\n",
        "                    continue\n",
        "                common_tracks = set(per_cam_boxes[camA].keys()) & set(per_cam_boxes[camB].keys())\n",
        "                for track in common_tracks:\n",
        "                    boxA, clsA, tsA, _, _, _ = per_cam_boxes[camA][track]\n",
        "                    boxB, clsB, tsB, _, _, _ = per_cam_boxes[camB][track]\n",
        "                    if clsA != clsB:\n",
        "                        continue\n",
        "\n",
        "                    if abs(boxA.bcs - boxB.bcs) > tau_bcs:\n",
        "                        # drop the lower-BCS instance\n",
        "                        if boxA.bcs >= boxB.bcs:\n",
        "                            to_drop.add((camB, track, tsB))\n",
        "                        else:\n",
        "                            to_drop.add((camA, track, tsA))\n",
        "\n",
        "            total_deleted_labels += len(to_drop)\n",
        "\n",
        "            # Write images + labels\n",
        "            # IMPORTANT: unique filenames across logs to avoid collisions\n",
        "            for cam in cameras:\n",
        "                entries = list(per_cam_boxes[cam].items())\n",
        "                if not entries:\n",
        "                    continue\n",
        "\n",
        "                # Use first entry to select the image to copy\n",
        "                _, (_, _, ts_img, img_path, W, H) = entries[0]\n",
        "\n",
        "                out_img_name = f\"{log_id}_{cam}_{ts_img}.jpg\"\n",
        "                out_lbl_name = f\"{log_id}_{cam}_{ts_img}.txt\"\n",
        "\n",
        "                lines = []\n",
        "                for track, (box, cls_id, ts_img2, _, W2, H2) in per_cam_boxes[cam].items():\n",
        "                    if (cam, track, ts_img2) in to_drop:\n",
        "                        continue\n",
        "                    lines.append(yolo_line_from_box(box, cls_id, W2, H2))\n",
        "\n",
        "                if drop_empty_images and len(lines) == 0:\n",
        "                    continue\n",
        "\n",
        "                dst_img = dataset_dir / \"images\" / split / out_img_name\n",
        "                shutil.copy(img_path, dst_img)\n",
        "\n",
        "                dst_lbl = dataset_dir / \"labels\" / split / out_lbl_name\n",
        "                with open(dst_lbl, \"w\") as f:\n",
        "                    f.write(\"\\n\".join(lines))\n",
        "\n",
        "    # data.yaml\n",
        "    data_yaml = dataset_dir / \"data.yaml\"\n",
        "    yaml_text = (\n",
        "        f\"path: {dataset_dir}\\n\"\n",
        "        f\"train: images/train\\n\"\n",
        "        f\"val: images/val\\n\"\n",
        "        f\"nc: {len(names)}\\n\"\n",
        "        f\"names: {json.dumps(names)}\\n\"\n",
        "    )\n",
        "    with open(data_yaml, \"w\") as f:\n",
        "        f.write(yaml_text)\n",
        "\n",
        "    return {\n",
        "        \"dataset_dir\": dataset_dir,\n",
        "        \"data_yaml\": data_yaml,\n",
        "        \"total_candidate_labels_before_pruning\": int(total_candidate_labels),\n",
        "        \"total_deleted_label_instances\": int(total_deleted_labels),\n",
        "        \"total_unique_3d_objects_seen\": int(len(total_unique_3d_objects)),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDr4-W6mAhq8"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 7 — Build UNPRUNED (baseline) dataset once (for evaluation)\n",
        "#         Then build pruned datasets for tau in [0.1..0.6]\n",
        "#         Report deletion statistics.\n",
        "# ============================================================\n",
        "\n",
        "# \"unpruned\" baseline: use a very large tau so abs(BCS_A-BCS_B) > tau never triggers\n",
        "BASELINE_TAU = 1e9\n",
        "\n",
        "baseline_info = build_yolo_from_av2_logs(\n",
        "    log_ids=LOG_IDS,\n",
        "    av2_root=AV2_ROOT,\n",
        "    ann_by_log=ANN_BY_LOG,\n",
        "    out_root=OUT_ROOT / \"baseline_unpruned\",\n",
        "    tau_bcs=BASELINE_TAU,\n",
        "    train_keys=TRAIN_KEYS,\n",
        "    val_keys=VAL_KEYS,\n",
        "    class_map=CLASS_MAP,\n",
        "    names=NAMES,\n",
        "    max_ts_diff_ns=MAX_TS_DIFF_NS,\n",
        "    drop_empty_images=False,\n",
        ")\n",
        "\n",
        "print(\"\\n=== BASELINE (UNPRUNED) DATASET BUILT ===\")\n",
        "print(\"Dataset:\", baseline_info[\"dataset_dir\"])\n",
        "print(\"Total candidate label instances (before pruning):\", baseline_info[\"total_candidate_labels_before_pruning\"])\n",
        "print(\"Total deleted label instances:\", baseline_info[\"total_deleted_label_instances\"])\n",
        "print(\"Total unique 3D objects seen:\", baseline_info[\"total_unique_3d_objects_seen\"])\n",
        "\n",
        "PRUNED_INFOS = {}\n",
        "\n",
        "for tau in TAU_LIST:\n",
        "    info = build_yolo_from_av2_logs(\n",
        "        log_ids=LOG_IDS,\n",
        "        av2_root=AV2_ROOT,\n",
        "        ann_by_log=ANN_BY_LOG,\n",
        "        out_root=OUT_ROOT / f\"pruned_tau{tau:.1f}\",\n",
        "        tau_bcs=float(tau),\n",
        "        train_keys=TRAIN_KEYS,\n",
        "        val_keys=VAL_KEYS,\n",
        "        class_map=CLASS_MAP,\n",
        "        names=NAMES,\n",
        "        max_ts_diff_ns=MAX_TS_DIFF_NS,\n",
        "        drop_empty_images=False,\n",
        "    )\n",
        "    PRUNED_INFOS[tau] = info\n",
        "\n",
        "    print(f\"\\n=== PRUNED DATASET tau={tau:.1f} BUILT ===\")\n",
        "    print(\"Dataset:\", info[\"dataset_dir\"])\n",
        "    print(\"Total candidate label instances (before pruning):\", info[\"total_candidate_labels_before_pruning\"])\n",
        "    print(\"Total deleted label instances:\", info[\"total_deleted_label_instances\"])\n",
        "    print(\"Total unique 3D objects seen:\", info[\"total_unique_3d_objects_seen\"])\n",
        "\n",
        "print(\"\\n=== SUMMARY (All taus) ===\")\n",
        "base_before = baseline_info[\"total_candidate_labels_before_pruning\"]\n",
        "print(\"Baseline candidate labels:\", base_before)\n",
        "for tau in TAU_LIST:\n",
        "    deleted = PRUNED_INFOS[tau][\"total_deleted_label_instances\"]\n",
        "    before  = PRUNED_INFOS[tau][\"total_candidate_labels_before_pruning\"]\n",
        "    uniq3d  = PRUNED_INFOS[tau][\"total_unique_3d_objects_seen\"]\n",
        "    print(f\"tau={tau:.1f}  before={before}  deleted={deleted}  remaining={before-deleted}  uniq3d={uniq3d}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrA3ADL9AjgE"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 7.5 — Build UNPRUNED baseline once, then pruned datasets for all taus\n",
        "# ============================================================\n",
        "\n",
        "baseline_info = build_yolo_from_av2_logs(\n",
        "    log_ids=LOG_IDS,\n",
        "    av2_root=AV2_ROOT,\n",
        "    ann_by_log=ANN_BY_LOG,\n",
        "    out_root=OUT_ROOT / \"baseline_unpruned\",\n",
        "    tau_bcs=BASELINE_TAU,\n",
        "    train_keys=TRAIN_KEYS,\n",
        "    val_keys=VAL_KEYS,\n",
        "    class_map=CLASS_MAP,\n",
        "    names=NAMES,\n",
        "    max_ts_diff_ns=MAX_TS_DIFF_NS,\n",
        "    drop_empty_images=False,\n",
        ")\n",
        "\n",
        "print(\"\\n=== BASELINE (UNPRUNED) BUILT ===\")\n",
        "print(baseline_info)\n",
        "\n",
        "PRUNED_INFOS = {}\n",
        "for tau in TAU_LIST:\n",
        "    info = build_yolo_from_av2_logs(\n",
        "        log_ids=LOG_IDS,\n",
        "        av2_root=AV2_ROOT,\n",
        "        ann_by_log=ANN_BY_LOG,\n",
        "        out_root=OUT_ROOT / f\"pruned_tau{tau:.1f}\",\n",
        "        tau_bcs=float(tau),\n",
        "        train_keys=TRAIN_KEYS,\n",
        "        val_keys=VAL_KEYS,\n",
        "        class_map=CLASS_MAP,\n",
        "        names=NAMES,\n",
        "        max_ts_diff_ns=MAX_TS_DIFF_NS,\n",
        "        drop_empty_images=False,\n",
        "    )\n",
        "    PRUNED_INFOS[tau] = info\n",
        "    print(f\"\\n=== PRUNED tau={tau:.1f} BUILT ===\")\n",
        "    print(info)\n",
        "\n",
        "print(\"\\n=== DELETION SUMMARY ===\")\n",
        "for tau in TAU_LIST:\n",
        "    before = PRUNED_INFOS[tau][\"total_candidate_labels_before_pruning\"]\n",
        "    deleted = PRUNED_INFOS[tau][\"total_deleted_label_instances\"]\n",
        "    print(f\"tau={tau:.1f}  before={before}  deleted={deleted}  remaining={before-deleted}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSMECztkBQ9r"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 8 — Train on baseline/pruned datasets (ALL 8 LOGS),\n",
        "#          evaluate ALL models on UNPRUNED validation\n",
        "#          and write a single report table + CSV.\n",
        "# ============================================================\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG (edit if you want)\n",
        "# ----------------------------\n",
        "EPOCHS = 30\n",
        "IMGSZ  = 640\n",
        "BATCH  = 16\n",
        "SEED   = 7\n",
        "\n",
        "# Make sure these exist from Step 7.5:\n",
        "# baseline_info, PRUNED_INFOS, TAU_LIST, OUT_ROOT\n",
        "\n",
        "BASELINE_DIR = Path(baseline_info[\"dataset_dir\"])      # unpruned (train+val splits exist inside)\n",
        "EVAL_DIR     = BASELINE_DIR                            # evaluate on UNPRUNED validation for everyone\n",
        "\n",
        "print(\"Baseline (unpruned) dataset:\", BASELINE_DIR)\n",
        "print(\"Eval dataset (unpruned):\", EVAL_DIR)\n",
        "print(\"Pruned dataset dirs:\")\n",
        "for tau in TAU_LIST:\n",
        "    print(f\"  tau={tau:.1f} -> {Path(PRUNED_INFOS[tau]['dataset_dir'])}\")\n",
        "\n",
        "# ----------------------------\n",
        "# TRAIN + EVAL helper\n",
        "# ----------------------------\n",
        "def train_and_eval_yolo(\n",
        "    train_data_dir: Path,\n",
        "    eval_data_dir: Path,\n",
        "    run_name: str,\n",
        "    epochs: int = 30,\n",
        "    imgsz: int = 640,\n",
        "    batch: int = 16,\n",
        "    seed: int = 7\n",
        "):\n",
        "    train_data_dir = Path(train_data_dir)\n",
        "    eval_data_dir  = Path(eval_data_dir)\n",
        "\n",
        "    # 1) train\n",
        "    model = YOLO(\"yolov8n.pt\")\n",
        "    train_res = model.train(\n",
        "        data=str(train_data_dir / \"data.yaml\"),\n",
        "        epochs=epochs,\n",
        "        imgsz=imgsz,\n",
        "        batch=batch,\n",
        "        device=0,            # GPU\n",
        "        cache=True,\n",
        "        workers=4,\n",
        "        name=run_name,\n",
        "        project=str(train_data_dir / \"runs\"),\n",
        "        verbose=True,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    # 2) eval on UNPRUNED validation (same eval dir for all)\n",
        "    metrics = model.val(\n",
        "        data=str(eval_data_dir / \"data.yaml\"),\n",
        "        device=0\n",
        "    )\n",
        "\n",
        "    precision = float(metrics.box.mp)\n",
        "    recall    = float(metrics.box.mr)\n",
        "    map50     = float(metrics.box.map50)\n",
        "    map5095   = float(metrics.box.map)\n",
        "    f1        = (2 * precision * recall / (precision + recall + 1e-12))\n",
        "\n",
        "    # optional: read last train box loss from Ultralytics results.csv\n",
        "    box_loss = None\n",
        "    results_csv = Path(train_res.save_dir) / \"results.csv\"\n",
        "    if results_csv.exists():\n",
        "        df = pd.read_csv(results_csv)\n",
        "        # Ultralytics naming can vary; this catches common cases\n",
        "        cand_cols = [c for c in df.columns if \"train/box_loss\" in c.lower() or c.lower() == \"train/box_loss\"]\n",
        "        if cand_cols:\n",
        "            box_loss = float(df[cand_cols[0]].iloc[-1])\n",
        "\n",
        "    return {\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1,\n",
        "        \"mAP50\": map50,\n",
        "        \"mAP50-95\": map5095,\n",
        "        \"Box Loss\": box_loss\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# RUN ALL MODELS\n",
        "# ----------------------------\n",
        "rows = []\n",
        "\n",
        "# (A) Baseline: train unpruned -> eval unpruned\n",
        "base_metrics = train_and_eval_yolo(\n",
        "    train_data_dir=BASELINE_DIR,\n",
        "    eval_data_dir=EVAL_DIR,\n",
        "    run_name=\"baseline_train_unpruned__eval_unpruned\",\n",
        "    epochs=EPOCHS, imgsz=IMGSZ, batch=BATCH, seed=SEED\n",
        ")\n",
        "rows.append({\n",
        "    \"Model Strategy\": \"Baseline (train unpruned → eval unpruned)\",\n",
        "    \"tau_BCS\": \"unpruned\",\n",
        "    \"candidate_labels_before\": baseline_info[\"total_candidate_labels_before_pruning\"],\n",
        "    \"deleted_labels\": baseline_info[\"total_deleted_label_instances\"],\n",
        "    \"remaining_labels\": baseline_info[\"total_candidate_labels_before_pruning\"] - baseline_info[\"total_deleted_label_instances\"],\n",
        "    \"unique_3d_objects_seen\": baseline_info[\"total_unique_3d_objects_seen\"],\n",
        "    **base_metrics\n",
        "})\n",
        "\n",
        "# (B) Pruned: train pruned(tau) -> eval unpruned\n",
        "for tau in TAU_LIST:\n",
        "    train_dir = Path(PRUNED_INFOS[tau][\"dataset_dir\"])\n",
        "\n",
        "    pr_metrics = train_and_eval_yolo(\n",
        "        train_data_dir=train_dir,\n",
        "        eval_data_dir=EVAL_DIR,\n",
        "        run_name=f\"train_pruned_tau{tau:.1f}__eval_unpruned\",\n",
        "        epochs=EPOCHS, imgsz=IMGSZ, batch=BATCH, seed=SEED\n",
        "    )\n",
        "\n",
        "    before  = PRUNED_INFOS[tau][\"total_candidate_labels_before_pruning\"]\n",
        "    deleted = PRUNED_INFOS[tau][\"total_deleted_label_instances\"]\n",
        "\n",
        "    rows.append({\n",
        "        \"Model Strategy\": f\"Pruned (train tau={tau:.1f} → eval unpruned)\",\n",
        "        \"tau_BCS\": float(tau),\n",
        "        \"candidate_labels_before\": before,\n",
        "        \"deleted_labels\": deleted,\n",
        "        \"remaining_labels\": before - deleted,\n",
        "        \"unique_3d_objects_seen\": PRUNED_INFOS[tau][\"total_unique_3d_objects_seen\"],\n",
        "        **pr_metrics\n",
        "    })\n",
        "\n",
        "report_df = pd.DataFrame(rows, columns=[\n",
        "    \"Model Strategy\", \"tau_BCS\",\n",
        "    \"candidate_labels_before\", \"deleted_labels\", \"remaining_labels\", \"unique_3d_objects_seen\",\n",
        "    \"Precision\", \"Recall\", \"F1-Score\", \"mAP50\", \"mAP50-95\", \"Box Loss\"\n",
        "])\n",
        "\n",
        "print(\"\\n=== FINAL YOLO THRESHOLD REPORT (train varies, eval always unpruned) ===\")\n",
        "print(report_df.to_string(index=False))\n",
        "\n",
        "# Save to Drive\n",
        "report_path = Path(OUT_ROOT) / \"yolo_threshold_report.csv\"\n",
        "report_df.to_csv(report_path, index=False)\n",
        "print(\"\\nSaved report to:\", report_path)\n",
        "\n",
        "# Also keep it in memory for later use\n",
        "REPORT_DF = report_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yFWxEDrmgwLl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}